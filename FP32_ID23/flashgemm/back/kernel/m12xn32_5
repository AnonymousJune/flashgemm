// 将C预取隐藏在k维计算中，预取到L2 Cache，条件预取，性能还是预取到L3比较好
.macro	KERNEL12x32_PACK_K1							

   vbroadcastss	8(%%rax), %%zmm2       // A2   			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm8 // A0*(B0-15) 			
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm9 // A0*(B16-31) 			

   vbroadcastss	12(%%rax), %%zmm3       // A3    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm10 // A1*(B0-15) 		
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm11 // A1*(B16-31) 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm0      // A4   			
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm13 		

   prefetcht2 		128(%%rbx)                 		

   vbroadcastss	20(%%rax), %%zmm1     // A5		
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2     // A6    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm17 		

   prefetcht2 		192(%%rbx)                 		

   vbroadcastss	28(%%rax), %%zmm3     // A7    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0     // A8    			
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm21 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%zmm1     // A9    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm23 		

   vbroadcastss	40(%%rax), %%zmm2     // A10    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm25 		

   prefetcht0 		384(%%rax)                 		

   vbroadcastss	44(%%rax), %%zmm3    // A11    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm26 		
   vmovups 		(%%rbx), %%zmm6 // next B0       			
	addq  			$48, %%rax 	 //下一组A(已读12个)					
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0     // next A0    				
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm28 		
   vmovups 		64(%%rbx), %%zmm7 // next B1       		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    // next A1    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm30 		
   vmovups 		%%zmm4, (%%rbp)  // pack B0 to Bc     			
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm31 		
   vmovups 		%%zmm5, 64(%%rbp)// pack B1 to Bc       		
	addq  			$128, %%rbp 					

.endm 												

.macro	KERNEL12x32_PACK_K2							

   vbroadcastss	8(%%rax), %%zmm2     // next A2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	12(%%rax), %%zmm3     // next A3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		

   prefetcht2 		128(%%rbx)                 		

   vbroadcastss	16(%%rax), %%zmm0     // next A4    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm13 		

   prefetcht2 		192(%%rbx)                 		

   vbroadcastss	20(%%rax), %%zmm1     // next A5    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2     // next A6    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	28(%%rax), %%zmm3     // next A7    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0     // next A8    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%zmm1     // next A9    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   vbroadcastss	40(%%rax), %%zmm2     // next A10    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	44(%%rax), %%zmm3     // next A11    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
   vmovups 		(%%rbx), %%zmm4 // next next B0       			
	addq  			$48, %%rax 	 // 下一组A(已读12个) 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0    	 // next next A0			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28		
   vmovups 		64(%%rbx), %%zmm5 // next next B1      		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    	 // next next A1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
   vmovups 		%%zmm6, (%%rbp)   // pack B0 to Bc      			
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		
   vmovups 		%%zmm7, 64(%%rbp) // pack B1 to Bc      		
	addq  			$128, %%rbp 					

.endm 												

.macro	KERNEL12x32_PACK_END_K						

   vbroadcastss	8(%%rax), %%zmm2    // next A2    			

   vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			

   vbroadcastss	12(%%rax), %%zmm3    // next A3    			

   vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm0    // next A4    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm13 		

   vbroadcastss	20(%%rax), %%zmm1    // next A5    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2    // next A6    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	28(%%rax), %%zmm3    // next A7    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0    // next A8   			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

   vbroadcastss	36(%%rax), %%zmm1    // next A9    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   vbroadcastss	40(%%rax), %%zmm2    // next A10    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	44(%%rax), %%zmm3    // next A11    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
	addq  			$48, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   vmovups 		%%zmm6, (%%rbp)  // pack B0 to Bc       			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

   vmovups 		%%zmm7, 64(%%rbp)// pack B0 to Bc       		
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		

.endm 												

//-----------------------------------------------------------------

.macro	KERNEL12x32_K1								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm8 			
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm9 			

   vbroadcastss	12(%%rax), %%zmm3    			

   vfmadd231ps		%%zmm1, %%zmm4, %%zmm10 		
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm11 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm13 		

   vbroadcastss	20(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm17 		

   vbroadcastss	28(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm21 		

	addq  			$128, %%rbx 					

   vbroadcastss	36(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm23 		

   prefetcht0 		64(%%rbx)                 		

   vbroadcastss	40(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm25 		

   vbroadcastss	44(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm26 		
   vmovups 		(%%rbx), %%zmm6        			
	addq  			$48, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm28 		
   vmovups 		64(%%rbx), %%zmm7       		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm30 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm31 		

.endm 												

.macro	KERNEL12x32_K2								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm13 		

   vbroadcastss	20(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	28(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

	addq  			$128, %%rbx 					

   vbroadcastss	36(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   prefetcht0 		64(%%rbx)                 		

   vbroadcastss	40(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	44(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
   vmovups 		(%%rbx), %%zmm4        			
	addq  			$48, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28 		
   vmovups 		64(%%rbx), %%zmm5       		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		

.endm 												

.macro	KERNEL12x32_END_K							

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			
   vbroadcastss	12(%%rax), %%zmm3    			

   vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm13 		

   vbroadcastss	20(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 		

   vbroadcastss	24(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	28(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   vbroadcastss	32(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

   vbroadcastss	36(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   vbroadcastss	40(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	44(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
	addq  			$48, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

   vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		

.endm 												

.macro	ADD_C_12x32									

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm8, %%zmm8			
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm9, %%zmm9			
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm10, %%zmm10		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm11, %%zmm11		
   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm12, %%zmm12		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm13, %%zmm13		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm14, %%zmm14		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm15, %%zmm15		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm16, %%zmm16		
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm17, %%zmm17		
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm18, %%zmm18		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm19, %%zmm19		

   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm20, %%zmm20		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm21, %%zmm21		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm22, %%zmm22		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm23, %%zmm23		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm24, %%zmm24		
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm25, %%zmm25		
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm26, %%zmm26		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm27, %%zmm27		

   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm28, %%zmm28		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm29, %%zmm29		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm30, %%zmm30		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm31, %%zmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_12x32 									

   vmovups 		%%zmm8, (%%r10)        			
   vmovups 		%%zmm9, 64(%%r10)        		
   vmovups 		%%zmm10, (%%r11)        		
   vmovups 		%%zmm11, 64(%%r11)        		
   vmovups 		%%zmm12, (%%r12)        		
   vmovups 		%%zmm13, 64(%%r12)        		
   vmovups 		%%zmm14, (%%r13)        		
   vmovups 		%%zmm15, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%zmm16, (%%r10)        		
   vmovups 		%%zmm17, 64(%%r10)        		
   vmovups 		%%zmm18, (%%r11)        		
   vmovups 		%%zmm19, 64(%%r11)        		
   vmovups 		%%zmm20, (%%r12)        		
   vmovups 		%%zmm21, 64(%%r12)        		
   vmovups 		%%zmm22, (%%r13)        		
   vmovups 		%%zmm23, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%zmm24, (%%r10)        		
   vmovups 		%%zmm25, 64(%%r10)        		
   vmovups 		%%zmm26, (%%r11)        		
   vmovups 		%%zmm27, 64(%%r11)        		
	subq 			$12, %%rdi 						
   vmovups 		%%zmm28, (%%r12)        		
   vmovups 		%%zmm29, 64(%%r12)        		
   vmovups 		%%zmm30, (%%r13)        		
   vmovups 		%%zmm31, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------

.macro	KERNEL8x32_K1								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm17 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm19 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm8    			
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm21 		

	addq  			$128, %%rbx 					

   vbroadcastss	20(%%rax), %%zmm9    			
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm23 		

   prefetcht0 		64(%%rbx)                 		

   vbroadcastss	24(%%rax), %%zmm10    			
   vfmadd231ps		%%zmm8, %%zmm4, %%zmm24 		
   vfmadd231ps		%%zmm8, %%zmm5, %%zmm25 		

   vbroadcastss	28(%%rax), %%zmm11    			
   vfmadd231ps		%%zmm9, %%zmm4, %%zmm26 		
   vmovups 		(%%rbx), %%zmm6        			
	addq  			$32, %%rax 						
   vfmadd231ps		%%zmm9, %%zmm5, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm10, %%zmm4, %%zmm28 		
   vmovups 		64(%%rbx), %%zmm7       		
   vfmadd231ps		%%zmm10, %%zmm5, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm11, %%zmm4, %%zmm30 		
   vfmadd231ps		%%zmm11, %%zmm5, %%zmm31 		

.endm 												

.macro	KERNEL8x32_K2								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm8    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

	addq  			$128, %%rbx 					

   vbroadcastss	20(%%rax), %%zmm9    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   prefetcht0 		64(%%rbx)                 		

   vbroadcastss	24(%%rax), %%zmm10    			
   vfmadd231ps		%%zmm8, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm8, %%zmm7, %%zmm25 		

   vbroadcastss	28(%%rax), %%zmm11    			
   vfmadd231ps		%%zmm9, %%zmm6, %%zmm26 		
   vmovups 		(%%rbx), %%zmm4        			
	addq  			$32, %%rax 						
   vfmadd231ps		%%zmm9, %%zmm7, %%zmm27 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm10, %%zmm6, %%zmm28 		
   vmovups 		64(%%rbx), %%zmm5       		
   vfmadd231ps		%%zmm10, %%zmm7, %%zmm29 		

   vbroadcastss	4(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm11, %%zmm6, %%zmm30 		
   vfmadd231ps		%%zmm11, %%zmm7, %%zmm31 		

.endm 												

.macro	KERNEL8x32_END_K							

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm16 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm17 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm18 		
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm19 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%zmm8    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm20 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm21 		

   vbroadcastss	20(%%rax), %%zmm9    			
   vfmadd231ps		%%zmm3, %%zmm6, %%zmm22 		
   vfmadd231ps		%%zmm3, %%zmm7, %%zmm23 		

   prefetcht0 		64(%%rbx)                 		

   vbroadcastss	24(%%rax), %%zmm10    			
   vfmadd231ps		%%zmm8, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm8, %%zmm7, %%zmm25 		

   vbroadcastss	28(%%rax), %%zmm11    			
   vfmadd231ps		%%zmm9, %%zmm6, %%zmm26 		
	addq  			$32, %%rax 						
   vfmadd231ps		%%zmm9, %%zmm7, %%zmm27 		

   vfmadd231ps		%%zmm10, %%zmm6, %%zmm28 		
   vfmadd231ps		%%zmm10, %%zmm7, %%zmm29 		
   vfmadd231ps		%%zmm11, %%zmm6, %%zmm30 		
   vfmadd231ps		%%zmm11, %%zmm7, %%zmm31 		

.endm 												

.macro	ADD_C_8x32									

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm16, %%zmm16		
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm17, %%zmm17		
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm18, %%zmm18		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm19, %%zmm19		

   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm20, %%zmm20		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm21, %%zmm21		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm22, %%zmm22		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm23, %%zmm23		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm24, %%zmm24		
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm25, %%zmm25		
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm26, %%zmm26		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm27, %%zmm27		

   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm28, %%zmm28		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm29, %%zmm29		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm30, %%zmm30		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm31, %%zmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_8x32 									

   vmovups 		%%zmm16, (%%r10)        		
   vmovups 		%%zmm17, 64(%%r10)        		
   vmovups 		%%zmm18, (%%r11)        		
   vmovups 		%%zmm19, 64(%%r11)        		
   vmovups 		%%zmm20, (%%r12)        		
   vmovups 		%%zmm21, 64(%%r12)        		
   vmovups 		%%zmm22, (%%r13)        		
   vmovups 		%%zmm23, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%zmm24, (%%r10)        		
   vmovups 		%%zmm25, 64(%%r10)        		
   vmovups 		%%zmm26, (%%r11)        		
   vmovups 		%%zmm27, 64(%%r11)        		
	subq 			$8, %%rdi 						
   vmovups 		%%zmm28, (%%r12)        		
   vmovups 		%%zmm29, 64(%%r12)        		
   vmovups 		%%zmm30, (%%r13)        		
   vmovups 		%%zmm31, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//---------------------------------------------------------------

.macro	KERNEL4x32_K1								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm24 		
	addq  			$128, %%rbx 					
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm25 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm4, %%zmm26 		
	addq  			$16, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm5, %%zmm27 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm2, %%zmm4, %%zmm28 		
   vmovups 		(%%rbx), %%zmm6       			
   vfmadd231ps		%%zmm2, %%zmm5, %%zmm29 		

   vmovups 		64(%%rbx), %%zmm7       		
   vfmadd231ps		%%zmm3, %%zmm4, %%zmm30 		
   vbroadcastss	4(%%rax), %%zmm1    			
   vfmadd231ps		%%zmm3, %%zmm5, %%zmm31 		
   prefetcht0 		64(%%rbx)                 		

.endm 												

.macro	KERNEL4x32_K2								

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
	addq  			$128, %%rbx 					
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
	addq  			$16, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	(%%rax), %%zmm0    				
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28 		
   vmovups 		(%%rbx), %%zmm4       			
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

	vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
   vmovups 		64(%%rbx), %%zmm5       		
	vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		
   vbroadcastss	4(%%rax), %%zmm1    			
   prefetcht0 		64(%%rbx)                 		

.endm 												

.macro	KERNEL4x32_END_K							

   vbroadcastss	8(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm6, %%zmm24 		
   vfmadd231ps		%%zmm0, %%zmm7, %%zmm25 		

   vbroadcastss	12(%%rax), %%zmm3    			
   vfmadd231ps		%%zmm1, %%zmm6, %%zmm26 		
	addq  			$16, %%rax 						
   vfmadd231ps		%%zmm1, %%zmm7, %%zmm27 		

   vfmadd231ps		%%zmm2, %%zmm6, %%zmm28 		
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm29 		

	vfmadd231ps		%%zmm3, %%zmm6, %%zmm30 		
	vfmadd231ps		%%zmm3, %%zmm7, %%zmm31 		

.endm 												

.macro	ADD_C_4x32									

   vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm24, %%zmm24		
   vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm25, %%zmm25		
   vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm26, %%zmm26		
   vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm27, %%zmm27		

   vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm28, %%zmm28		
   vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm29, %%zmm29		
   vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm30, %%zmm30		
   vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm31, %%zmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_4x32 									

   vmovups 		%%zmm24, (%%r10)        		
   vmovups 		%%zmm25, 64(%%r10)        		
   vmovups 		%%zmm26, (%%r11)        		
   vmovups 		%%zmm27, 64(%%r11)        		
	subq 			$4, %%rdi 						
   vmovups 		%%zmm28, (%%r12)        		
   vmovups 		%%zmm29, 64(%%r12)        		
   vmovups 		%%zmm30, (%%r13)        		
   vmovups 		%%zmm31, 64(%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												


//-----------------------------------------------------------------

.macro	KERNEL1x32_K1
   vbroadcastss	4(%%rax), %%zmm2    			
   vfmadd231ps		%%zmm0, %%zmm4, %%zmm24 		
	addq  			$128, %%rbx
   vmovups 		(%%rbx), %%zmm6       					
   prefetcht0 		64(%%rbx)  					
   vfmadd231ps		%%zmm0, %%zmm5, %%zmm25 		
   					
	addq  			$4, %%rax
   vmovups 		64(%%rbx), %%zmm7
   prefetcht0 		64(%%rax) 						
.endm

.macro	KERNEL1x32_K2
   vbroadcastss	4(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm24 		
	addq  			$128, %%rbx
   vmovups 		(%%rbx), %%zmm4       					
   prefetcht0 		64(%%rbx)  					
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm25 		
   					
	addq  			$4, %%rax
   vmovups 		64(%%rbx), %%zmm5
   prefetcht0 		64(%%rax)
.endm

.macro	KERNEL1x32_END_K
   vbroadcastss	4(%%rax), %%zmm0    			
   vfmadd231ps		%%zmm2, %%zmm6, %%zmm24 		      									
   vfmadd231ps		%%zmm2, %%zmm7, %%zmm25 		
   					
	addq  			$4, %%rax

.endm

.macro	ADD_C_1x32
   vmovups 		(%%r10), %%zmm10        			
	vaddps 			%%zmm10, %%zmm24, %%zmm24		
   vmovups 		64(%%r10), %%zmm11        		
	vaddps 			%%zmm11, %%zmm25, %%zmm25				

	mov  	%%rcx, %%r10 							 // C0
.endm

.macro	SAVE_1x32
   vmovups 		%%zmm24, (%%r10)        		
   vmovups 		%%zmm25, 64(%%r10)        		      		
	subq 			$1, %%rdi 						       		

	leaq  	(%%r10, %%r8, 4), %%rcx 		 // next C0
.endm

//-----------------------------------------------------------------

SMM_NN_KERNEL12x32:								

   mov 	%[C], %%rcx   	 						
   mov 	%[A], %%rax   	 						
   mov 	%[B], %%rbx   	 						

   prefetcht0 		(%%rax)                 		

	mov 	%[K], %%rdx 							 // K(kc)
	mov  	%[LN], %%r8 							
	mov  	%[Bc], %%r14 							
	movq  %[M], %%rdi 							
	mov 	%[k_tag], %%r15 						 // kk=0把C存回内存, 否则加回对应的C位置

   prefetcht0 		(%%rbx)                 		
	mov 	%%rbx, %%r9 							 // B
	mov 	%%rdx, %%rsi 							 // K

BEGIN_PACK:										

	mov 	%%r9, %%rbx 							 // B
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 			 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 			 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 			 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
	mov 	%%r14, %%rbp 							 // Bc

   vmovups		(%%rbx), %%zmm4 // B0-15        				
	vpxorq 		%%zmm8, %%zmm8, %%zmm8 				
	vpxorq 		%%zmm9, %%zmm9, %%zmm9 				
	vpxorq 		%%zmm10, %%zmm10, %%zmm10 			
	vpxorq 		%%zmm11, %%zmm11, %%zmm11 			
   vmovups 	64(%%rbx), %%zmm5  // B16-31      			

	vpxorq 		%%zmm12, %%zmm12, %%zmm12 			
	vpxorq 		%%zmm13, %%zmm13, %%zmm13 			
	vpxorq 		%%zmm14, %%zmm14, %%zmm14 			
	vpxorq 		%%zmm15, %%zmm15, %%zmm15 			
	vpxorq 		%%zmm16, %%zmm16, %%zmm16 			
	vpxorq 		%%zmm17, %%zmm17, %%zmm17			
	vpxorq 		%%zmm18, %%zmm18, %%zmm18 			
	vpxorq 		%%zmm19, %%zmm19, %%zmm19 			

   vbroadcastss	(%%rax), %%zmm0 // A0   				
   vbroadcastss	4(%%rax), %%zmm1// A1    			

	vpxorq 		%%zmm20, %%zmm20, %%zmm20 			
	vpxorq 		%%zmm21, %%zmm21, %%zmm21 			
   prefetcht1 		64(%%r10)                 		
	vpxorq 		%%zmm22, %%zmm22, %%zmm22 			
	vpxorq 		%%zmm23, %%zmm23, %%zmm23 			
   prefetcht1 		64(%%r11)                 		
	vpxorq 		%%zmm24, %%zmm24, %%zmm24 			
	vpxorq 		%%zmm25, %%zmm25, %%zmm25			
   prefetcht1 		64(%%r12)                 		
	vpxorq 		%%zmm26, %%zmm26, %%zmm26 			
	vpxorq 		%%zmm27, %%zmm27, %%zmm27 			
   prefetcht1 		64(%%r13)                 		
	vpxorq 		%%zmm28, %%zmm28, %%zmm28 			
	vpxorq 		%%zmm29, %%zmm29, %%zmm29			
	vpxorq 		%%zmm30, %%zmm30, %%zmm30 			
	vpxorq 		%%zmm31, %%zmm31, %%zmm31 	//C:zmm8-31(24个)

	subq 	$8, %%rdx 		// K-=8, cant process K<8 and K%8<>0					

PACK_K_PREFETCH_C:
   leaq 	(%%r13, %%r8, 4), %%r13
   prefetcht1 		(%%r13)
   prefetcht1 		64(%%r13)

MAIN_PACK_K:										

	KERNEL12x32_PACK_K1 							
	KERNEL12x32_PACK_K2 							
	KERNEL12x32_PACK_K1 							
	KERNEL12x32_PACK_K2 							
	KERNEL12x32_PACK_K1 							
	KERNEL12x32_PACK_K2 							
	KERNEL12x32_PACK_K1 							
   cmp 	$0, %%rdx           					
	je 		EDGE_PACK_K
	KERNEL12x32_PACK_K2 							
	subq 	$8, %%rdx
   cmp   $64, %%rdx
   jbe 	PACK_K_PREFETCH_C	 								
   jmp     MAIN_PACK_K       						

EDGE_PACK_K:										
   leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL12x32_PACK_END_K 							
	jmp  	BEGIN_SAVE 								

//-----------------------------------------------------------------

BEGIN_M:											

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K								

   vmovups		(%%rbx), %%zmm4    //B0-15    				
	vpxorq 		%%zmm8, %%zmm8, %%zmm8 				
	vpxorq 		%%zmm9, %%zmm9, %%zmm9 				
	vpxorq 		%%zmm10, %%zmm10, %%zmm10 			
	vpxorq 		%%zmm11, %%zmm11, %%zmm11 			
   vmovups 	64(%%rbx), %%zmm5     //B16-31  			

	vpxorq 		%%zmm12, %%zmm12, %%zmm12 			
	vpxorq 		%%zmm13, %%zmm13, %%zmm13 			
	vpxorq 		%%zmm14, %%zmm14, %%zmm14 			
	vpxorq 		%%zmm15, %%zmm15, %%zmm15 			
	vpxorq 		%%zmm16, %%zmm16, %%zmm16 			
	vpxorq 		%%zmm17, %%zmm17, %%zmm17			
	vpxorq 		%%zmm18, %%zmm18, %%zmm18 			
	vpxorq 		%%zmm19, %%zmm19, %%zmm19 			

   vbroadcastss	(%%rax), %%zmm0    	// A0			
   vbroadcastss	4(%%rax), %%zmm1    	// A1		

	vpxorq 		%%zmm20, %%zmm20, %%zmm20 			
	vpxorq 		%%zmm21, %%zmm21, %%zmm21 			
	vpxorq 		%%zmm22, %%zmm22, %%zmm22 			
	vpxorq 		%%zmm23, %%zmm23, %%zmm23 			
	vpxorq 		%%zmm24, %%zmm24, %%zmm24 			
	vpxorq 		%%zmm25, %%zmm25, %%zmm25			
	vpxorq 		%%zmm26, %%zmm26, %%zmm26 			
	vpxorq 		%%zmm27, %%zmm27, %%zmm27 			
	vpxorq 		%%zmm28, %%zmm28, %%zmm28 			
	vpxorq 		%%zmm29, %%zmm29, %%zmm29			
	vpxorq 		%%zmm30, %%zmm30, %%zmm30 			
	vpxorq 		%%zmm31, %%zmm31, %%zmm31 			

	subq 	$8, %%rdx 

K_M12_PREFETCH_C:
   leaq 	(%%r13, %%r8, 4), %%r13
   prefetcht1 		(%%r13)
   prefetcht1 		64(%%r13)

MAIN_K_M12:											

	KERNEL12x32_K1 									
	KERNEL12x32_K2 									
	KERNEL12x32_K1 									
	KERNEL12x32_K2 									
	KERNEL12x32_K1 									
	KERNEL12x32_K2 									
	KERNEL12x32_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K									
	KERNEL12x32_K2
   subq 	$8, %%rdx  									
   cmp   $64, %%rdx
   jbe 	K_M12_PREFETCH_C			
   jmp     MAIN_K_M12       							

EDGE_K:											
   leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL12x32_END_K 								

BEGIN_SAVE:										
	cmp 	$0, %%r15								
	je  	SAVE_C 									
	ADD_C_12x32 									

SAVE_C: 											
	SAVE_12x32 										
	cmpq  	$12, %%rdi 								
	jnb 	BEGIN_M 					//不小于（或等于）则跳转			

//-----------------------------------------------------------------

BEGIN_M8:											
   cmpq  	$8, %%rdi     // M % 8
	jb   	BEGIN_M4         //小于则跳转!!!改了

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%zmm4        				
	vpxorq 		%%zmm16, %%zmm16, %%zmm16 			
	vpxorq 		%%zmm17, %%zmm17, %%zmm17			
   vmovups 	64(%%rbx), %%zmm5       			
	vpxorq 		%%zmm18, %%zmm18, %%zmm18 			
	vpxorq 		%%zmm19, %%zmm19, %%zmm19 			

   vbroadcastss	(%%rax), %%zmm0    				
   vbroadcastss	4(%%rax), %%zmm1    			

	vpxorq 		%%zmm20, %%zmm20, %%zmm20 			
	vpxorq 		%%zmm21, %%zmm21, %%zmm21 			
   prefetcht1 		64(%%r10)                 		
	vpxorq 		%%zmm22, %%zmm22, %%zmm22 			
	vpxorq 		%%zmm23, %%zmm23, %%zmm23 			
   prefetcht1 		64(%%r11)                 		
	vpxorq 		%%zmm24, %%zmm24, %%zmm24 			
	vpxorq 		%%zmm25, %%zmm25, %%zmm25			
   prefetcht1 		64(%%r12)                 		
	vpxorq 		%%zmm26, %%zmm26, %%zmm26 			
	vpxorq 		%%zmm27, %%zmm27, %%zmm27 			
   prefetcht1 		64(%%r13)                 		
	vpxorq 		%%zmm28, %%zmm28, %%zmm28 			
	vpxorq 		%%zmm29, %%zmm29, %%zmm29			
	vpxorq 		%%zmm30, %%zmm30, %%zmm30 			
	vpxorq 		%%zmm31, %%zmm31, %%zmm31 			

	subq 	$8, %%rdx 								

K_M8_PREFETCH_C:
   leaq 	(%%r13, %%r8, 4), %%r13
   prefetcht1 		(%%r13)
   prefetcht1 		64(%%r13)

MAIN_K_M8:											

	KERNEL8x32_K1 									
	KERNEL8x32_K2 									
	KERNEL8x32_K1 									
	KERNEL8x32_K2 									
	KERNEL8x32_K1 									
	KERNEL8x32_K2 									
	KERNEL8x32_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M8 								
	KERNEL8x32_K2 									
	subq 	$8, %%rdx
   cmp   $32, %%rdx
   jbe 	K_M8_PREFETCH_C								
   jmp     MAIN_K_M8       						

EDGE_K_M8:											
   leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL8x32_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_8x32 							
	ADD_C_8x32 	
									
SAVE_C_8x32: 										
	SAVE_8x32 										

//----------------------------------------------------------------

BEGIN_M4:											

	cmpq  	$4, %%rdi     // M % 4
	jb   	BEGIN_M1         //小于则跳转!!!改了									

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%zmm4        				
   vmovups 	64(%%rbx), %%zmm5       			

   prefetcht1 		64(%%r10)                 		
	vpxorq 		%%zmm24, %%zmm24, %%zmm24 			
	vpxorq 		%%zmm25, %%zmm25, %%zmm25			
   vbroadcastss	(%%rax), %%zmm0    				
   prefetcht1 		64(%%r11)                 		
	vpxorq 		%%zmm26, %%zmm26, %%zmm26 			
	vpxorq 		%%zmm27, %%zmm27, %%zmm27 			
   vbroadcastss	4(%%rax), %%zmm1    			
   prefetcht1 		64(%%r12)                 		
	vpxorq 		%%zmm28, %%zmm28, %%zmm28 			
	vpxorq 		%%zmm29, %%zmm29, %%zmm29			
   prefetcht1 		64(%%r13)                 		
	vpxorq 		%%zmm30, %%zmm30, %%zmm30 			
	vpxorq 		%%zmm31, %%zmm31, %%zmm31 			

	subq 	$8, %%rdx 								

MAIN_K_M4:											

	KERNEL4x32_K1 									
	KERNEL4x32_K2 									
	KERNEL4x32_K1 									
	KERNEL4x32_K2 									
	KERNEL4x32_K1 									
	KERNEL4x32_K2 									
	KERNEL4x32_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4 								
	KERNEL4x32_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M4       						

EDGE_K_M4:											

	KERNEL4x32_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_4x32 							
	ADD_C_4x32 										

SAVE_C_4x32: 										
	SAVE_4x32 										

//----------------------------------------------------------------


BEGIN_M1:
	cmpq  	$1, %%rdi 
	jb   	END_M 			//小于则跳转						

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		                 		                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%zmm4//B0-15        				
   vmovups 	64(%%rbx), %%zmm5 //B16-31      			

   prefetcht1 		64(%%r10)                 		
	vpxorq 		%%zmm24, %%zmm24, %%zmm24 			 
	vpxorq 		%%zmm25, %%zmm25, %%zmm25			
   vbroadcastss	(%%rax), %%zmm0 //A0   				 						

	subq 	$8, %%rdx 					

MAIN_K_M1:
	KERNEL1x32_K1 									
	KERNEL1x32_K2 									
	KERNEL1x32_K1 									
	KERNEL1x32_K2 									
	KERNEL1x32_K1 									
	KERNEL1x32_K2 									
	KERNEL1x32_K1 									
   cmp 	$0, %%rdx           					
	je    EDGE_K_M1 								
	KERNEL1x32_K2 									

	subq 	$8, %%rdx 								
   jmp   MAIN_K_M1 

EDGE_K_M1:
   KERNEL1x32_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_1x32 							
	ADD_C_1x32

SAVE_C_1x32:
   SAVE_1x32

   cmpq  	$1, %%rdi 								
	jnb 	BEGIN_M1 					//不小于（或等于）则跳转

//----------------------------------------------------------------

END_M:												
