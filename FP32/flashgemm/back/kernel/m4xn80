.macro	KERNEL4x80_K1_MGN								

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm4, %%zmm8 			
	vfmadd231ps		%%zmm0, %%zmm5, %%zmm9 			

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm4, %%zmm10 		
	vfmadd231ps		%%zmm1, %%zmm5, %%zmm11 		

	prefetcht0 		256(%%rax)

	addq  			$16, %%rax
	addq  			$128, %%rbx	
							
	vbroadcastss	(%%rax), %%zmm0
	vmovups 		(%%rbx), %%zmm6
	vfmadd231ps		%%zmm2, %%zmm4, %%zmm12 		
	vfmadd231ps		%%zmm2, %%zmm5, %%zmm13 		

	vbroadcastss	4(%%rax), %%zmm1
	vmovups 		64(%%rbx), %%zmm7
	vfmadd231ps		%%zmm3, %%zmm4, %%zmm14 		
	vfmadd231ps		%%zmm3, %%zmm5, %%zmm15 		

	prefetcht0 		256(%%rbx) 		   					

.endm 												

.macro	KERNEL4x80_K2_MGN								

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
	vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
	vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		

	prefetcht0 		256(%%rax)

	addq  			$16, %%rax
	addq  			$128, %%rbx                  		

	vbroadcastss	(%%rax), %%zmm0
	vmovups 		(%%rbx), %%zmm4    			
	vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
	vfmadd231ps		%%zmm2, %%zmm7, %%zmm13

	vbroadcastss	4(%%rax), %%zmm1
	vmovups 		64(%%rbx), %%zmm5    			
	vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
	vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 		

	prefetcht0 		256(%%rbx)                 				
    					
.endm 												

.macro	KERNEL4x80_END_K_MGN							

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			
	vfmadd231ps		%%zmm0, %%zmm7, %%zmm9 			

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		
	vfmadd231ps		%%zmm1, %%zmm7, %%zmm11 		
   			
	vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		
	vfmadd231ps		%%zmm2, %%zmm7, %%zmm13 		

	vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		
	vfmadd231ps		%%zmm3, %%zmm7, %%zmm15 				

	addq  			$128, %%rbx 	// TODO							

.endm 												

.macro	ADD_C_4x80_MGN									

	vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm8, %%zmm8			
	vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm9, %%zmm9			
	vmovups 		(%%r11), %%zmm2        			
	vaddps 			%%zmm2, %%zmm10, %%zmm10		
	vmovups 		64(%%r11), %%zmm3        		
	vaddps 			%%zmm3, %%zmm11, %%zmm11		
	vmovups 		(%%r12), %%zmm4        			
	vaddps 			%%zmm4, %%zmm12, %%zmm12		
	vmovups 		64(%%r12), %%zmm5        		
	vaddps 			%%zmm5, %%zmm13, %%zmm13		
	vmovups 		(%%r13), %%zmm6        			
	vaddps 			%%zmm6, %%zmm14, %%zmm14		
	vmovups 		64(%%r13), %%zmm7        		
	vaddps 			%%zmm7, %%zmm15, %%zmm15						

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_4x80_MGN 									

	vmovups 		%%zmm8, (%%r10)        			
	vmovups 		%%zmm9, 64(%%r10)        		
	vmovups 		%%zmm10, (%%r11)        		
	vmovups 		%%zmm11, 64(%%r11)        		
	vmovups 		%%zmm12, (%%r12)        		
	vmovups 		%%zmm13, 64(%%r12)        		
	vmovups 		%%zmm14, (%%r13)        		
	vmovups 		%%zmm15, 64(%%r13)        		        		
			
	subq 		$32, %%rdi 					// TODO
	addq  	   	$128, %%rcx 				// C0

.endm 												

//-----------------------------------------------------------------

.macro	KERNEL4x64_K1_MGN								

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm4, %%zmm8 			 			

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm4, %%zmm10 		 		

	prefetcht0 		256(%%rax) 
	addq  			$16, %%rax
	addq  			$64, %%rbx

	vbroadcastss	(%%rax), %%zmm0 
	vmovups 		(%%rbx), %%zmm6     			
	vfmadd231ps		%%zmm2, %%zmm4, %%zmm12 		 		

	vbroadcastss	4(%%rax), %%zmm1    			
	vfmadd231ps		%%zmm3, %%zmm4, %%zmm14 		 		

	prefetcht0 		256(%%rbx)                 		
   			 		
.endm 												

.macro	KERNEL4x64_K2_MGN								

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 						

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		 		

	prefetcht0 		256(%%rax)
	addq  			$16, %%rax
	addq  			$64, %%rbx                		

	vbroadcastss	(%%rax), %%zmm0
	vmovups 		(%%rbx), %%zmm4    			
	vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 		 		

	vbroadcastss	4(%%rax), %%zmm1    			
	vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 		 		

	prefetcht0 		256(%%rbx)                 		 			

.endm 												

.macro	KERNEL4x64_END_K_MGN							

	vbroadcastss	8(%%rax), %%zmm2    			
	vfmadd231ps		%%zmm0, %%zmm6, %%zmm8 			 			

	vbroadcastss	12(%%rax), %%zmm3    			
	vfmadd231ps		%%zmm1, %%zmm6, %%zmm10 		 		

	prefetcht0 		256(%%rax)                 		
   			
	vfmadd231ps		%%zmm2, %%zmm6, %%zmm12 				
   			
	vfmadd231ps		%%zmm3, %%zmm6, %%zmm14 				 				

	addq  			$64, %%rbx 	// TODO					
 		
.endm 												

.macro	ADD_C_4x64_MGN									

	vmovups 	(%%r10), %%zmm0        			
	vaddps 		%%zmm0, %%zmm8, %%zmm8			
	vmovups 	(%%r11), %%zmm2        			
	vaddps 		%%zmm2, %%zmm10, %%zmm10		
	vmovups 	(%%r12), %%zmm4        			
	vaddps 		%%zmm4, %%zmm12, %%zmm12		
	vmovups 	(%%r13), %%zmm6        			
	vaddps 		%%zmm6, %%zmm14, %%zmm14									

	mov			%%rcx, %%r10						// C0
	leaq		(%%r10, %%r8, 4), %%r11 			// C1
	leaq		(%%r11, %%r8, 4), %%r12 			// C2
	leaq		(%%r12, %%r8, 4), %%r13 			// C3

.endm 												

.macro	SAVE_4x64_MGN 									

	vmovups 		%%zmm8, (%%r10)        			        		
	vmovups 		%%zmm10, (%%r11)        		        		
	vmovups 		%%zmm12, (%%r12)        		        		
	vmovups 		%%zmm14, (%%r13)        		        		       		 		
       		       		
	subq 			$16, %%rdi
	addq  	   		$64, %%rcx 				// C0

.endm 		

//-----------------------------------------------------------------

.macro	KERNEL4x16_K1_MGN								

	movss			8(%%rax), %%xmm2    			
	vfmadd231ps		%%xmm0, %%xmm4, %%xmm8 			 			

	movss			12(%%rax), %%xmm3    			
	vfmadd231ps		%%xmm1, %%xmm4, %%xmm10 		 		

	prefetcht0 		256(%%rax)
	addq  			$16, %%rax 	
	addq  			$4, %%rbx 		// TODO                 		

	movss			(%%rax), %%xmm0
	movss 		   	(%%rbx), %%xmm6    			
	vfmadd231ps		%%xmm2, %%xmm4, %%xmm12 		 		

	movss			4(%%rax), %%xmm1    			
	vfmadd231ps		%%xmm3, %%xmm4, %%xmm14 		 		
			
	prefetcht0 		256(%%rbx)					 				 		

.endm 												

.macro	KERNEL4x16_K2_MGN								

	movss			8(%%rax), %%xmm2    			
	vfmadd231ps		%%xmm0, %%xmm6, %%xmm8 						

	movss			12(%%rax), %%xmm3    			
	vfmadd231ps		%%xmm1, %%xmm6, %%xmm10 		 		

	prefetcht0 		256(%%rax) 
	addq  			$16, %%rax
	addq  			$4, %%rbx 		// TODO	                		

	movss			(%%rax), %%xmm0
	movss 			(%%rbx), %%xmm4    			
	vfmadd231ps		%%xmm2, %%xmm6, %%xmm12 		 		

	movss			4(%%rax), %%xmm1    			
	vfmadd231ps		%%xmm3, %%xmm6, %%xmm14 		 				 		

	prefetcht0 		256(%%rbx)                 		
   			
.endm 												

.macro	KERNEL4x16_END_K_MGN							

	movss			8(%%rax), %%xmm2    			
	vfmadd231ps		%%xmm0, %%xmm6, %%xmm8 			 			

	movss			12(%%rax), %%xmm3    			
	vfmadd231ps		%%xmm1, %%xmm6, %%xmm10 		 		

	prefetcht0 		256(%%rax)                 		
   			
	vfmadd231ps		%%xmm2, %%xmm6, %%xmm12 				
	vfmadd231ps		%%xmm3, %%xmm6, %%xmm14 				
				
	addq  			$4, %%rbx 	// TODO							

.endm 												

.macro	ADD_C_4x16_MGN									

	movss 			(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm8, %%xmm8			
	movss 			(%%r11), %%xmm2        			
	vaddps 			%%xmm2, %%xmm10, %%xmm10		
	movss 			(%%r12), %%xmm4        			
	vaddps 			%%xmm4, %%xmm12, %%xmm12		
	movss 			(%%r13), %%xmm6        			
	vaddps 			%%xmm6, %%xmm14, %%xmm14									

	mov         	%%rcx, %%r10 						// C0
	leaq 	      	(%%r10, %%r8, 4), %%r11 			// C1
	leaq 	      	(%%r11, %%r8, 4), %%r12 			// C2
	leaq     		(%%r12, %%r8, 4), %%r13 			// C3

.endm 												

.macro	SAVE_4x16_MGN 									

	vmovss 		%%xmm8, (%%r10)        		       		
	vmovss 		%%xmm10, (%%r11)        		        		
	vmovss 		%%xmm12, (%%r12)        		       		
	vmovss 		%%xmm14, (%%r13)        		 		
			
	subq 		$1, %%rdi
	addq  	   	$4, %%rcx

.endm 		


//-----------------------------------------------------------------
// pack A
   movl %[LK], %%r8d    
   movl %[LK], %%r15d    

   mov %[Ac], %%r9    
   movl %[K], %%r10d   
   mov %[A], %%r11    
   mov %[A], %%r12    
   mov %[A], %%r13    
   mov %[A], %%r14    

   shr $4, %%r10    //K循环边界   
   shl $2, %%r8       
   shl $3, %%r15       

   add %%r8, %%r14     
   add %%r8, %%r12     
   add %%r15, %%r13     
   add %%r15, %%r14     

   vmovups (%%r11), %%zmm2       
   vmovups (%%r12), %%zmm3       
   vmovups (%%r13), %%zmm4       
   vmovups (%%r14), %%zmm5       
   jmp NPACK_M4_MGN                   

NPACK_Pre_M4_MGN:              
   add $64, %%r11      //K维度上后16个元素位置
   add $64, %%r12      //K维度上后16个元素位置      
   add $64, %%r13      //K维度上后16个元素位置      
   add $64, %%r14      //K维度上后16个元素位置      
   add $256, %%r9      //Ac指针后移 
   vmovups (%%r11), %%zmm2       
   vmovups (%%r12), %%zmm3       
   vmovups (%%r13), %%zmm4       
   vmovups (%%r14), %%zmm5       

NPACK_M4_MGN:                 

   movl    $0xaa, %%eax       
   kmovd   %%eax, %%k1            
   vunpcklps %%zmm3, %%zmm2, %%zmm0        //2元素连续
   vunpcklps %%zmm5, %%zmm4, %%zmm26    

   vpermq   $0x80, %%zmm26, %%zmm0%{%%k1%} //4元素连续     
   vmovups %%xmm0, (%%r9)         
   vextractf32x4  $0x1, %%zmm0, %%xmm28      
   vextractf32x4  $0x2, %%zmm0, %%xmm29      
   vextractf32x4  $0x3, %%zmm0, %%xmm30      

   vmovups %%xmm28, 64(%%r9)         
   vmovups %%xmm29, 128(%%r9)         
   vmovups %%xmm30, 192(%%r9)         

   movl    $0x55, %%eax       
   kmovd   %%eax, %%k1            

   vunpcklps %%zmm3, %%zmm2, %%zmm0    
   vunpcklps %%zmm5, %%zmm4, %%zmm26    

   vpermq  $0x31, %%zmm0, %%zmm26%{%%k1%}      
   vmovups %%xmm26, 16(%%r9)         
   vextractf32x4  $0x1, %%zmm26, %%xmm28      
   vextractf32x4  $0x2, %%zmm26, %%xmm29      
   vextractf32x4  $0x3, %%zmm26, %%xmm30      

   vmovups %%xmm28, 80(%%r9)         
   vmovups %%xmm29, 144(%%r9)         
   vmovups %%xmm30, 208(%%r9)         

   movl    $0xaa, %%eax       
   kmovd   %%eax, %%k1            
   vunpckhps %%zmm3, %%zmm2, %%zmm0    
   vunpckhps %%zmm5, %%zmm4, %%zmm26    
   vpermq   $0x80, %%zmm26, %%zmm0%{%%k1%}      
   vmovups %%xmm0, 32(%%r9)         
   vextractf32x4  $0x1, %%zmm0, %%xmm28      
   vextractf32x4  $0x2, %%zmm0, %%xmm29      
   vextractf32x4  $0x3, %%zmm0, %%xmm30      

   vmovups %%xmm28, 96(%%r9)         
   vmovups %%xmm29, 160(%%r9)         
   vmovups %%xmm30, 224(%%r9)         
   movl    $0x55, %%eax       
   kmovd   %%eax, %%k1            

   vunpckhps %%zmm3, %%zmm2, %%zmm0    
   vunpckhps %%zmm5, %%zmm4, %%zmm26    

   vpermq  $0x31, %%zmm0, %%zmm26%{%%k1%}      
   vmovups %%xmm26, 48(%%r9)         
   vextractf32x4  $0x1, %%zmm26, %%xmm28      
   vextractf32x4  $0x2, %%zmm26, %%xmm29      
   vextractf32x4  $0x3, %%zmm26, %%xmm30      
   sub    $1, %%r10       
   vmovups %%xmm28, 112(%%r9)         
   vmovups %%xmm29, 176(%%r9)         
   vmovups %%xmm30, 240(%%r9)         

   je  NPACK_END_M4_MGN           
   jmp NPACK_Pre_M4_MGN           

NPACK_END_M4_MGN:                         
               
//-----------------------------------------------------------------

SMM_KERNEL4x80_MGN:								

	mov 	%[C], %%rcx   	 						
	mov 	%[Ac], %%rax   	 						
	mov 	%[B], %%rbx   	 						

	prefetcht0 		(%%rax)             

	mov 	%[K], %%rdx 							 // K(kc)
	mov  	%[LN], %%r8
	mov  	%[Ac], %%r14                      // 存储Ac地址 							 							
	movq  	%[N], %%rdi 
	mov 	%[k_tag], %%r15 						 // kk=0把C存回内存, 否则加回对应的C位置

	prefetcht0 		(%%rbx)
	mov 	%%rdx, %%rsi 							 // K

//-----------------------------------------------------------------

BEGIN_M4N32_MGN:	
   	cmpq     $32, %%rdi         // N % 32
	jb   	   BEGIN_M4N16_MGN

	mov 	%%r14, %%rax                     // Ac
   	prefetcht0 		(%%rax)

	mov  	%%rcx, %%r10                     // C0
   	prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11          // C1
   	prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12          // C2
   	prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13          // C3
   	prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx                     // K								

   	vmovups			(%%rbx), %%zmm4            // B0-15
	vbroadcastss	(%%rax), %%zmm0    	// A0	    				
	vpxorq 			%%zmm8, %%zmm8, %%zmm8 				
	vpxorq 			%%zmm9, %%zmm9, %%zmm9 				
	vpxorq 			%%zmm10, %%zmm10, %%zmm10 			
	vpxorq 			%%zmm11, %%zmm11, %%zmm11

   	vmovups     	64(%%rbx), %%zmm5
	vbroadcastss	4(%%rax), %%zmm1    	// A1 		 			
	vpxorq 			%%zmm12, %%zmm12, %%zmm12 			
	vpxorq 			%%zmm13, %%zmm13, %%zmm13 			
	vpxorq 			%%zmm14, %%zmm14, %%zmm14 			
	vpxorq 			%%zmm15, %%zmm15, %%zmm15

	subq 	$8, %%rdx 

K_M4N32_PREFETCH_C_MGN:
   	leaq 	(%%r13, %%r8, 4), %%r13
   	prefetcht2 		(%%r13)
   	prefetcht2 		64(%%r13)

MAIN_K_M4N32_MGN:											

	KERNEL4x80_K1_MGN 									
	KERNEL4x80_K2_MGN 									
	KERNEL4x80_K1_MGN 									
	KERNEL4x80_K2_MGN 									
	KERNEL4x80_K1_MGN 									
	KERNEL4x80_K2_MGN 									
	KERNEL4x80_K1_MGN 									
   	cmp 	$0, %%rdx           					
	je 		EDGE_K_M4N32_MGN									
	KERNEL4x80_K2_MGN
   	subq 	$8, %%rdx  									
   	cmp   	$64, %%rdx
   	jbe 	K_M4N32_PREFETCH_C_MGN			
   	jmp     MAIN_K_M4N32_MGN       							

EDGE_K_M4N32_MGN:											
   	leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL4x80_END_K_MGN 								

BEGIN_SAVE_M4N32_MGN:										
	cmp 	$0, %%r15								
	je  	SAVE_C_M4N32_MGN 									
	ADD_C_4x80_MGN 									

SAVE_C_M4N32_MGN: 											
	SAVE_4x80_MGN 										
	cmpq  	$32, %%rdi 								
	jnb 	BEGIN_M4N32_MGN 					//不小于（或等于）则跳转			

//----------------------------------------------------------------

BEGIN_M4N16_MGN:		
   	cmpq     $16, %%rdi         // N % 16
	jb   	   BEGIN_M4N1_MGN            // TODO 

	mov 	%%r14, %%rax                     // Ac
   	prefetcht0 		(%%rax)

	mov  	%%rcx, %%r10                     // C0
   	prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11          // C1
   	prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12          // C2
   	prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13          // C3
   	prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx                     // K								

   	vmovups			(%%rbx), %%zmm4    	// B0-15
   	vbroadcastss	(%%rax), %%zmm0    	// A0			
	vbroadcastss	4(%%rax), %%zmm1    // A1

   	vpxorq 		%%zmm8, %%zmm8, %%zmm8 				
	vpxorq 		%%zmm10, %%zmm10, %%zmm10 			
	vpxorq 		%%zmm12, %%zmm12, %%zmm12 			
	vpxorq 		%%zmm14, %%zmm14, %%zmm14 			 						

	subq 	$8, %%rdx 

K_M4N16_PREFETCH_C_MGN:
   leaq 	(%%r13, %%r8, 4), %%r13
   prefetcht2 		(%%r13)
   prefetcht2 		64(%%r13)

MAIN_K_M4N16_MGN:											

	KERNEL4x64_K1_MGN 									
	KERNEL4x64_K2_MGN 									
	KERNEL4x64_K1_MGN 									
	KERNEL4x64_K2_MGN 									
	KERNEL4x64_K1_MGN 									
	KERNEL4x64_K2_MGN 									
	KERNEL4x64_K1_MGN 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4N16_MGN									
	KERNEL4x64_K2_MGN
   subq 	$8, %%rdx  									
   cmp   $32, %%rdx
   jbe 	K_M4N16_PREFETCH_C_MGN			
   jmp     MAIN_K_M4N16_MGN       							

EDGE_K_M4N16_MGN:											
   leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL4x64_END_K_MGN 																		
	cmp 	$0, %%r15								
	je  	SAVE_C_M4N16_MGN 									
	ADD_C_4x64_MGN 									

SAVE_C_M4N16_MGN: 											
	SAVE_4x64_MGN 										

//----------------------------------------------------------------

BEGIN_M4N1_MGN:		
   	cmpq     $1, %%rdi         // N % 16
	jb   	   END_M4N1_MGN            // TODO 

	mov 	%%r14, %%rax                     // Ac
   	prefetcht0 		(%%rax)

	mov  	%%rcx, %%r10                     // C0
   	prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11          // C1
   	prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12          // C2
   	prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13          // C3
   	prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx                     // K								

   	movss		(%%rbx), %%xmm4    	// B0
   	movss	(%%rax), %%xmm0    		// A0			
   	movss	4(%%rax), %%xmm1    	// A1	

   	vpxorq 		%%xmm8, %%xmm8, %%xmm8 				
	vpxorq 		%%xmm10, %%xmm10, %%xmm10 			
	vpxorq 		%%xmm12, %%xmm12, %%xmm12 			
	vpxorq 		%%xmm14, %%xmm14, %%xmm14 			 			 			

	subq 	$8, %%rdx 

K_M4N1_PREFETCH_C_MGN:
   leaq 	(%%r13, %%r8, 4), %%r13
   prefetcht2 		(%%r13)
   prefetcht2 		64(%%r13)

MAIN_K_M4N1_MGN:											

	KERNEL4x16_K1_MGN 									
	KERNEL4x16_K2_MGN 									
	KERNEL4x16_K1_MGN 									
	KERNEL4x16_K2_MGN 									
	KERNEL4x16_K1_MGN 									
	KERNEL4x16_K2_MGN 									
	KERNEL4x16_K1_MGN 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4N1_MGN									
	KERNEL4x16_K2_MGN
   subq 	$8, %%rdx  									
   cmp   $32, %%rdx
   jbe 	K_M4N1_PREFETCH_C_MGN			
   jmp     MAIN_K_M4N1_MGN       							

EDGE_K_M4N1_MGN:											
   leaq 	(%%r12, %%r8, 4), %%r13
	KERNEL4x16_END_K_MGN 																		
	cmp 	$0, %%r15								
	je  	SAVE_C_M4N1_MGN 									
	ADD_C_4x16_MGN 									

SAVE_C_M4N1_MGN: 											
	SAVE_4x16_MGN 	


//----------------------------------------------------------------

END_M4N1_MGN:												
