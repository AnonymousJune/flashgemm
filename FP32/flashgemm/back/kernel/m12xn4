.macro	KERNEL12x4_PACK_K1							

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm21 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm22 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm26 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm27 		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm29 		
   vmovups 		(%%rbx), %%xmm6        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm31 		
   vmovups 		%%xmm4, (%%rbp)       			
	addq  			$16, %%rbp                //64->16 						

.endm 												

.macro	KERNEL12x4_PACK_K2							

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm21 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   prefetcht0 		16(%%rbx)       //          		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
   vmovups 		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		
   vmovups 		%%xmm6, (%%rbp)       			
	addq  			$16, %%rbp                //64->16 						

.endm 												

.macro	KERNEL12x4_PACK_END_K						

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm21 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
	addq  			$48, %%rax 						

   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		
   vmovups 		%%xmm6, (%%rbp)       			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL12x4_K1								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm21 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm22 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm26 		

	addq  			$16, %%rbx 						 // B

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm27 		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm29 		
   vmovups 		(%%rbx), %%xmm6        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm31 		

.endm 												

.macro	KERNEL12x4_K2								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm21 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

	addq 			$16, %%rbx    //				

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   prefetcht0 		16(%%rbx)  //               		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
   vmovups 		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		

.endm 												

.macro	KERNEL12x4_END_K							

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm20 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm21 		

   vbroadcastss	16(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm23 		

   vbroadcastss	24(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	28(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   vbroadcastss	32(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

   vbroadcastss	36(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   vbroadcastss	40(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	44(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
   vmovups 		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		

.endm 												

.macro	ADD_C_12x4									

   vmovups 		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm20, %%xmm20		
   vmovups 		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm21, %%xmm21		
   vmovups 		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm22, %%xmm22		
   vmovups 		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm23, %%xmm23		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%xmm4        			
	vaddps 			%%xmm4, %%xmm24, %%xmm24		
   vmovups 		(%%r11), %%xmm5        			
	vaddps 			%%xmm5, %%xmm25, %%xmm25		
   vmovups 		(%%r12), %%xmm6        			
	vaddps 			%%xmm6, %%xmm26, %%xmm26		
   vmovups 		(%%r13), %%xmm7        			
	vaddps 			%%xmm7, %%xmm27, %%xmm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   vmovups 		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   vmovups 		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   vmovups 		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_12x4 									

   vmovups 		%%xmm20, (%%r10)        		
   vmovups 		%%xmm21, (%%r11)        		
	subq 			$12, %%rdi 						
   vmovups 		%%xmm22, (%%r12)        		
   vmovups 		%%xmm23, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%xmm24, (%%r10)        		
   vmovups 		%%xmm25, (%%r11)        		
   vmovups 		%%xmm26, (%%r12)        		
   vmovups 		%%xmm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%xmm28, (%%r10)        		
   vmovups 		%%xmm29, (%%r11)        		
   vmovups 		%%xmm30, (%%r12)        		
   vmovups 		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL8x4_K1								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm24 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%xmm8    			
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm26 		

   vbroadcastss	20(%%rax), %%xmm9    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm27 		

   vbroadcastss	24(%%rax), %%xmm10    			
   vfmadd231ps		%%xmm8, %%xmm4, %%xmm28 		

   prefetcht0 		16(%%rbx) //                		

   vbroadcastss	28(%%rax), %%xmm11    			
   vfmadd231ps		%%xmm9, %%xmm4, %%xmm29 		
	addq  			$16, %%rbx 						 //
   vfmadd231ps		%%xmm10, %%xmm4, %%xmm30 		

	addq  			$32, %%rax 						
   vmovups 		(%%rbx), %%xmm6        			
   vfmadd231ps		%%xmm11, %%xmm4, %%xmm31 		

   vbroadcastss	(%%rax), %%xmm0    				
   vbroadcastss	4(%%rax), %%xmm1    			

.endm 												

.macro	KERNEL8x4_K2								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%xmm8    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

   vbroadcastss	20(%%rax), %%xmm9    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   vbroadcastss	24(%%rax), %%xmm10    			
   vfmadd231ps		%%xmm8, %%xmm6, %%xmm28 		

   prefetcht0 		16(%%rbx)              //   		

   vbroadcastss	28(%%rax), %%xmm11    			
   vfmadd231ps		%%xmm9, %%xmm6, %%xmm29 		
	addq  			$16, %%rbx 						 // B
   vfmadd231ps		%%xmm10, %%xmm6, %%xmm30 		

	addq  			$32, %%rax 						
   vmovups 		(%%rbx), %%xmm4        			
   vfmadd231ps		%%xmm11, %%xmm6, %%xmm31 		

   vbroadcastss	(%%rax), %%xmm0    				
   vbroadcastss	4(%%rax), %%xmm1    			

.endm 												

.macro	KERNEL8x4_END_K							

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm24 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%xmm8    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm26 		

   vbroadcastss	20(%%rax), %%xmm9    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm27 		

   vbroadcastss	24(%%rax), %%xmm10    			
   vfmadd231ps		%%xmm8, %%xmm6, %%xmm28 		

   vbroadcastss	28(%%rax), %%xmm11    			
   vfmadd231ps		%%xmm9, %%xmm6, %%xmm29 		
   vfmadd231ps		%%xmm10, %%xmm6, %%xmm30 		

	addq  			$32, %%rax 						
   vfmadd231ps		%%xmm11, %%xmm6, %%xmm31 		

.endm 												

.macro	ADD_C_8x4									

   vmovups 		(%%r10), %%xmm4        			
	vaddps 			%%xmm4, %%xmm24, %%xmm24		
   vmovups 		(%%r11), %%xmm5        			
	vaddps 			%%xmm5, %%xmm25, %%xmm25		
   vmovups 		(%%r12), %%xmm6        			
	vaddps 			%%xmm6, %%xmm26, %%xmm26		
   vmovups 		(%%r13), %%xmm7        			
	vaddps 			%%xmm7, %%xmm27, %%xmm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   vmovups 		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   vmovups 		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   vmovups 		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_8x4 									

   vmovups 		%%xmm24, (%%r10)        		
   vmovups 		%%xmm25, (%%r11)        		
	subq 			$8, %%rdi 						
   vmovups 		%%xmm26, (%%r12)        		
   vmovups 		%%xmm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%xmm28, (%%r10)        		
   vmovups 		%%xmm29, (%%r11)        		
   vmovups 		%%xmm30, (%%r12)        		
   vmovups 		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL4x4_K1								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm28 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm4, %%xmm29 		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$16, %%rbx 						 // B
   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm4, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm4, %%xmm31 		

   prefetcht0 		16(%%rbx)                 		
   vmovups 		(%%rbx), %%xmm6        			

.endm 												

.macro	KERNEL4x4_K2								

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$16, %%rbx 						 // B
   vbroadcastss	(%%rax), %%xmm0    				
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		

   vbroadcastss	4(%%rax), %%xmm1    			
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		

   vmovups 		(%%rbx), %%xmm4        			
   prefetcht0 		16(%%rbx)                 		

.endm 												

.macro	KERNEL4x4_END_K							

   vbroadcastss	8(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm6, %%xmm28 		

   vbroadcastss	12(%%rax), %%xmm3    			
   vfmadd231ps		%%xmm1, %%xmm6, %%xmm29 		
	addq  			$16, %%rax 						

   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 		
   vfmadd231ps		%%xmm3, %%xmm6, %%xmm31 		

.endm 												

.macro	ADD_C_4x4									

   vmovups 		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   vmovups 		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   vmovups 		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   vmovups 		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_4x4 	
   subq 			$4, %%rdi 						

   vmovups 		%%xmm28, (%%r10)        		
   vmovups 		%%xmm29, (%%r11)        		
   vmovups 		%%xmm30, (%%r12)        		
   vmovups 		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------


.macro	KERNEL1x4_K1
   prefetcht0 		256(%%rax)
   addq  			$4, %%rax 

   vbroadcastss	(%%rax), %%xmm2    			
   vfmadd231ps		%%xmm0, %%xmm4, %%xmm30 				
							           		
	addq  			$16, %%rbx 						 // B	
   vmovups 		   (%%rbx), %%xmm6  
   prefetcht0 		16(%%rbx)        			

.endm 												

.macro	KERNEL1x4_K2		
   prefetcht0 		256(%%rax)  						
	addq  			$4, %%rax

   vbroadcastss	(%%rax), %%xmm0    			
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 					
                  		
	addq  			$16, %%rbx 						 // B		
   vmovups 		   (%%rbx), %%xmm4        			
   prefetcht0 		16(%%rbx)                 		

.endm 												

.macro	KERNEL1x4_END_K									
   vfmadd231ps		%%xmm2, %%xmm6, %%xmm30 					
	addq  			$4, %%rax 								
.endm 												

.macro	ADD_C_1x4											
   vmovups 		(%%r10), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30				
	mov  	%%rcx, %%r10 							   // C0
.endm 												

.macro	SAVE_1x4 									
   vmovups 		%%xmm30, (%%r10)        		
   subq 			$1, %%rdi        		       		
	leaq  	(%%r10, %%r8, 4), %%rcx 			// C0

.endm 												

//-----------------------------------------------------------------

SMM_NN_KERNEL12x4:								

   mov 	%[C], %%rcx   	 						
   mov 	%[A], %%rax   	 						
   mov 	%[B], %%rbx   	 						

   prefetcht0 		(%%rax)                 		

	mov 	%[K], %%rdx 							 // K
	mov  	%[LN], %%r8 							
	mov  	%[Bc], %%r14 							
	mov  	%[M], %%rdi 							
	mov 	%[k_tag], %%r15 						

   prefetcht0 		(%%rbx)                 		
	mov 	%%rbx, %%r9 							 // B
	mov 	%%rdx, %%rsi 							 // K

BEGIN_PACK12x4:									

	mov 	%%r9, %%rbx 							 // B
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
	mov 	%%r14, %%rbp 							 // Bc

   vmovups		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm20, %%xmm20, %%xmm20 			
	vpxorq 		%%xmm21, %%xmm21, %%xmm21 			
   vbroadcastss	(%%rax), %%xmm0    				
	vpxorq 		%%xmm22, %%xmm22, %%xmm22 			
	vpxorq 		%%xmm23, %%xmm23, %%xmm23 			
   vbroadcastss	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_PACK_K12x4:									

	KERNEL12x4_PACK_K1 							
	KERNEL12x4_PACK_K2 							
	KERNEL12x4_PACK_K1 							
	KERNEL12x4_PACK_K2 							
	KERNEL12x4_PACK_K1 							
	KERNEL12x4_PACK_K2 							
	KERNEL12x4_PACK_K1 							
   cmp 	$0, %%rdx           					
	je 		EDGE_PACK_K12x4 						
	KERNEL12x4_PACK_K2 							

	subq 	$8, %%rdx 								
   jmp     MAIN_PACK_K12x4       					

EDGE_PACK_K12x4:									

	KERNEL12x4_PACK_END_K 							
	jmp  	BEGIN_SAVE_12x4 						

//-----------------------------------------------------------------

BEGIN_M12x4:										

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10) 
	leaq 	(%%r10, %%r8, 4), %%r11 			 // C1
   prefetcht1 		(%%r11) 
	leaq 	(%%r11, %%r8, 4), %%r12 			 // C2
   prefetcht1 		(%%r12) 
	leaq 	(%%r12, %%r8, 4), %%r13 			 // C3
   prefetcht1 		(%%r13) 

	mov 	%%rsi, %%rdx 							 // K									

   vmovups		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm20, %%xmm20, %%xmm20 			
	vpxorq 		%%xmm21, %%xmm21, %%xmm21 			
	vpxorq 		%%xmm22, %%xmm22, %%xmm22 			
   vbroadcastss	(%%rax), %%xmm0    				
	vpxorq 		%%xmm23, %%xmm23, %%xmm23 			
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
   vbroadcastss	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K12x4:										

	KERNEL12x4_K1 									
	KERNEL12x4_K2 									
	KERNEL12x4_K1 									
	KERNEL12x4_K2 									
	KERNEL12x4_K1 									
	KERNEL12x4_K2 									
	KERNEL12x4_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K12x4 							
	KERNEL12x4_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K12x4       						

EDGE_K12x4:										

	KERNEL12x4_END_K 								

BEGIN_SAVE_12x4:									
	cmp 	$0, %%r15								
	je  	SAVE_C12x4 							
	ADD_C_12x4 									

SAVE_C12x4: 										
	SAVE_12x4 										

	cmpq  	$12, %%rdi 								
	jnb 	BEGIN_M12x4                 // 不小于（或等于）则跳转 							

//------------------------------------------------------------------

BEGIN_M8_N4:										
   cmpq  	$8, %%rdi  								 // M % 8
	jb   	BEGIN_M4_N4 	                      //小于则跳转

	mov 	%%r14, %%rbx 							    // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							    // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
   vmovups		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
   vbroadcastss	(%%rax), %%xmm0    				
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
   vbroadcastss	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K_M8_N4:										

	KERNEL8x4_K1 									
	KERNEL8x4_K2 									
	KERNEL8x4_K1 									
	KERNEL8x4_K2 									
	KERNEL8x4_K1 									
	KERNEL8x4_K2 									
	KERNEL8x4_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M8_N4 							
	KERNEL8x4_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M8_N4       					

EDGE_K_M8_N4:										

	KERNEL8x4_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_8x4 							
	ADD_C_8x4 										

SAVE_C_8x4: 										
	SAVE_8x4 										

//----------------------------------------------------------------

BEGIN_M4_N4:										

	cmpq  	$4, %%rdi  							 // M % 4
	jb   	BEGIN_M1_N4 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%xmm4        				
   vbroadcastss	(%%rax), %%xmm0    				
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
   vbroadcastss	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K_M4_N4:										

	KERNEL4x4_K1 									
	KERNEL4x4_K2 									
	KERNEL4x4_K1 									
	KERNEL4x4_K2 									
	KERNEL4x4_K1 									
	KERNEL4x4_K2 									
	KERNEL4x4_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4_N4 							
	KERNEL4x4_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M4_N4       					

EDGE_K_M4_N4:										

	KERNEL4x4_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_4x4 							
	ADD_C_4x4 										

SAVE_C_4x4: 										
	SAVE_4x4 										


//----------------------------------------------------------------

BEGIN_M1_N4:										
	cmpq  	$1, %%rdi  							 // M % 1
	jb   	END_M_N4 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		              		                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%xmm4        				
   vbroadcastss	(%%rax), %%xmm0    		 // A0				
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 						

	subq 	$8, %%rdx 									

MAIN_K_M1_N4:										
	KERNEL1x4_K1 									
	KERNEL1x4_K2 									
	KERNEL1x4_K1 									
	KERNEL1x4_K2 									
	KERNEL1x4_K1 									
	KERNEL1x4_K2 									
	KERNEL1x4_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M1_N4 							
	KERNEL1x4_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M1_N4   					

EDGE_K_M1_N4:										
	KERNEL1x4_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_1x4 							
	ADD_C_1x4 
									
SAVE_C_1x4: 										
	SAVE_1x4 
   cmpq  	$1, %%rdi 
	jnb 	BEGIN_M1_N4 					//不小于（或等于）则跳转									

//-----------------------------------------------------------------


END_M_N4:											

