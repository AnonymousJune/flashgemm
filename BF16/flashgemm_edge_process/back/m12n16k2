.macro bf16_pack_b_n16
   vpunpcklwd  %%ymm7, %%ymm6, %%ymm4                      
   vpunpckhwd  %%ymm7, %%ymm6, %%ymm5                   

   vextracti32x4   $0x1, %%ymm4, %%xmm2                                         
   vextracti32x4   $0x0, %%ymm5, %%xmm3                     
   vextracti32x4   $0x1, %%ymm5, %%xmm6

   vbroadcastss    (%%rax), %%zmm0                           // A0 in zmm0
   vbroadcastss    4(%%rax), %%zmm1                          // A1 in zmm1   
   
   vinserti32x4    $0x2, %%xmm2, %%zmm4, %%zmm4
   vinserti32x4    $0x1, %%xmm3, %%zmm4, %%zmm4
   vinserti32x4    $0x3, %%xmm6, %%zmm4, %%zmm4

   vmovups         %%zmm4, (%%rbp)                           // store back B
   addq            $64, %%rbp
.endm

.macro bf16_kernel_m12n16k2_pack                
   bf16_pack_b_n16

   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                                   

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                                  

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                                  

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16                                  

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18

   vmovdqu16     (%%rbx), %%ymm6                               // load next B
   prefetcht2  32(%%rbx)                                    
   leaq        (%%rbx, %%r8, 2), %%rbx                      

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20                                  

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22                                  

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24                                  

   vmovdqu16         (%%rbx), %%ymm7                         // load next B
   prefetcht2  32(%%rbx)                                    
   leaq        (%%rbx, %%r8, 2), %%rbx

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26
   addq          $48, %%rax                                 

   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28

   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30
.endm                                                       

.macro bf16_kernel_m12n16k2_pack_end // deference is no prefetch A and B 
   bf16_pack_b_n16                           

   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26
   addq          $48, %%rax                                 

   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28

   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30
.endm  

.macro    bf16_kernel_m12n16k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18

   vbroadcastss    32(%%rax), %%zmm0
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20

   addq             $64, %%rbx
   vmovdqu16        (%%rbx), %%zmm6
   prefetcht0       64(%%rbx)                           

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26                 
   
   addq              $48, %%rax

   vbroadcastss    (%%rax), %%zmm0
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28

   vbroadcastss    4(%%rax), %%zmm1
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30
.endm                                                       

.macro    bf16_kernel_m12n16k2_2                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20

   addq              $64, %%rbx
   vmovdqu16         (%%rbx), %%zmm4
   prefetcht0        64(%%rbx)

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22                         

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm24

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm26                 
                             
   addq              $48, %%rax

   vbroadcastss    (%%rax), %%zmm0                          
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm28                 
   
   vbroadcastss    4(%%rax), %%zmm1                         
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm30
.endm                                                         

.macro    bf16_add_c_m12n16                                  
   vmovups         (%%r10), %%zmm0
   vaddps             %%zmm0, %%zmm8, %%zmm8
   vmovups         (%%r11), %%zmm2
   vaddps             %%zmm2, %%zmm10, %%zmm10
   vmovups         (%%r12), %%zmm4
   vaddps             %%zmm4, %%zmm12, %%zmm12
   vmovups         (%%r13), %%zmm6
   vaddps             %%zmm6, %%zmm14, %%zmm14

   leaq             (%%r13, %%r8, 4), %%r10                 // C0
   leaq             (%%r10, %%r8, 4), %%r11                 // C1
   leaq             (%%r11, %%r8, 4), %%r12                 // C2
   leaq             (%%r12, %%r8, 4), %%r13                 // C3

   vmovups         (%%r10), %%zmm0                          
   vaddps             %%zmm0, %%zmm16, %%zmm16
   vmovups         (%%r11), %%zmm2                          
   vaddps             %%zmm2, %%zmm18, %%zmm18
   vmovups         (%%r12), %%zmm4                          
   vaddps             %%zmm4, %%zmm20, %%zmm20
   vmovups         (%%r13), %%zmm6                          
   vaddps             %%zmm6, %%zmm22, %%zmm22

   leaq             (%%r13, %%r8, 4), %%r10                 // C0
   leaq             (%%r10, %%r8, 4), %%r11                 // C1
   leaq             (%%r11, %%r8, 4), %%r12                 // C2
   leaq             (%%r12, %%r8, 4), %%r13                 // C3

   vmovups         (%%r10), %%zmm0                          
   vaddps             %%zmm0, %%zmm24, %%zmm24
   vmovups         (%%r11), %%zmm2                          
   vaddps             %%zmm2, %%zmm26, %%zmm26
   vmovups         (%%r12), %%zmm4                          
   vaddps             %%zmm4, %%zmm28, %%zmm28
   vmovups         (%%r13), %%zmm6                          
   vaddps             %%zmm6, %%zmm30, %%zmm30

   mov      %%rcx, %%r10                                    // C0
   leaq     (%%r10, %%r8, 4), %%r11                         // C1
   leaq     (%%r11, %%r8, 4), %%r12                         // C2
   leaq     (%%r12, %%r8, 4), %%r13                         // C3
.endm                                                       

.macro    bf16_save_c_m12n16                                   
   vmovups         %%zmm8, (%%r10)
   vmovups         %%zmm10, (%%r11)
   vmovups         %%zmm12, (%%r12)
   vmovups         %%zmm14, (%%r13)

   leaq     (%%r13, %%r8, 4), %%r10                         // C0
   leaq     (%%r10, %%r8, 4), %%r11                         // C1
   leaq     (%%r11, %%r8, 4), %%r12                         // C2
   leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm16, (%%r10)
   vmovups         %%zmm18, (%%r11)
   vmovups         %%zmm20, (%%r12)
   vmovups         %%zmm22, (%%r13)

   leaq     (%%r13, %%r8, 4), %%r10                         // C0
   leaq     (%%r10, %%r8, 4), %%r11                         // C1
   leaq     (%%r11, %%r8, 4), %%r12                         // C2
   leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm24, (%%r10)
   vmovups         %%zmm26, (%%r11)
   vmovups         %%zmm28, (%%r12)
   vmovups         %%zmm30, (%%r13)

   subq            $12, %%rdi                             
   leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

//-----------------------------------------------------------------

.macro    bf16_kernel_m8n16k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10

   prefetcht0      256(%%rax)
   addq             $64, %%rbx
   vmovdqu16        (%%rbx), %%zmm6
   prefetcht0       64(%%rbx)                      

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16
   
   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18
   
   addq              $32, %%rax

   vbroadcastss     (%%rax), %%zmm0
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20
                          
   vbroadcastss     4(%%rax), %%zmm1                 
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22
.endm                                                       

.macro    bf16_kernel_m8n16k2_2                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10

   prefetcht0        256(%%rax)
   addq              $64, %%rbx
   vmovdqu16         (%%rbx), %%zmm4                            
   prefetcht0        64(%%rbx)

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12

   vbroadcastss    20(%%rax), %%zmm1
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18
   
   addq             $32, %%rax

   vbroadcastss    (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20

   vbroadcastss    4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22
.endm                                                       

.macro    bf16_add_c_m8n16                                  
   vmovups         (%%r10), %%zmm0                          
   vaddps             %%zmm0, %%zmm8, %%zmm8
   vmovups         (%%r11), %%zmm2                          
   vaddps             %%zmm2, %%zmm10, %%zmm10
   vmovups         (%%r12), %%zmm4                          
   vaddps             %%zmm4, %%zmm12, %%zmm12
   vmovups         (%%r13), %%zmm6                          
   vaddps             %%zmm6, %%zmm14, %%zmm14

   leaq             (%%r13, %%r8, 4), %%r10                 // C0
   leaq             (%%r10, %%r8, 4), %%r11                 // C1
   leaq             (%%r11, %%r8, 4), %%r12                 // C2
   leaq             (%%r12, %%r8, 4), %%r13                 // C3

   vmovups         (%%r10), %%zmm0                          
   vaddps             %%zmm0, %%zmm16, %%zmm16
   vmovups         (%%r11), %%zmm2                          
   vaddps             %%zmm2, %%zmm18, %%zmm18
   vmovups         (%%r12), %%zmm4                          
   vaddps             %%zmm4, %%zmm20, %%zmm20
   vmovups         (%%r13), %%zmm6                          
   vaddps             %%zmm6, %%zmm22, %%zmm22

   mov               %%rcx, %%r10                           // C0
   leaq             (%%r10, %%r8, 4), %%r11                 // C1
   leaq             (%%r11, %%r8, 4), %%r12                 // C2
   leaq             (%%r12, %%r8, 4), %%r13                 // C3
.endm                                                       

.macro    bf16_save_c_m8n16                                   
   vmovups         %%zmm8, (%%r10)
   vmovups         %%zmm10, (%%r11)
   vmovups         %%zmm12, (%%r12)
   vmovups         %%zmm14, (%%r13)

   leaq     (%%r13, %%r8, 4), %%r10                         // C0
   leaq     (%%r10, %%r8, 4), %%r11                         // C1
   leaq     (%%r11, %%r8, 4), %%r12                         // C2
   leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm16, (%%r10)
   vmovups         %%zmm18, (%%r11)
   vmovups         %%zmm20, (%%r12)
   vmovups         %%zmm22, (%%r13)
                       
   subq            $8, %%rdi                                              
   leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

//-----------------------------------------------------------------

.macro    bf16_kernel_m4n16k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8

   addq             $64, %%rbx
   vmovdqu16        (%%rbx), %%zmm6
   prefetcht0       64(%%rbx)

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10

   prefetcht0      256(%%rax) 
   addq              $16, %%rax

   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12

   vbroadcastss     4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14
.endm                                                       

.macro    bf16_kernel_m4n16k2_2                               
   vbroadcastss     8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8
   
   addq             $64, %%rbx
   vmovdqu16        (%%rbx), %%zmm4                            
   prefetcht0       64(%%rbx)        

   vbroadcastss     12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10

   prefetcht0        256(%%rax)
   addq             $16, %%rax

   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12

   vbroadcastss     4(%%rax), %%zmm1
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14
.endm                                                       

.macro    bf16_kernel_m4n16k2_end
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10 

   prefetcht0         256(%%rax)                            
   addq             $16, %%rax

   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12

   vbroadcastss     4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14
.endm 

.macro    bf16_add_c_m4n16                                  
   vmovups         (%%r10), %%zmm0                          
   vaddps             %%zmm0, %%zmm8, %%zmm8
   vmovups         (%%r11), %%zmm2                          
   vaddps             %%zmm2, %%zmm10, %%zmm10
   vmovups         (%%r12), %%zmm4                          
   vaddps             %%zmm4, %%zmm12, %%zmm12
   vmovups         (%%r13), %%zmm6                          
   vaddps             %%zmm6, %%zmm14, %%zmm14

   mov               %%rcx, %%r10                           // C0
   leaq             (%%r10, %%r8, 4), %%r11                 // C1
   leaq             (%%r11, %%r8, 4), %%r12                 // C2
   leaq             (%%r12, %%r8, 4), %%r13                 // C3
.endm                                                       

.macro    bf16_save_c_m4n16
   vmovups         %%zmm8, (%%r10)
   vmovups         %%zmm10, (%%r11)
   vmovups         %%zmm12, (%%r12)
   vmovups         %%zmm14, (%%r13)
                       
   subq            $4, %%rdi                                              
   leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

//-----------------------------------------------------------------
//-----------------------------------------------------------------

GEMM_BF16_N16:                                  
   mov     %[C], %%rcx                                      
   mov     %[A], %%rax                                      
   mov     %[B], %%rbx                                      

   prefetcht0         (%%rax)                               

   mov     %[K], %%rdx                                      
   mov     %[LN], %%r8                                     
   mov     %[Bc], %%r14                                    
   mov     %[M], %%rdi

   mov     %[LK], %%r15
   mov     %%rax, %%r9                                  

   prefetcht0         (%%rbx)                                                                  
   mov     %%rdx, %%rsi                                     

//-----------------------------------------------------------------

BF16_BEGIN_PACK_N16:
   mov     %%r14, %%rbp                                      // Bc

   vmovdqu16 (%%rbx), %%ymm6                                  
   prefetcht2  32(%%rbx)                                    
   leaq    (%%rbx, %%r8, 2), %%rbx                          
   vmovdqu16 (%%rbx), %%ymm7                                  
   prefetcht2  32(%%rbx)                                    
   leaq    (%%rbx, %%r8, 2), %%rbx                          

   mov     %%rsi, %%rdx                                      // K
   vpxorq         %%zmm8, %%zmm8, %%zmm8                                        
   vpxorq         %%zmm10, %%zmm10, %%zmm10
   vpxorq         %%zmm12, %%zmm12, %%zmm12
   vpxorq         %%zmm14, %%zmm14, %%zmm14
   vpxorq         %%zmm16, %%zmm16, %%zmm16
   vpxorq         %%zmm18, %%zmm18, %%zmm18
   vpxorq         %%zmm20, %%zmm20, %%zmm20
   vpxorq         %%zmm22, %%zmm22, %%zmm22
   vpxorq         %%zmm24, %%zmm24, %%zmm24
   vpxorq         %%zmm26, %%zmm26, %%zmm26
   vpxorq         %%zmm28, %%zmm28, %%zmm28
   vpxorq         %%zmm30, %%zmm30, %%zmm30
   mov     %%rcx, %%r13
   cmpq    $16, %%rdx
   jb      BF16_PACK_MAIN_M12N16K2
   subq    $16, %%rdx

BF16_PACK_MAIN_M12N16K16:
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   bf16_kernel_m12n16k2_pack
   cmpq    $0, %%rdx                                        
   je      BF16_PACK_SAVEC_M12N16
   bf16_kernel_m12n16k2_pack
   cmpq    $16, %%rdx                                        
   jb      BF16_PACK_MAIN_M12N16K2
   subq    $16, %%rdx                               
   cmpq    $192, %%rdx                      // 192/16 = 12 rows of C data
   jbe     BF16_PACK_K_PREFETCH_C_M12N16    // prefeth C before loop end
   jmp     BF16_PACK_MAIN_M12N16K16

BF16_PACK_MAIN_M12N16K2:
   subq    $2, %%rdx
   cmpq    $0, %%rdx
   je      BF16_PACK_SAVEC_M12N16
   bf16_kernel_m12n16k2_pack
   jmp     BF16_PACK_MAIN_M12N16K2

BF16_PACK_K_PREFETCH_C_M12N16:
   prefetcht2         (%%r13)
   leaq     (%%r13, %%r8, 4), %%r13
   jmp BF16_PACK_MAIN_M12N16K16

BF16_PACK_SAVEC_M12N16: 
	bf16_kernel_m12n16k2_pack_end 
	jmp		BF16_BEGIN_SAVE_M12N16 


//-----------------------------------------------------------------

BF16_BEGIN_M12N16:
   cmpq    $12, %%rdi 
   jb      BF16_BEGIN_M8N16

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14
   vpxorq         %%zmm16, %%zmm16, %%zmm16
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1
   vpxorq         %%zmm20, %%zmm20, %%zmm20
   vpxorq         %%zmm22, %%zmm22, %%zmm22
   vpxorq         %%zmm24, %%zmm24, %%zmm24
   vpxorq         %%zmm26, %%zmm26, %%zmm26
   vpxorq         %%zmm28, %%zmm28, %%zmm28
   vpxorq         %%zmm30, %%zmm30, %%zmm30                           
   mov     %%rcx, %%r13
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M12N16K2
   subq    $16, %%rdx

BF16_MAIN_K_M12N16K16:   // loop K+=4
   bf16_kernel_m12n16k2_1
   bf16_kernel_m12n16k2_2
   bf16_kernel_m12n16k2_1
   bf16_kernel_m12n16k2_2
   bf16_kernel_m12n16k2_1
   bf16_kernel_m12n16k2_2
   bf16_kernel_m12n16k2_1
   bf16_kernel_m12n16k2_2
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N16
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M12N16K2
   subq  $16, %%rdx                                         
   cmpq  $192, %%rdx        // 192/16 = 12 rows of C data                                
   jbe   BF16_K_PREFETCH_C_M12N16                          
   jmp   BF16_MAIN_K_M12N16K16

BF16_K_PREFETCH_C_M12N16:                                      
   prefetcht2         (%%r13)
   leaq     (%%r13, %%r8, 4), %%r13
   jmp BF16_MAIN_K_M12N16K16                                

BF16_MAIN_M12N16K2:
   bf16_kernel_m12n16k2_1
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N16
   bf16_kernel_m12n16k2_2
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N16
   jmp     BF16_MAIN_M12N16K2

BF16_BEGIN_SAVE_M12N16:                                            
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3
   cmpq     $0, %[k_tag]                                      
   je       BF16_SAVE_C_M12N16                                     
   bf16_add_c_m12n16                                        

BF16_SAVE_C_M12N16:                                                
   bf16_save_c_m12n16
   imul     $24, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp     BF16_BEGIN_M12N16

//-----------------------------------------------------------------

BF16_BEGIN_M8N16:
   cmpq    $8, %%rdi 
   jb      BF16_BEGIN_M4N16

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vpxorq         %%zmm8, %%zmm8, %%zmm8
   vpxorq         %%zmm10, %%zmm10, %%zmm10
   vpxorq         %%zmm12, %%zmm12, %%zmm12
   vpxorq         %%zmm14, %%zmm14, %%zmm14
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1
   vpxorq         %%zmm16, %%zmm16, %%zmm16
   vpxorq         %%zmm18, %%zmm18, %%zmm18
   vpxorq         %%zmm20, %%zmm20, %%zmm20
   vpxorq         %%zmm22, %%zmm22, %%zmm22
   mov     %%rcx, %%r13
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M8N16K2
   subq    $16, %%rdx

BF16_MAIN_K_M8N16K16:
   bf16_kernel_m8n16k2_1
   bf16_kernel_m8n16k2_2
   bf16_kernel_m8n16k2_1
   bf16_kernel_m8n16k2_2
   bf16_kernel_m8n16k2_1
   bf16_kernel_m8n16k2_2
   bf16_kernel_m8n16k2_1
   bf16_kernel_m8n16k2_2
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N16
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M8N16K2
   subq  $16, %%rdx                                         
   cmpq  $128, %%rdx        // 128/16 = 8 rows of C data                                
   jbe   BF16_K_PREFETCH_C_M8N16                          
   jmp   BF16_MAIN_K_M8N16K16

BF16_K_PREFETCH_C_M8N16:                                      
   prefetcht2         (%%r13)
   leaq     (%%r13, %%r8, 4), %%r13
   jmp BF16_MAIN_K_M8N16K16                                

BF16_MAIN_M8N16K2:
   bf16_kernel_m8n16k2_1
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N16
   bf16_kernel_m8n16k2_2
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N16
   jmp     BF16_MAIN_M8N16K2

BF16_BEGIN_SAVE_M8N16:                                            
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3
   cmpq     $0, %[k_tag]                                      
   je       BF16_SAVE_C_M8N16                                // when beta==0&&kk==0, no need to add     
   bf16_add_c_m8n16                                        

BF16_SAVE_C_M8N16:                                                
   bf16_save_c_m8n16
   imul     $16, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M8N16

//-----------------------------------------------------------------

BF16_BEGIN_M4N16:
   cmpq    $0, %%rdi 
   je      BF16_END_N16

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1 
   vpxorq         %%zmm8, %%zmm8, %%zmm8
   vpxorq         %%zmm10, %%zmm10, %%zmm10
   vpxorq         %%zmm12, %%zmm12, %%zmm12
   vpxorq         %%zmm14, %%zmm14, %%zmm14
                                                       
   mov     %%rcx, %%r13
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M4N16K2
   subq  $16, %%rdx

BF16_MAIN_K_M4N16K16:   // loop K+=4
   bf16_kernel_m4n16k2_1
   bf16_kernel_m4n16k2_2
   bf16_kernel_m4n16k2_1
   bf16_kernel_m4n16k2_2
   bf16_kernel_m4n16k2_1
   bf16_kernel_m4n16k2_2
   bf16_kernel_m4n16k2_1
   bf16_kernel_m4n16k2_2
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N16
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M4N16K2
   subq  $16, %%rdx                                         
   cmpq  $64, %%rdx        // 64/16 = 4 rows of C data                                
   jbe   BF16_K_PREFETCH_C_M4N16                          
   jmp   BF16_MAIN_K_M4N16K16

BF16_K_PREFETCH_C_M4N16:
   prefetcht2         (%%r13)
   leaq     (%%r13, %%r8, 4), %%r13
   jmp BF16_MAIN_K_M4N16K16

BF16_MAIN_M4N16K2:
   bf16_kernel_m4n16k2_1
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N16
   bf16_kernel_m4n16k2_2
   subq    $2, %%rdx
   cmpq    $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N16
   jmp     BF16_MAIN_M4N16K2

BF16_BEGIN_SAVE_M4N16:                                            
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3
   cmpq     $0, %[k_tag]                                      
   je       BF16_SAVE_C_M4N16                                     
   bf16_add_c_m4n16                                       

BF16_SAVE_C_M4N16:
   cmpq     $3, %%rdi
   je       BF16_SAVE_C_M3N16
   cmpq     $2, %%rdi
   je       BF16_SAVE_C_M2N16
   cmpq     $1, %%rdi
   je       BF16_SAVE_C_M1N16
   bf16_save_c_m4n16
   imul     $8, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M4N16

BF16_SAVE_C_M3N16:
   vmovups         %%zmm12, (%%r12)                     

BF16_SAVE_C_M2N16:
   vmovups         %%zmm10, (%%r11)                         

BF16_SAVE_C_M1N16:
   vmovups         %%zmm8, (%%r10)

BF16_END_N16: