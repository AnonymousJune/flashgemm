// 这是融合GEMM中的kernel，加了一层选择是否打包结果矩阵的逻辑
void SMM_NN_KERNELm12xn32(float *C, float *Cc, float *A, float *B, long M,
						  long N, long K, long LN, long LK, long LNc, float *Bc, long k_tag, long packC);

void SMM_NN_KERNELm12xn32(float *C, float *Cc, float *A, float *B, long M,
						  long N, long K, long LN, long LK, long LNc, float *Bc, long k_tag, long packC)
{

	asm volatile(
		// 将C预取隐藏在k维计算中，预取到L3 Cache，条件预取
		// 添加一个参数packC，未计算完成的C数据还是采用原先的行优先存储格式，只有算完了才用打包格式存回
		// 空出%%rsi寄存器用于存放Cc地址

		".macro PACK_C_SAVE12x32                                     \n"
		"   mov   %[LNc], %%r11                                      \n"
		"   mul   $12, %%r11                                         \n"

		"   vunpcklps %%zmm10, %%zmm9, %%zmm0                        \n"
		"   vunpcklps %%zmm14, %%zmm12, %%zmm1                       \n"
		"   vunpcklps %%zmm18, %%zmm16, %%zmm2                       \n"
		"   vunpcklps %%zmm22, %%zmm20, %%zmm3                       \n"

		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, (%%rsi)                      \n"
		"   vextractf32x4 $0x1, %%zmm4, 192(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 384(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 576(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 48(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm6, 240(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 432(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 624(%%rsi)                   \n"

		"   vunpckhps %%zmm10, %%zmm9, %%zmm0                        \n"
		"   vunpckhps %%zmm14, %%zmm12, %%zmm1                       \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 16(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm5, 208(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 400(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 592(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 64(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm7, 256(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 448(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 640(%%rsi)                   \n"

		"   vunpckhps %%zmm18, %%zmm16, %%zmm2                       \n"
		"   vunpckhps %%zmm22, %%zmm20, %%zmm3                       \n"

		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, 96(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm4, 288(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 480(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 672(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 144(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm6, 336(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 528(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 720(%%rsi)                   \n"

		"   vunpcklps %%zmm26, %%zmm24, %%zmm0                       \n"
		"   vunpcklps %%zmm30, %%zmm28, %%zmm1                       \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 112(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm5, 304(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 496(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 688(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 160(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm7, 352(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 544(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 736(%%rsi)                   \n"

		"   vunpckhps %%zmm26, %%zmm24, %%zmm2                       \n"
		"   vunpckhps %%zmm30, %%zmm28, %%zmm3                       \n"

		"   addq    $32, %%rsi                                       \n"
		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, (%%rsi)                      \n"
		"   vextractf32x4 $0x1, %%zmm4, 192(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 384(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 576(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 48(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm6, 240(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 432(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 624(%%rsi)                   \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 96(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm5, 288(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 480(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 672(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 144(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm7, 336(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 528(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 720(%%rsi)                   \n"

		"   addq  $736, %%rsi                                        \n" // next 16 float

		"   vunpcklps %%zmm11, %%zmm9, %%zmm0                        \n"
		"   vunpcklps %%zmm15, %%zmm13, %%zmm1                       \n"
		"   vunpcklps %%zmm19, %%zmm17, %%zmm2                       \n"
		"   vunpcklps %%zmm23, %%zmm21, %%zmm3                       \n"

		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, (%%rsi)                      \n"
		"   vextractf32x4 $0x1, %%zmm4, 192(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 384(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 576(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 48(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm6, 240(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 432(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 624(%%rsi)                   \n"

		"   vunpckhps %%zmm11, %%zmm9, %%zmm0                        \n"
		"   vunpckhps %%zmm15, %%zmm13, %%zmm1                       \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 16(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm5, 208(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 400(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 592(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 64(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm7, 256(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 448(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 640(%%rsi)                   \n"

		"   vunpckhps %%zmm19, %%zmm17, %%zmm2                       \n"
		"   vunpckhps %%zmm23, %%zmm21, %%zmm3                       \n"

		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, 96(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm4, 288(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 480(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 672(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 144(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm6, 336(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 528(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 720(%%rsi)                   \n"

		"   vunpcklps %%zmm27, %%zmm25, %%zmm0                       \n"
		"   vunpcklps %%zmm31, %%zmm29, %%zmm1                       \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 112(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm5, 304(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 496(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 688(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 160(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm7, 352(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 544(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 736(%%rsi)                   \n"

		"   vunpckhps %%zmm27, %%zmm25, %%zmm2                       \n"
		"   vunpckhps %%zmm31, %%zmm29, %%zmm3                       \n"

		"   addq    $32, %%rsi                                       \n"
		"   vunpcklpd %%zmm1, %%zmm0, %%zmm4                         \n"
		"   vextractf32x4 $0x0, %%zmm4, (%%rsi)                      \n"
		"   vextractf32x4 $0x1, %%zmm4, 192(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm4, 384(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm4, 576(%%rsi)                   \n"

		"   vunpckhpd %%zmm1, %%zmm0, %%zmm6                         \n"
		"   vextractf32x4 $0x0, %%zmm6, 48(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm6, 240(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm6, 432(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm6, 624(%%rsi)                   \n"

		"   vunpcklpd %%zmm3, %%zmm2, %%zmm5                         \n"
		"   vextractf32x4 $0x0, %%zmm5, 96(%%rsi)                    \n"
		"   vextractf32x4 $0x1, %%zmm5, 288(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm5, 480(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm5, 672(%%rsi)                   \n"

		"   vunpckhpd %%zmm3, %%zmm2, %%zmm7                         \n"
		"   vextractf32x4 $0x0, %%zmm7, 144(%%rsi)                   \n"
		"   vextractf32x4 $0x1, %%zmm7, 336(%%rsi)                   \n"
		"   vextractf32x4 $0x2, %%zmm7, 528(%%rsi)                   \n"
		"   vextractf32x4 $0x3, %%zmm7, 720(%%rsi)                   \n"

		"   subq  $800, %%rsi                                        \n"
		"   addq     %%r11, %%rsi                                    \n"

		".endm                                                       \n"

		//-----------------------------------------------------------------

		".macro    KERNEL12x32_PACK_K1                               \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n" // A2
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm8                \n" // A0*(B0-15)
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm9                \n" // A0*(B16-31)

		"   vbroadcastss    12(%%rax), %%zmm3                        \n" // A3
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm10               \n" // A1*(B0-15)
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm11               \n" // A1*(B16-31)

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n" // A4
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm13               \n"

		"   prefetcht2         128(%%rbx)                            \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n" // A5
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n" // A6
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm17               \n"

		"   prefetcht2         192(%%rbx)                            \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n" // A7
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n" // A8
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm21               \n"

		"    leaq      (%%rbx, %%r8, 4), %%rbx                       \n" // B

		"   vbroadcastss    36(%%rax), %%zmm1                        \n" // A9
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm23               \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n" // A10
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm25               \n"

		"   prefetcht0         384(%%rax)                            \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n" // A11
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm6                          \n" // next B0
		"    addq              $48, %%rax                            \n" // 下一组A(已读12个)
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n" // next A0
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm28               \n"
		"   vmovups         64(%%rbx), %%zmm7                        \n" // next B1
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm29               \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n" // next A1
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm30               \n"
		"   vmovups         %%zmm4, (%%rbp)                          \n" // pack B0 to Bc
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm31               \n"
		"   vmovups         %%zmm5, 64(%%rbp)                        \n" // pack B1 to Bc
		"    addq              $128, %%rbp                           \n"

		".endm                                                       \n"

		".macro    KERNEL12x32_PACK_K2                               \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n" // next A2
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm8                \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm9                \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n" // next A3
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm10               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm11               \n"

		"   prefetcht2         128(%%rbx)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n" // next A4
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm13               \n"

		"   prefetcht2         192(%%rbx)                            \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n" // next A5
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n" // next A6
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n" // next A7
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n" // next A8
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"    leaq      (%%rbx, %%r8, 4), %%rbx                       \n" // B

		"   vbroadcastss    36(%%rax), %%zmm1                        \n" // next A9
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n" // next A10
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n" // next A11
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm4                          \n" // next next B0
		"    addq              $48, %%rax                            \n" // 下一组A(已读12个)
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n" // next next A0
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vmovups         64(%%rbx), %%zmm5                        \n" // next next B1
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n" // next next A1
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm30               \n"
		"   vmovups         %%zmm6, (%%rbp)                          \n" // pack B0 to Bc
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm31               \n"
		"   vmovups         %%zmm7, 64(%%rbp)                        \n" // pack B1 to Bc
		"    addq              $128, %%rbp                           \n"

		".endm                                                       \n"

		".macro    KERNEL12x32_PACK_END_K                            \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n" // next A2

		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm8                \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm9                \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n" // next A3

		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm10               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm11               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n" // next A4
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm13               \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n" // next A5
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n" // next A6
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n" // next A7
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n" // next A8
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"   vbroadcastss    36(%%rax), %%zmm1                        \n" // next A9
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n" // next A10
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n" // next A11
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"    addq              $48, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   vmovups         %%zmm6, (%%rbp)                          \n" // pack B0 to Bc
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"   vmovups         %%zmm7, 64(%%rbp)                        \n" // pack B0 to Bc
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm30               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm31               \n"

		".endm                                                       \n"

		//-----------------------------------------------------------------

		".macro    KERNEL12x32_K1                                    \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm8                \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm9                \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"

		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm10               \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm11               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm13               \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm17               \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm21               \n"

		"    addq              $128, %%rbx                           \n"

		"   vbroadcastss    36(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm23               \n"

		"   prefetcht0         64(%%rbx)                             \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm25               \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm6                          \n"
		"    addq              $48, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm28               \n"
		"   vmovups         64(%%rbx), %%zmm7                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm29               \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm30               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm31               \n"

		".endm                                                       \n"

		".macro    KERNEL12x32_K2                                    \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm8                \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm9                \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm10               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm11               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm13               \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"    addq              $128, %%rbx                           \n"

		"   vbroadcastss    36(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   prefetcht0         64(%%rbx)                             \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm4                          \n"
		"    addq              $48, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vmovups         64(%%rbx), %%zmm5                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm30               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm31               \n"

		".endm                                                       \n"

		".macro    KERNEL12x32_END_K                                 \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm8                \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm9                \n"
		"   vbroadcastss    12(%%rax), %%zmm3                        \n"

		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm10               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm11               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm12               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm13               \n"

		"   vbroadcastss    20(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm14               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm15               \n"

		"   vbroadcastss    24(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    28(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   vbroadcastss    32(%%rax), %%zmm0                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"   vbroadcastss    36(%%rax), %%zmm1                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   vbroadcastss    40(%%rax), %%zmm2                        \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    44(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"    addq              $48, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm30               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm31               \n"

		".endm                                                       \n"

		".macro    ADD_C_12x32                                       \n"

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm8, %%zmm8               \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm9, %%zmm9               \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm10, %%zmm10             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm11, %%zmm11             \n"
		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm12, %%zmm12             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm13, %%zmm13             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm14, %%zmm14             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm15, %%zmm15             \n"

		"    leaq          (%%r13, %%r8, 4), %%r10                   \n" // C0
		"    leaq             (%%r10, %%r8, 4), %%r11                \n" // C1
		"    leaq             (%%r11, %%r8, 4), %%r12                \n" // C2
		"    leaq             (%%r12, %%r8, 4), %%r13                \n" // C3

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm16, %%zmm16             \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm17, %%zmm17             \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm18, %%zmm18             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm19, %%zmm19             \n"

		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm20, %%zmm20             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm21, %%zmm21             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm22, %%zmm22             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm23, %%zmm23             \n"

		"    leaq          (%%r13, %%r8, 4), %%r10                   \n" // C0
		"    leaq             (%%r10, %%r8, 4), %%r11                \n" // C1
		"    leaq             (%%r11, %%r8, 4), %%r12                \n" // C2
		"    leaq             (%%r12, %%r8, 4), %%r13                \n" // C3

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm24, %%zmm24             \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm25, %%zmm25             \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm26, %%zmm26             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm27, %%zmm27             \n"

		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm28, %%zmm28             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm29, %%zmm29             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm30, %%zmm30             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm31, %%zmm31             \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		".endm                                                       \n"

		".macro    SAVE_12x32                                        \n"

		"   vmovups         %%zmm8, (%%r10)                          \n"
		"   vmovups         %%zmm9, 64(%%r10)                        \n"
		"   vmovups         %%zmm10, (%%r11)                         \n"
		"   vmovups         %%zmm11, 64(%%r11)                       \n"
		"   vmovups         %%zmm12, (%%r12)                         \n"
		"   vmovups         %%zmm13, 64(%%r12)                       \n"
		"   vmovups         %%zmm14, (%%r13)                         \n"
		"   vmovups         %%zmm15, 64(%%r13)                       \n"

		"    leaq  (%%r13, %%r8, 4), %%r10                           \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		"   vmovups         %%zmm16, (%%r10)                         \n"
		"   vmovups         %%zmm17, 64(%%r10)                       \n"
		"   vmovups         %%zmm18, (%%r11)                         \n"
		"   vmovups         %%zmm19, 64(%%r11)                       \n"
		"   vmovups         %%zmm20, (%%r12)                         \n"
		"   vmovups         %%zmm21, 64(%%r12)                       \n"
		"   vmovups         %%zmm22, (%%r13)                         \n"
		"   vmovups         %%zmm23, 64(%%r13)                       \n"

		"    leaq  (%%r13, %%r8, 4), %%r10                           \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		"   vmovups         %%zmm24, (%%r10)                         \n"
		"   vmovups         %%zmm25, 64(%%r10)                       \n"
		"   vmovups         %%zmm26, (%%r11)                         \n"
		"   vmovups         %%zmm27, 64(%%r11)                       \n"
		"    subq             $12, %%rdi                             \n"
		"   vmovups         %%zmm28, (%%r12)                         \n"
		"   vmovups         %%zmm29, 64(%%r12)                       \n"
		"   vmovups         %%zmm30, (%%r13)                         \n"
		"   vmovups         %%zmm31, 64(%%r13)                       \n"

		"    leaq      (%%r13, %%r8, 4), %%rcx                       \n" // C0

		".endm                                                       \n"

		//-----------------------------------------------------------------

		".macro    KERNEL8x32_K1                                     \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm17               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm19               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm8                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm21               \n"

		"    addq              $128, %%rbx                           \n"

		"   vbroadcastss    20(%%rax), %%zmm9                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm23               \n"

		"   prefetcht0         64(%%rbx)                             \n"

		"   vbroadcastss    24(%%rax), %%zmm10                       \n"
		"   vfmadd231ps        %%zmm8, %%zmm4, %%zmm24               \n"
		"   vfmadd231ps        %%zmm8, %%zmm5, %%zmm25               \n"

		"   vbroadcastss    28(%%rax), %%zmm11                       \n"
		"   vfmadd231ps        %%zmm9, %%zmm4, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm6                          \n"
		"    addq              $32, %%rax                            \n"
		"   vfmadd231ps        %%zmm9, %%zmm5, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm10, %%zmm4, %%zmm28              \n"
		"   vmovups         64(%%rbx), %%zmm7                        \n"
		"   vfmadd231ps        %%zmm10, %%zmm5, %%zmm29              \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   vfmadd231ps        %%zmm11, %%zmm4, %%zmm30              \n"
		"   vfmadd231ps        %%zmm11, %%zmm5, %%zmm31              \n"

		".endm                                                       \n"

		".macro    KERNEL8x32_K2                                     \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm8                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"    addq              $128, %%rbx                           \n"

		"   vbroadcastss    20(%%rax), %%zmm9                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   prefetcht0         64(%%rbx)                             \n"

		"   vbroadcastss    24(%%rax), %%zmm10                       \n"
		"   vfmadd231ps        %%zmm8, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm8, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    28(%%rax), %%zmm11                       \n"
		"   vfmadd231ps        %%zmm9, %%zmm6, %%zmm26               \n"
		"   vmovups         (%%rbx), %%zmm4                          \n"
		"    addq              $32, %%rax                            \n"
		"   vfmadd231ps        %%zmm9, %%zmm7, %%zmm27               \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm10, %%zmm6, %%zmm28              \n"
		"   vmovups         64(%%rbx), %%zmm5                        \n"
		"   vfmadd231ps        %%zmm10, %%zmm7, %%zmm29              \n"

		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   vfmadd231ps        %%zmm11, %%zmm6, %%zmm30              \n"
		"   vfmadd231ps        %%zmm11, %%zmm7, %%zmm31              \n"

		".endm                                                       \n"

		".macro    KERNEL8x32_END_K                                  \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm16               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm17               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm18               \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm19               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    16(%%rax), %%zmm8                        \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm20               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm21               \n"

		"   vbroadcastss    20(%%rax), %%zmm9                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm6, %%zmm22               \n"
		"   vfmadd231ps        %%zmm3, %%zmm7, %%zmm23               \n"

		"   prefetcht0         64(%%rbx)                             \n"

		"   vbroadcastss    24(%%rax), %%zmm10                       \n"
		"   vfmadd231ps        %%zmm8, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm8, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    28(%%rax), %%zmm11                       \n"
		"   vfmadd231ps        %%zmm9, %%zmm6, %%zmm26               \n"
		"    addq              $32, %%rax                            \n"
		"   vfmadd231ps        %%zmm9, %%zmm7, %%zmm27               \n"

		"   vfmadd231ps        %%zmm10, %%zmm6, %%zmm28              \n"
		"   vfmadd231ps        %%zmm10, %%zmm7, %%zmm29              \n"
		"   vfmadd231ps        %%zmm11, %%zmm6, %%zmm30              \n"
		"   vfmadd231ps        %%zmm11, %%zmm7, %%zmm31              \n"

		".endm                                                       \n"

		".macro    ADD_C_8x32                                        \n"

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm16, %%zmm16             \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm17, %%zmm17             \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm18, %%zmm18             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm19, %%zmm19             \n"

		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm20, %%zmm20             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm21, %%zmm21             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm22, %%zmm22             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm23, %%zmm23             \n"

		"    leaq              (%%r13, %%r8, 4), %%r10               \n" // C0
		"    leaq             (%%r10, %%r8, 4), %%r11                \n" // C1
		"    leaq             (%%r11, %%r8, 4), %%r12                \n" // C2
		"    leaq             (%%r12, %%r8, 4), %%r13                \n" // C3

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm24, %%zmm24             \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm25, %%zmm25             \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm26, %%zmm26             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm27, %%zmm27             \n"

		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm28, %%zmm28             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm29, %%zmm29             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm30, %%zmm30             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm31, %%zmm31             \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		".endm                                                       \n"

		".macro    SAVE_8x32                                         \n"

		"   vmovups         %%zmm16, (%%r10)                         \n"
		"   vmovups         %%zmm17, 64(%%r10)                       \n"
		"   vmovups         %%zmm18, (%%r11)                         \n"
		"   vmovups         %%zmm19, 64(%%r11)                       \n"
		"   vmovups         %%zmm20, (%%r12)                         \n"
		"   vmovups         %%zmm21, 64(%%r12)                       \n"
		"   vmovups         %%zmm22, (%%r13)                         \n"
		"   vmovups         %%zmm23, 64(%%r13)                       \n"

		"    leaq      (%%r13, %%r8, 4), %%r10                       \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		"   vmovups         %%zmm24, (%%r10)                         \n"
		"   vmovups         %%zmm25, 64(%%r10)                       \n"
		"   vmovups         %%zmm26, (%%r11)                         \n"
		"   vmovups         %%zmm27, 64(%%r11)                       \n"
		"    subq             $8, %%rdi                              \n"
		"   vmovups         %%zmm28, (%%r12)                         \n"
		"   vmovups         %%zmm29, 64(%%r12)                       \n"
		"   vmovups         %%zmm30, (%%r13)                         \n"
		"   vmovups         %%zmm31, 64(%%r13)                       \n"

		"    leaq      (%%r13, %%r8, 4), %%rcx                       \n" // C0

		".endm                                                       \n"

		//---------------------------------------------------------------

		".macro    KERNEL4x32_K1                                     \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm24               \n"
		"    addq              $128, %%rbx                           \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm25               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm4, %%zmm26               \n"
		"    addq              $16, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm5, %%zmm27               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm4, %%zmm28               \n"
		"   vmovups         (%%rbx), %%zmm6                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm5, %%zmm29               \n"

		"   vmovups         64(%%rbx), %%zmm7                        \n"
		"   vfmadd231ps        %%zmm3, %%zmm4, %%zmm30               \n"
		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   vfmadd231ps        %%zmm3, %%zmm5, %%zmm31               \n"
		"   prefetcht0         64(%%rbx)                             \n"

		".endm                                                       \n"

		".macro    KERNEL4x32_K2                                     \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"    addq              $128, %%rbx                           \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"    addq              $16, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   prefetcht0         256(%%rax)                            \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vmovups         (%%rbx), %%zmm4                          \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"    vfmadd231ps        %%zmm3, %%zmm6, %%zmm30              \n"
		"   vmovups         64(%%rbx), %%zmm5                        \n"
		"    vfmadd231ps        %%zmm3, %%zmm7, %%zmm31              \n"
		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   prefetcht0         64(%%rbx)                             \n"

		".endm                                                       \n"

		".macro    KERNEL4x32_END_K                                  \n"

		"   vbroadcastss    8(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm0, %%zmm7, %%zmm25               \n"

		"   vbroadcastss    12(%%rax), %%zmm3                        \n"
		"   vfmadd231ps        %%zmm1, %%zmm6, %%zmm26               \n"
		"    addq              $16, %%rax                            \n"
		"   vfmadd231ps        %%zmm1, %%zmm7, %%zmm27               \n"

		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm28               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm29               \n"

		"    vfmadd231ps        %%zmm3, %%zmm6, %%zmm30              \n"
		"    vfmadd231ps        %%zmm3, %%zmm7, %%zmm31              \n"

		".endm                                                       \n"

		".macro    ADD_C_4x32                                        \n"

		"   vmovups         (%%r10), %%zmm0                          \n"
		"    vaddps             %%zmm0, %%zmm24, %%zmm24             \n"
		"   vmovups         64(%%r10), %%zmm1                        \n"
		"    vaddps             %%zmm1, %%zmm25, %%zmm25             \n"
		"   vmovups         (%%r11), %%zmm2                          \n"
		"    vaddps             %%zmm2, %%zmm26, %%zmm26             \n"
		"   vmovups         64(%%r11), %%zmm3                        \n"
		"    vaddps             %%zmm3, %%zmm27, %%zmm27             \n"

		"   vmovups         (%%r12), %%zmm4                          \n"
		"    vaddps             %%zmm4, %%zmm28, %%zmm28             \n"
		"   vmovups         64(%%r12), %%zmm5                        \n"
		"    vaddps             %%zmm5, %%zmm29, %%zmm29             \n"
		"   vmovups         (%%r13), %%zmm6                          \n"
		"    vaddps             %%zmm6, %%zmm30, %%zmm30             \n"
		"   vmovups         64(%%r13), %%zmm7                        \n"
		"    vaddps             %%zmm7, %%zmm31, %%zmm31             \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3

		".endm                                                       \n"

		".macro    SAVE_4x32                                         \n"

		"   vmovups         %%zmm24, (%%r10)                         \n"
		"   vmovups         %%zmm25, 64(%%r10)                       \n"
		"   vmovups         %%zmm26, (%%r11)                         \n"
		"   vmovups         %%zmm27, 64(%%r11)                       \n"
		"    subq             $4, %%rdi                              \n"
		"   vmovups         %%zmm28, (%%r12)                         \n"
		"   vmovups         %%zmm29, 64(%%r12)                       \n"
		"   vmovups         %%zmm30, (%%r13)                         \n"
		"   vmovups         %%zmm31, 64(%%r13)                       \n"

		"    leaq      (%%r13, %%r8, 4), %%rcx                       \n" // C0

		".endm                                                       \n"

		//-----------------------------------------------------------------

		".macro    KERNEL1x32_K1                                     \n"
		"   vbroadcastss    4(%%rax), %%zmm2                         \n"
		"   vfmadd231ps        %%zmm0, %%zmm4, %%zmm24               \n"
		"    addq              $128, %%rbx                           \n"
		"   vmovups         (%%rbx), %%zmm6                          \n"
		"   prefetcht0         64(%%rbx)                             \n"
		"   vfmadd231ps        %%zmm0, %%zmm5, %%zmm25               \n"

		"    addq              $4, %%rax                             \n"
		"   vmovups         64(%%rbx), %%zmm7                        \n"
		"   prefetcht0         64(%%rax)                             \n"
		".endm                                                       \n"

		".macro    KERNEL1x32_K2                                     \n"
		"   vbroadcastss    4(%%rax), %%zmm0                         \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm24               \n"
		"    addq              $128, %%rbx                           \n"
		"   vmovups         (%%rbx), %%zmm4                          \n"
		"   prefetcht0         64(%%rbx)                             \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm25               \n"

		"    addq              $4, %%rax                             \n"
		"   vmovups         64(%%rbx), %%zmm5                        \n"
		"   prefetcht0         64(%%rax)                             \n"
		".endm                                                       \n"

		".macro    KERNEL1x32_END_K                                  \n"
		"   vbroadcastss    4(%%rax), %%zmm0                         \n"
		"   vfmadd231ps        %%zmm2, %%zmm6, %%zmm24               \n"
		"   vfmadd231ps        %%zmm2, %%zmm7, %%zmm25               \n"

		"    addq              $4, %%rax                             \n"

		".endm                                                       \n"

		".macro    ADD_C_1x32                                        \n"
		"   vmovups         (%%r10), %%zmm10                         \n"
		"    vaddps             %%zmm10, %%zmm24, %%zmm24            \n"
		"   vmovups         64(%%r10), %%zmm11                       \n"
		"    vaddps             %%zmm11, %%zmm25, %%zmm25            \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		".endm                                                       \n"

		".macro    SAVE_1x32                                         \n"
		"   vmovups         %%zmm24, (%%r10)                         \n"
		"   vmovups         %%zmm25, 64(%%r10)                       \n"
		"    subq             $1, %%rdi                              \n"

		"    leaq      (%%r10, %%r8, 4), %%rcx                       \n" // next C0
		".endm                                                       \n"

		//-----------------------------------------------------------------

		"SMM_NN_KERNEL12x32:                                         \n"
		"   movl    $0xaa, %%eax                                     \n"
		"   movl    $0xcc, %%r15d                                    \n"
		"   movl    $0x33, %%eax                                     \n"

		"   kmovd   %%eax, %%k1                                      \n"
		"   kmovd   %%r15d, %%k2                                     \n"
		"   kmovd   %%eax, %%k3                                      \n"

		"   mov     %[C], %%rcx                                      \n"
		"   mov   %[Cc], %%rsi                                       \n"
		"   mov     %[A], %%rax                                      \n"
		"   mov     %[B], %%rbx                                      \n"

		"   prefetcht0         (%%rax)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K(kc)
		"    mov      %[LN], %%r8                                    \n"
		"    mov      %[Bc], %%r14                                   \n"
		"    movq  %[M], %%rdi                                       \n"
		"    mov     %[k_tag], %%r15                                 \n" // kk=0把C存回内存, 否则加回对应的C位置

		"   prefetcht0         (%%rbx)                               \n"
		"    mov     %%rbx, %%r9                                     \n" // B

		"BEGIN_PACK:                                                 \n"

		"    mov     %%r9, %%rbx                                     \n" // B
		"   prefetcht0         (%%rbx)                               \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"   prefetcht2         (%%r10)                               \n"
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"   prefetcht2         (%%r11)                               \n"
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"   prefetcht2         (%%r12)                               \n"
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3
		"   prefetcht2         (%%r13)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K
		"    mov     %%r14, %%rbp                                    \n" // Bc

		"   vmovups        (%%rbx), %%zmm4                           \n" // B0-15
		"    vpxorq         %%zmm8, %%zmm8, %%zmm8                   \n"
		"    vpxorq         %%zmm9, %%zmm9, %%zmm9                   \n"
		"    vpxorq         %%zmm10, %%zmm10, %%zmm10                \n"
		"    vpxorq         %%zmm11, %%zmm11, %%zmm11                \n"
		"   vmovups     64(%%rbx), %%zmm5                            \n" // B16-31

		"    vpxorq         %%zmm12, %%zmm12, %%zmm12                \n"
		"    vpxorq         %%zmm13, %%zmm13, %%zmm13                \n"
		"    vpxorq         %%zmm14, %%zmm14, %%zmm14                \n"
		"    vpxorq         %%zmm15, %%zmm15, %%zmm15                \n"
		"    vpxorq         %%zmm16, %%zmm16, %%zmm16                \n"
		"    vpxorq         %%zmm17, %%zmm17, %%zmm17                \n"
		"    vpxorq         %%zmm18, %%zmm18, %%zmm18                \n"
		"    vpxorq         %%zmm19, %%zmm19, %%zmm19                \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n" // A0
		"   vbroadcastss    4(%%rax), %%zmm1                         \n" // A1

		"    vpxorq         %%zmm20, %%zmm20, %%zmm20                \n"
		"    vpxorq         %%zmm21, %%zmm21, %%zmm21                \n"
		"   prefetcht2         64(%%r10)                             \n"
		"    vpxorq         %%zmm22, %%zmm22, %%zmm22                \n"
		"    vpxorq         %%zmm23, %%zmm23, %%zmm23                \n"
		"   prefetcht2         64(%%r11)                             \n"
		"    vpxorq         %%zmm24, %%zmm24, %%zmm24                \n"
		"    vpxorq         %%zmm25, %%zmm25, %%zmm25                \n"
		"   prefetcht2         64(%%r12)                             \n"
		"    vpxorq         %%zmm26, %%zmm26, %%zmm26                \n"
		"    vpxorq         %%zmm27, %%zmm27, %%zmm27                \n"
		"   prefetcht2         64(%%r13)                             \n"
		"    vpxorq         %%zmm28, %%zmm28, %%zmm28                \n"
		"    vpxorq         %%zmm29, %%zmm29, %%zmm29                \n"
		"    vpxorq         %%zmm30, %%zmm30, %%zmm30                \n"
		"    vpxorq         %%zmm31, %%zmm31, %%zmm31                \n" // C:zmm8-31(24个)

		"    subq     $8, %%rdx                                      \n" // K-=8, cant process K<8 and K%8<>0

		"PACK_K_PREFETCH_C:                                          \n"
		"   leaq     (%%r13, %%r8, 4), %%r13                         \n"
		"   prefetcht2         (%%r13)                               \n"
		"   prefetcht2         64(%%r13)                             \n"

		"MAIN_PACK_K:                                                \n"

		"    KERNEL12x32_PACK_K1                                     \n"
		"    KERNEL12x32_PACK_K2                                     \n"
		"    KERNEL12x32_PACK_K1                                     \n"
		"    KERNEL12x32_PACK_K2                                     \n"
		"    KERNEL12x32_PACK_K1                                     \n"
		"    KERNEL12x32_PACK_K2                                     \n"
		"    KERNEL12x32_PACK_K1                                     \n"
		"   cmp     $0, %%rdx                                        \n"
		"    je         EDGE_PACK_K                                  \n"
		"    KERNEL12x32_PACK_K2                                     \n"
		"    subq     $8, %%rdx                                      \n"
		"   cmp   $64, %%rdx                                         \n"
		"   jbe     PACK_K_PREFETCH_C                                \n"
		"   jmp     MAIN_PACK_K                                      \n"

		"EDGE_PACK_K:                                                \n"
		"   leaq     (%%r12, %%r8, 4), %%r13                         \n"
		"    KERNEL12x32_PACK_END_K                                  \n"
		"    jmp      BEGIN_SAVE                                     \n"

		//-----------------------------------------------------------------

		"BEGIN_M:                                                    \n"

		"    mov     %%r14, %%rbx                                    \n" // Bc
		"   prefetcht0         (%%rbx)                               \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"   prefetcht1         (%%r10)                               \n"
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"   prefetcht1         (%%r11)                               \n"
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"   prefetcht1         (%%r12)                               \n"
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3
		"   prefetcht1         (%%r13)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K

		"   vmovups        (%%rbx), %%zmm4                           \n" // B0-15
		"    vpxorq         %%zmm8, %%zmm8, %%zmm8                   \n"
		"    vpxorq         %%zmm9, %%zmm9, %%zmm9                   \n"
		"    vpxorq         %%zmm10, %%zmm10, %%zmm10                \n"
		"    vpxorq         %%zmm11, %%zmm11, %%zmm11                \n"
		"   vmovups     64(%%rbx), %%zmm5                            \n" // B16-31

		"    vpxorq         %%zmm12, %%zmm12, %%zmm12                \n"
		"    vpxorq         %%zmm13, %%zmm13, %%zmm13                \n"
		"    vpxorq         %%zmm14, %%zmm14, %%zmm14                \n"
		"    vpxorq         %%zmm15, %%zmm15, %%zmm15                \n"
		"    vpxorq         %%zmm16, %%zmm16, %%zmm16                \n"
		"    vpxorq         %%zmm17, %%zmm17, %%zmm17                \n"
		"    vpxorq         %%zmm18, %%zmm18, %%zmm18                \n"
		"    vpxorq         %%zmm19, %%zmm19, %%zmm19                \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n" // A0
		"   vbroadcastss    4(%%rax), %%zmm1                         \n" // A1

		"    vpxorq         %%zmm20, %%zmm20, %%zmm20                \n"
		"    vpxorq         %%zmm21, %%zmm21, %%zmm21                \n"
		"    vpxorq         %%zmm22, %%zmm22, %%zmm22                \n"
		"    vpxorq         %%zmm23, %%zmm23, %%zmm23                \n"
		"    vpxorq         %%zmm24, %%zmm24, %%zmm24                \n"
		"    vpxorq         %%zmm25, %%zmm25, %%zmm25                \n"
		"    vpxorq         %%zmm26, %%zmm26, %%zmm26                \n"
		"    vpxorq         %%zmm27, %%zmm27, %%zmm27                \n"
		"    vpxorq         %%zmm28, %%zmm28, %%zmm28                \n"
		"    vpxorq         %%zmm29, %%zmm29, %%zmm29                \n"
		"    vpxorq         %%zmm30, %%zmm30, %%zmm30                \n"
		"    vpxorq         %%zmm31, %%zmm31, %%zmm31                \n"

		"    subq     $8, %%rdx                                      \n"

		"K_M12_PREFETCH_C:                                           \n"
		"   leaq     (%%r13, %%r8, 4), %%r13                         \n"
		"   prefetcht2         (%%r13)                               \n"
		"   prefetcht2         64(%%r13)                             \n"

		"MAIN_K_M12:                                                 \n"

		"    KERNEL12x32_K1                                          \n"
		"    KERNEL12x32_K2                                          \n"
		"    KERNEL12x32_K1                                          \n"
		"    KERNEL12x32_K2                                          \n"
		"    KERNEL12x32_K1                                          \n"
		"    KERNEL12x32_K2                                          \n"
		"    KERNEL12x32_K1                                          \n"
		"   cmp     $0, %%rdx                                        \n"
		"    je         EDGE_K                                       \n"
		"    KERNEL12x32_K2                                          \n"
		"   subq     $8, %%rdx                                       \n"
		"   cmp   $64, %%rdx                                         \n"
		"   jbe     K_M12_PREFETCH_C                                 \n"
		"   jmp     MAIN_K_M12                                       \n"

		"EDGE_K:                                                     \n"
		"   leaq     (%%r12, %%r8, 4), %%r13                         \n"
		"    KERNEL12x32_END_K                                       \n"

		"BEGIN_SAVE:                                                 \n"
		"    cmp     $0, %%r15                                       \n"
		"    je      SAVE_C                                          \n"
		"    ADD_C_12x32                                             \n"

		"SAVE_C:                                                     \n"
		"   cmpq     $1, %[packC]                                    \n"
		"   je       PACK_SAVE_C                                     \n"
		"    SAVE_12x32                                              \n"
		"   cmpq      $12, %%rdi                                     \n"
		"    jnb     BEGIN_M                                         \n" // 不小于（或等于）则跳转
		"   jmp   BEGIN_M8                                           \n"

		"PACK_SAVE_C:                                                \n"
		"   PACK_C_SAVE12x32                                         \n"
		"    cmpq      $12, %%rdi                                    \n"
		"    jnb     BEGIN_M                                         \n" // 不小于（或等于）则跳转

		//-----------------------------------------------------------------

		"BEGIN_M8:                                                   \n"
		"   cmpq      $8, %%rdi                                      \n" // M % 8
		"    jb       BEGIN_M4                                       \n" // 小于则跳转!!!改了

		"    mov     %%r14, %%rbx                                    \n" // Bc
		"   prefetcht0         (%%rbx)                               \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"   prefetcht2         (%%r10)                               \n"
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"   prefetcht2         (%%r11)                               \n"
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"   prefetcht2         (%%r12)                               \n"
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3
		"   prefetcht2         (%%r13)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K

		"   vmovups        (%%rbx), %%zmm4                           \n"
		"    vpxorq         %%zmm16, %%zmm16, %%zmm16                \n"
		"    vpxorq         %%zmm17, %%zmm17, %%zmm17                \n"
		"   vmovups     64(%%rbx), %%zmm5                            \n"
		"    vpxorq         %%zmm18, %%zmm18, %%zmm18                \n"
		"    vpxorq         %%zmm19, %%zmm19, %%zmm19                \n"

		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   vbroadcastss    4(%%rax), %%zmm1                         \n"

		"    vpxorq         %%zmm20, %%zmm20, %%zmm20                \n"
		"    vpxorq         %%zmm21, %%zmm21, %%zmm21                \n"
		"   prefetcht2         64(%%r10)                             \n"
		"    vpxorq         %%zmm22, %%zmm22, %%zmm22                \n"
		"    vpxorq         %%zmm23, %%zmm23, %%zmm23                \n"
		"   prefetcht2         64(%%r11)                             \n"
		"    vpxorq         %%zmm24, %%zmm24, %%zmm24                \n"
		"    vpxorq         %%zmm25, %%zmm25, %%zmm25                \n"
		"   prefetcht2         64(%%r12)                             \n"
		"    vpxorq         %%zmm26, %%zmm26, %%zmm26                \n"
		"    vpxorq         %%zmm27, %%zmm27, %%zmm27                \n"
		"   prefetcht2         64(%%r13)                             \n"
		"    vpxorq         %%zmm28, %%zmm28, %%zmm28                \n"
		"    vpxorq         %%zmm29, %%zmm29, %%zmm29                \n"
		"    vpxorq         %%zmm30, %%zmm30, %%zmm30                \n"
		"    vpxorq         %%zmm31, %%zmm31, %%zmm31                \n"

		"    subq     $8, %%rdx                                      \n"

		"K_M8_PREFETCH_C:                                            \n"
		"   leaq     (%%r13, %%r8, 4), %%r13                         \n"
		"   prefetcht2         (%%r13)                               \n"
		"   prefetcht2         64(%%r13)                             \n"

		"MAIN_K_M8:                                                  \n"

		"    KERNEL8x32_K1                                           \n"
		"    KERNEL8x32_K2                                           \n"
		"    KERNEL8x32_K1                                           \n"
		"    KERNEL8x32_K2                                           \n"
		"    KERNEL8x32_K1                                           \n"
		"    KERNEL8x32_K2                                           \n"
		"    KERNEL8x32_K1                                           \n"
		"   cmp     $0, %%rdx                                        \n"
		"    je         EDGE_K_M8                                    \n"
		"    KERNEL8x32_K2                                           \n"
		"    subq     $8, %%rdx                                      \n"
		"   cmp   $32, %%rdx                                         \n"
		"   jbe     K_M8_PREFETCH_C                                  \n"
		"   jmp     MAIN_K_M8                                        \n"

		"EDGE_K_M8:                                                  \n"
		"   leaq     (%%r12, %%r8, 4), %%r13                         \n"
		"    KERNEL8x32_END_K                                        \n"

		"    cmp     $0, %%r15                                       \n"
		"    je      SAVE_C_8x32                                     \n"
		"    ADD_C_8x32                                              \n"

		"SAVE_C_8x32:                                                \n"
		"    SAVE_8x32                                               \n"

		//----------------------------------------------------------------

		"BEGIN_M4:                                                   \n"

		"    cmpq      $4, %%rdi                                     \n" // M % 4
		"    jb       BEGIN_M1                                       \n" // 小于则跳转!!!改了

		"    mov     %%r14, %%rbx                                    \n" // Bc
		"   prefetcht0         (%%rbx)                               \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"   prefetcht2         (%%r10)                               \n"
		"    leaq     (%%r10, %%r8, 4), %%r11                        \n" // C1
		"   prefetcht2         (%%r11)                               \n"
		"    leaq     (%%r11, %%r8, 4), %%r12                        \n" // C2
		"   prefetcht2         (%%r12)                               \n"
		"    leaq     (%%r12, %%r8, 4), %%r13                        \n" // C3
		"   prefetcht2         (%%r13)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K

		"   vmovups        (%%rbx), %%zmm4                           \n"
		"   vmovups     64(%%rbx), %%zmm5                            \n"

		"   prefetcht2         64(%%r10)                             \n"
		"    vpxorq         %%zmm24, %%zmm24, %%zmm24                \n"
		"    vpxorq         %%zmm25, %%zmm25, %%zmm25                \n"
		"   vbroadcastss    (%%rax), %%zmm0                          \n"
		"   prefetcht2         64(%%r11)                             \n"
		"    vpxorq         %%zmm26, %%zmm26, %%zmm26                \n"
		"    vpxorq         %%zmm27, %%zmm27, %%zmm27                \n"
		"   vbroadcastss    4(%%rax), %%zmm1                         \n"
		"   prefetcht2         64(%%r12)                             \n"
		"    vpxorq         %%zmm28, %%zmm28, %%zmm28                \n"
		"    vpxorq         %%zmm29, %%zmm29, %%zmm29                \n"
		"   prefetcht2         64(%%r13)                             \n"
		"    vpxorq         %%zmm30, %%zmm30, %%zmm30                \n"
		"    vpxorq         %%zmm31, %%zmm31, %%zmm31                \n"

		"    subq     $8, %%rdx                                      \n"

		"MAIN_K_M4:                                                  \n"

		"    KERNEL4x32_K1                                           \n"
		"    KERNEL4x32_K2                                           \n"
		"    KERNEL4x32_K1                                           \n"
		"    KERNEL4x32_K2                                           \n"
		"    KERNEL4x32_K1                                           \n"
		"    KERNEL4x32_K2                                           \n"
		"    KERNEL4x32_K1                                           \n"
		"   cmp     $0, %%rdx                                        \n"
		"    je         EDGE_K_M4                                    \n"
		"    KERNEL4x32_K2                                           \n"

		"    subq     $8, %%rdx                                      \n"
		"   jmp     MAIN_K_M4                                        \n"

		"EDGE_K_M4:                                                  \n"

		"    KERNEL4x32_END_K                                        \n"

		"    cmp     $0, %%r15                                       \n"
		"    je      SAVE_C_4x32                                     \n"
		"    ADD_C_4x32                                              \n"

		"SAVE_C_4x32:                                                \n"
		"    SAVE_4x32                                               \n"

		//----------------------------------------------------------------

		"BEGIN_M1:                                                   \n"
		"    cmpq      $1, %%rdi                                     \n"
		"    jb       END_M                                          \n" // 小于则跳转

		"    mov     %%r14, %%rbx                                    \n" // Bc
		"   prefetcht0         (%%rbx)                               \n"

		"    mov      %%rcx, %%r10                                   \n" // C0
		"   prefetcht2         (%%r10)                               \n"

		"    mov     %[K], %%rdx                                     \n" // K

		"   vmovups        (%%rbx), %%zmm4                           \n" // B0-15
		"   vmovups     64(%%rbx), %%zmm5                            \n" // B16-31

		"   prefetcht2         64(%%r10)                             \n"
		"    vpxorq         %%zmm24, %%zmm24, %%zmm24                \n"
		"    vpxorq         %%zmm25, %%zmm25, %%zmm25                \n"
		"   vbroadcastss    (%%rax), %%zmm0                          \n" // A0

		"    subq     $8, %%rdx                                      \n"

		"MAIN_K_M1:                                                  \n"
		"    KERNEL1x32_K1                                           \n"
		"    KERNEL1x32_K2                                           \n"
		"    KERNEL1x32_K1                                           \n"
		"    KERNEL1x32_K2                                           \n"
		"    KERNEL1x32_K1                                           \n"
		"    KERNEL1x32_K2                                           \n"
		"    KERNEL1x32_K1                                           \n"
		"   cmp     $0, %%rdx                                        \n"
		"    je    EDGE_K_M1                                         \n"
		"    KERNEL1x32_K2                                           \n"

		"    subq     $8, %%rdx                                      \n"
		"   jmp   MAIN_K_M1                                          \n"

		"EDGE_K_M1:                                                  \n"
		"   KERNEL1x32_END_K                                         \n"

		"    cmp     $0, %%r15                                       \n"
		"    je      SAVE_C_1x32                                     \n"
		"    ADD_C_1x32                                              \n"

		"SAVE_C_1x32:                                                \n"
		"   SAVE_1x32                                                \n"

		"   cmpq      $1, %%rdi                                      \n"
		"    jnb     BEGIN_M1                                        \n" // 不小于（或等于）则跳转

		//----------------------------------------------------------------

		"END_M:                                                      \n"

		:
		:
		[C] "m"(C),
		[Cc] "m"(Cc),
		[A] "m"(A),
		[B] "m"(B),
		[M] "m"(M),
		[N] "m"(N),
		[K] "m"(K),
		[LN] "m"(LN),
		[LK] "m"(LK),
		[LNc] "m"(LNc),
		[Bc] "m"(Bc),
		[packC] "m"(packC)
		: "rax", "rbx", "rcx", "rdx", "rdi", "rsi", "rbp", "r8", "r9", "r10", "r11", "r12",
		  "r13", "r14", "r15", "zmm0", "zmm1", "zmm2", "zmm3", "zmm4", "zmm5",
		  "zmm6", "zmm7", "zmm8", "zmm9", "zmm10", "zmm11", "zmm12", "zmm13",
		  "zmm14", "zmm15", "zmm16", "zmm17", "zmm18", "zmm19", "zmm20", "zmm21",
		  "zmm22", "zmm23", "zmm24", "zmm25", "zmm26", "zmm27", "zmm28", "zmm29",
		  "zmm30", "zmm31", "memory"

	);
}
