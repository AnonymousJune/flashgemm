".macro    KERNEL12x4_PACK_K1                                \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm21               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm22               \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm26               \n"

"    leaq      (%%rbx, %%r8, 4), %%rbx                       \n"// B

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm27               \n"

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm29               \n"
"   vmovups         (%%rbx), %%xmm6                          \n"
"    addq              $48, %%rax                            \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm31               \n"
"   vmovups         %%xmm4, (%%rbp)                          \n"
"    addq              $16, %%rbp                            \n"//64->16                         

".endm                                                       \n"

".macro    KERNEL12x4_PACK_K2                                \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm21               \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm22               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"    leaq      (%%rbx, %%r8, 4), %%rbx                       \n"// B

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   prefetcht0         16(%%rbx)                             \n"//                  

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"   vmovups         (%%rbx), %%xmm4                          \n"
"    addq              $48, %%rax                            \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"
"   vmovups         %%xmm6, (%%rbp)                          \n"
"    addq              $16, %%rbp                            \n"//64->16                         

".endm                                                       \n"

".macro    KERNEL12x4_PACK_END_K                             \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm21               \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm22               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"    addq              $48, %%rax                            \n"

"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"
"   vmovups         %%xmm6, (%%rbp)                          \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"

".endm                                                       \n"

//-----------------------------------------------------------------------

".macro    KERNEL12x4_K1                                     \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm21               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm22               \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm26               \n"

"    addq              $16, %%rbx                            \n"// B

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm27               \n"

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm29               \n"
"   vmovups         (%%rbx), %%xmm6                          \n"
"    addq              $48, %%rax                            \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm31               \n"

".endm                                                       \n"

".macro    KERNEL12x4_K2                                     \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm21               \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm22               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"    addq             $16, %%rbx                             \n"//                

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   prefetcht0         16(%%rbx)                             \n"//                       

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"   vmovups         (%%rbx), %%xmm4                          \n"
"    addq              $48, %%rax                            \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"

".endm                                                       \n"

".macro    KERNEL12x4_END_K                                  \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm20               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm21               \n"

"   vbroadcastss    16(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm22               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    20(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm23               \n"

"   vbroadcastss    24(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    28(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   vbroadcastss    32(%%rax), %%xmm0                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"   vbroadcastss    36(%%rax), %%xmm1                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   vbroadcastss    40(%%rax), %%xmm2                        \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    44(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"   vmovups         (%%rbx), %%xmm4                          \n"
"    addq              $48, %%rax                            \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"

".endm                                                       \n"

".macro    ADD_C_12x4                                        \n"

"   vmovups         (%%r10), %%xmm0                          \n"
"    vaddps             %%xmm0, %%xmm20, %%xmm20             \n"
"   vmovups         (%%r11), %%xmm1                          \n"
"    vaddps             %%xmm1, %%xmm21, %%xmm21             \n"
"   vmovups         (%%r12), %%xmm2                          \n"
"    vaddps             %%xmm2, %%xmm22, %%xmm22             \n"
"   vmovups         (%%r13), %%xmm3                          \n"
"    vaddps             %%xmm3, %%xmm23, %%xmm23             \n"

"    leaq              (%%r13, %%r8, 4), %%r10               \n"// C0
"    leaq             (%%r10, %%r8, 4), %%r11                \n"// C1
"    leaq             (%%r11, %%r8, 4), %%r12                \n"// C2
"    leaq             (%%r12, %%r8, 4), %%r13                \n"// C3

"   vmovups         (%%r10), %%xmm4                          \n"
"    vaddps             %%xmm4, %%xmm24, %%xmm24             \n"
"   vmovups         (%%r11), %%xmm5                          \n"
"    vaddps             %%xmm5, %%xmm25, %%xmm25             \n"
"   vmovups         (%%r12), %%xmm6                          \n"
"    vaddps             %%xmm6, %%xmm26, %%xmm26             \n"
"   vmovups         (%%r13), %%xmm7                          \n"
"    vaddps             %%xmm7, %%xmm27, %%xmm27             \n"

"    leaq              (%%r13, %%r8, 4), %%r10               \n"// C0
"    leaq             (%%r10, %%r8, 4), %%r11                \n"// C1
"    leaq             (%%r11, %%r8, 4), %%r12                \n"// C2
"    leaq             (%%r12, %%r8, 4), %%r13                \n"// C3

"   vmovups         (%%r10), %%xmm0                          \n"
"    vaddps             %%xmm0, %%xmm28, %%xmm28             \n"
"   vmovups         (%%r11), %%xmm1                          \n"
"    vaddps             %%xmm1, %%xmm29, %%xmm29             \n"
"   vmovups         (%%r12), %%xmm2                          \n"
"    vaddps             %%xmm2, %%xmm30, %%xmm30             \n"
"   vmovups         (%%r13), %%xmm3                          \n"
"    vaddps             %%xmm3, %%xmm31, %%xmm31             \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

".endm                                                       \n"

".macro    SAVE_12x4                                         \n"

"   vmovups         %%xmm20, (%%r10)                         \n"
"   vmovups         %%xmm21, (%%r11)                         \n"
"    subq             $12, %%rdi                             \n"
"   vmovups         %%xmm22, (%%r12)                         \n"
"   vmovups         %%xmm23, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%r10                       \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

"   vmovups         %%xmm24, (%%r10)                         \n"
"   vmovups         %%xmm25, (%%r11)                         \n"
"   vmovups         %%xmm26, (%%r12)                         \n"
"   vmovups         %%xmm27, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%r10                       \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

"   vmovups         %%xmm28, (%%r10)                         \n"
"   vmovups         %%xmm29, (%%r11)                         \n"
"   vmovups         %%xmm30, (%%r12)                         \n"
"   vmovups         %%xmm31, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%rcx                       \n"// C0

".endm                                                       \n"

//-----------------------------------------------------------------------

".macro    KERNEL8x4_K1                                      \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm24               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm25               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    16(%%rax), %%xmm8                        \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm26               \n"

"   vbroadcastss    20(%%rax), %%xmm9                        \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm27               \n"

"   vbroadcastss    24(%%rax), %%xmm10                       \n"
"   vfmadd231ps        %%xmm8, %%xmm4, %%xmm28               \n"

"   prefetcht0         16(%%rbx)                             \n"//                        

"   vbroadcastss    28(%%rax), %%xmm11                       \n"
"   vfmadd231ps        %%xmm9, %%xmm4, %%xmm29               \n"
"    addq              $16, %%rbx                            \n"//
"   vfmadd231ps        %%xmm10, %%xmm4, %%xmm30              \n"

"    addq              $32, %%rax                            \n"
"   vmovups         (%%rbx), %%xmm6                          \n"
"   vfmadd231ps        %%xmm11, %%xmm4, %%xmm31              \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"

".endm                                                       \n"

".macro    KERNEL8x4_K2                                      \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    16(%%rax), %%xmm8                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"   vbroadcastss    20(%%rax), %%xmm9                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   vbroadcastss    24(%%rax), %%xmm10                       \n"
"   vfmadd231ps        %%xmm8, %%xmm6, %%xmm28               \n"

"   prefetcht0         16(%%rbx)                             \n"//           

"   vbroadcastss    28(%%rax), %%xmm11                       \n"
"   vfmadd231ps        %%xmm9, %%xmm6, %%xmm29               \n"
"    addq              $16, %%rbx                            \n"// B
"   vfmadd231ps        %%xmm10, %%xmm6, %%xmm30              \n"

"    addq              $32, %%rax                            \n"
"   vmovups         (%%rbx), %%xmm4                          \n"
"   vfmadd231ps        %%xmm11, %%xmm6, %%xmm31              \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"

".endm                                                       \n"

".macro    KERNEL8x4_END_K                                   \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm24               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm25               \n"

"   prefetcht0         256(%%rax)                            \n"

"   vbroadcastss    16(%%rax), %%xmm8                        \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm26               \n"

"   vbroadcastss    20(%%rax), %%xmm9                        \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm27               \n"

"   vbroadcastss    24(%%rax), %%xmm10                       \n"
"   vfmadd231ps        %%xmm8, %%xmm6, %%xmm28               \n"

"   vbroadcastss    28(%%rax), %%xmm11                       \n"
"   vfmadd231ps        %%xmm9, %%xmm6, %%xmm29               \n"
"   vfmadd231ps        %%xmm10, %%xmm6, %%xmm30              \n"

"    addq              $32, %%rax                            \n"
"   vfmadd231ps        %%xmm11, %%xmm6, %%xmm31              \n"

".endm                                                       \n"

".macro    ADD_C_8x4                                         \n"

"   vmovups         (%%r10), %%xmm4                          \n"
"    vaddps             %%xmm4, %%xmm24, %%xmm24             \n"
"   vmovups         (%%r11), %%xmm5                          \n"
"    vaddps             %%xmm5, %%xmm25, %%xmm25             \n"
"   vmovups         (%%r12), %%xmm6                          \n"
"    vaddps             %%xmm6, %%xmm26, %%xmm26             \n"
"   vmovups         (%%r13), %%xmm7                          \n"
"    vaddps             %%xmm7, %%xmm27, %%xmm27             \n"

"    leaq              (%%r13, %%r8, 4), %%r10               \n"// C0
"    leaq             (%%r10, %%r8, 4), %%r11                \n"// C1
"    leaq             (%%r11, %%r8, 4), %%r12                \n"// C2
"    leaq             (%%r12, %%r8, 4), %%r13                \n"// C3

"   vmovups         (%%r10), %%xmm0                          \n"
"    vaddps             %%xmm0, %%xmm28, %%xmm28             \n"
"   vmovups         (%%r11), %%xmm1                          \n"
"    vaddps             %%xmm1, %%xmm29, %%xmm29             \n"
"   vmovups         (%%r12), %%xmm2                          \n"
"    vaddps             %%xmm2, %%xmm30, %%xmm30             \n"
"   vmovups         (%%r13), %%xmm3                          \n"
"    vaddps             %%xmm3, %%xmm31, %%xmm31             \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

".endm                                                       \n"

".macro    SAVE_8x4                                          \n"

"   vmovups         %%xmm24, (%%r10)                         \n"
"   vmovups         %%xmm25, (%%r11)                         \n"
"    subq             $8, %%rdi                              \n"
"   vmovups         %%xmm26, (%%r12)                         \n"
"   vmovups         %%xmm27, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%r10                       \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

"   vmovups         %%xmm28, (%%r10)                         \n"
"   vmovups         %%xmm29, (%%r11)                         \n"
"   vmovups         %%xmm30, (%%r12)                         \n"
"   vmovups         %%xmm31, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%rcx                       \n"// C0

".endm                                                       \n"

//-----------------------------------------------------------------------

".macro    KERNEL4x4_K1                                      \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm28               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm4, %%xmm29               \n"
"    addq              $16, %%rax                            \n"

"   prefetcht0         256(%%rax)                            \n"
"    addq              $16, %%rbx                            \n"// B
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm4, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm4, %%xmm31               \n"

"   prefetcht0         16(%%rbx)                             \n"
"   vmovups         (%%rbx), %%xmm6                          \n"

".endm                                                       \n"

".macro    KERNEL4x4_K2                                      \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"    addq              $16, %%rax                            \n"

"   prefetcht0         256(%%rax)                            \n"
"    addq              $16, %%rbx                            \n"// B
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"

"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"

"   vmovups         (%%rbx), %%xmm4                          \n"
"   prefetcht0         16(%%rbx)                             \n"

".endm                                                       \n"

".macro    KERNEL4x4_END_K                                   \n"

"   vbroadcastss    8(%%rax), %%xmm2                         \n"
"   vfmadd231ps        %%xmm0, %%xmm6, %%xmm28               \n"

"   vbroadcastss    12(%%rax), %%xmm3                        \n"
"   vfmadd231ps        %%xmm1, %%xmm6, %%xmm29               \n"
"    addq              $16, %%rax                            \n"

"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"
"   vfmadd231ps        %%xmm3, %%xmm6, %%xmm31               \n"

".endm                                                       \n"

".macro    ADD_C_4x4                                         \n"

"   vmovups         (%%r10), %%xmm0                          \n"
"    vaddps             %%xmm0, %%xmm28, %%xmm28             \n"
"   vmovups         (%%r11), %%xmm1                          \n"
"    vaddps             %%xmm1, %%xmm29, %%xmm29             \n"
"   vmovups         (%%r12), %%xmm2                          \n"
"    vaddps             %%xmm2, %%xmm30, %%xmm30             \n"
"   vmovups         (%%r13), %%xmm3                          \n"
"    vaddps             %%xmm3, %%xmm31, %%xmm31             \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3

".endm                                                       \n"

".macro    SAVE_4x4                                          \n"
"   subq             $4, %%rdi                               \n"

"   vmovups         %%xmm28, (%%r10)                         \n"
"   vmovups         %%xmm29, (%%r11)                         \n"
"   vmovups         %%xmm30, (%%r12)                         \n"
"   vmovups         %%xmm31, (%%r13)                         \n"

"    leaq      (%%r13, %%r8, 4), %%rcx                       \n"// C0

".endm                                                       \n"

//-----------------------------------------------------------------------


".macro    KERNEL1x4_K1                                      \n"
"   prefetcht0         256(%%rax)                            \n"
"   addq              $4, %%rax                              \n"

"   vbroadcastss    (%%rax), %%xmm2                          \n"
"   vfmadd231ps        %%xmm0, %%xmm4, %%xmm30               \n"
							           		
"    addq              $16, %%rbx                            \n"// B    
"   vmovups            (%%rbx), %%xmm6                       \n"
"   prefetcht0         16(%%rbx)                             \n"

".endm                                                       \n"

".macro    KERNEL1x4_K2                                      \n"
"   prefetcht0         256(%%rax)                            \n"
"    addq              $4, %%rax                             \n"

"   vbroadcastss    (%%rax), %%xmm0                          \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"
                  		
"    addq              $16, %%rbx                            \n"// B        
"   vmovups            (%%rbx), %%xmm4                       \n"
"   prefetcht0         16(%%rbx)                             \n"

".endm                                                       \n"

".macro    KERNEL1x4_END_K                                   \n"
"   vfmadd231ps        %%xmm2, %%xmm6, %%xmm30               \n"
"    addq              $4, %%rax                             \n"
".endm                                                       \n"

".macro    ADD_C_1x4                                         \n"
"   vmovups         (%%r10), %%xmm2                          \n"
"    vaddps             %%xmm2, %%xmm30, %%xmm30             \n"
"    mov      %%rcx, %%r10                                   \n"// C0
".endm                                                       \n"

".macro    SAVE_1x4                                          \n"
"   vmovups         %%xmm30, (%%r10)                         \n"
"   subq             $1, %%rdi                               \n"
"    leaq      (%%r10, %%r8, 4), %%rcx                       \n"// C0

".endm                                                       \n"

//-----------------------------------------------------------------

"SMM_NN_KERNEL12x4:                                          \n"

"   mov     %[C], %%rcx                                      \n"
"   mov     %[A], %%rax                                      \n"
"   mov     %[B], %%rbx                                      \n"

"   prefetcht0         (%%rax)                               \n"

"    mov     %[K], %%rdx                                     \n"// K
"    mov      %[LN], %%r8                                    \n"
"    mov      %[Bc], %%r14                                   \n"
"    mov      %[M], %%rdi                                    \n"
"    mov     %[k_tag], %%r15                                 \n"

"   prefetcht0         (%%rbx)                               \n"
"    mov     %%rbx, %%r9                                     \n"// B
"    mov     %%rdx, %%rsi                                    \n"// K

"BEGIN_PACK12x4:                                             \n"

"    mov     %%r9, %%rbx                                     \n"// B
"   prefetcht0         (%%rbx)                               \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"   prefetcht1         (%%r10)                               \n"
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"   prefetcht1         (%%r11)                               \n"
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"   prefetcht1         (%%r12)                               \n"
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3
"   prefetcht1         (%%r13)                               \n"

"    mov     %%rsi, %%rdx                                    \n"// K
"    mov     %%r14, %%rbp                                    \n"// Bc

"   vmovups        (%%rbx), %%xmm4                           \n"
"    vpxorq         %%xmm20, %%xmm20, %%xmm20                \n"
"    vpxorq         %%xmm21, %%xmm21, %%xmm21                \n"
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"    vpxorq         %%xmm22, %%xmm22, %%xmm22                \n"
"    vpxorq         %%xmm23, %%xmm23, %%xmm23                \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"    vpxorq         %%xmm24, %%xmm24, %%xmm24                \n"
"    vpxorq         %%xmm25, %%xmm25, %%xmm25                \n"
"    vpxorq         %%xmm26, %%xmm26, %%xmm26                \n"
"    vpxorq         %%xmm27, %%xmm27, %%xmm27                \n"
"    vpxorq         %%xmm28, %%xmm28, %%xmm28                \n"
"    vpxorq         %%xmm29, %%xmm29, %%xmm29                \n"
"    vpxorq         %%xmm30, %%xmm30, %%xmm30                \n"
"    vpxorq         %%xmm31, %%xmm31, %%xmm31                \n"

"    subq     $8, %%rdx                                      \n"

"MAIN_PACK_K12x4:                                            \n"

"    KERNEL12x4_PACK_K1                                      \n"
"    KERNEL12x4_PACK_K2                                      \n"
"    KERNEL12x4_PACK_K1                                      \n"
"    KERNEL12x4_PACK_K2                                      \n"
"    KERNEL12x4_PACK_K1                                      \n"
"    KERNEL12x4_PACK_K2                                      \n"
"    KERNEL12x4_PACK_K1                                      \n"
"   cmp     $0, %%rdx                                        \n"
"    je         EDGE_PACK_K12x4                              \n"
"    KERNEL12x4_PACK_K2                                      \n"

"    subq     $8, %%rdx                                      \n"
"   jmp     MAIN_PACK_K12x4                                  \n"

"EDGE_PACK_K12x4:                                            \n"

"    KERNEL12x4_PACK_END_K                                   \n"
"    jmp      BEGIN_SAVE_12x4                                \n"

//-----------------------------------------------------------------

"BEGIN_M12x4:                                                \n"

"    mov     %%r14, %%rbx                                    \n"// Bc
"   prefetcht0         (%%rbx)                               \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"   prefetcht1         (%%r10)                               \n"
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"   prefetcht1         (%%r11)                               \n"
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"   prefetcht1         (%%r12)                               \n"
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3
"   prefetcht1         (%%r13)                               \n"

"    mov     %%rsi, %%rdx                                    \n"// K                                    

"   vmovups        (%%rbx), %%xmm4                           \n"
"    vpxorq         %%xmm20, %%xmm20, %%xmm20                \n"
"    vpxorq         %%xmm21, %%xmm21, %%xmm21                \n"
"    vpxorq         %%xmm22, %%xmm22, %%xmm22                \n"
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"    vpxorq         %%xmm23, %%xmm23, %%xmm23                \n"
"    vpxorq         %%xmm24, %%xmm24, %%xmm24                \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"    vpxorq         %%xmm25, %%xmm25, %%xmm25                \n"
"    vpxorq         %%xmm26, %%xmm26, %%xmm26                \n"
"    vpxorq         %%xmm27, %%xmm27, %%xmm27                \n"
"    vpxorq         %%xmm28, %%xmm28, %%xmm28                \n"
"    vpxorq         %%xmm29, %%xmm29, %%xmm29                \n"
"    vpxorq         %%xmm30, %%xmm30, %%xmm30                \n"
"    vpxorq         %%xmm31, %%xmm31, %%xmm31                \n"

"    subq     $8, %%rdx                                      \n"

"MAIN_K12x4:                                                 \n"

"    KERNEL12x4_K1                                           \n"
"    KERNEL12x4_K2                                           \n"
"    KERNEL12x4_K1                                           \n"
"    KERNEL12x4_K2                                           \n"
"    KERNEL12x4_K1                                           \n"
"    KERNEL12x4_K2                                           \n"
"    KERNEL12x4_K1                                           \n"
"   cmp     $0, %%rdx                                        \n"
"    je         EDGE_K12x4                                   \n"
"    KERNEL12x4_K2                                           \n"

"    subq     $8, %%rdx                                      \n"
"   jmp     MAIN_K12x4                                       \n"

"EDGE_K12x4:                                                 \n"

"    KERNEL12x4_END_K                                        \n"

"BEGIN_SAVE_12x4:                                            \n"
"    cmp     $0, %%r15                                       \n"
"    je      SAVE_C12x4                                      \n"
"    ADD_C_12x4                                              \n"

"SAVE_C12x4:                                                 \n"
"    SAVE_12x4                                               \n"

"    cmpq      $12, %%rdi                                    \n"
"    jnb     BEGIN_M12x4                                     \n"// 不小于（或等于）则跳转                             

//------------------------------------------------------------------

"BEGIN_M8_N4:                                                \n"
"   cmpq      $8, %%rdi                                      \n"// M % 8
"    jb       BEGIN_M4_N4                                    \n"//小于则跳转

"    mov     %%r14, %%rbx                                    \n"// Bc
"   prefetcht0         (%%rbx)                               \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"   prefetcht1         (%%r10)                               \n"
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"   prefetcht1         (%%r11)                               \n"
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"   prefetcht1         (%%r12)                               \n"
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3
"   prefetcht1         (%%r13)                               \n"

"    mov     %%rsi, %%rdx                                    \n"// K
"   vmovups        (%%rbx), %%xmm4                           \n"
"    vpxorq         %%xmm24, %%xmm24, %%xmm24                \n"
"    vpxorq         %%xmm25, %%xmm25, %%xmm25                \n"
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"    vpxorq         %%xmm26, %%xmm26, %%xmm26                \n"
"    vpxorq         %%xmm27, %%xmm27, %%xmm27                \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"    vpxorq         %%xmm28, %%xmm28, %%xmm28                \n"
"    vpxorq         %%xmm29, %%xmm29, %%xmm29                \n"
"    vpxorq         %%xmm30, %%xmm30, %%xmm30                \n"
"    vpxorq         %%xmm31, %%xmm31, %%xmm31                \n"

"    subq     $8, %%rdx                                      \n"

"MAIN_K_M8_N4:                                               \n"

"    KERNEL8x4_K1                                            \n"
"    KERNEL8x4_K2                                            \n"
"    KERNEL8x4_K1                                            \n"
"    KERNEL8x4_K2                                            \n"
"    KERNEL8x4_K1                                            \n"
"    KERNEL8x4_K2                                            \n"
"    KERNEL8x4_K1                                            \n"
"   cmp     $0, %%rdx                                        \n"
"    je         EDGE_K_M8_N4                                 \n"
"    KERNEL8x4_K2                                            \n"

"    subq     $8, %%rdx                                      \n"
"   jmp     MAIN_K_M8_N4                                     \n"

"EDGE_K_M8_N4:                                               \n"

"    KERNEL8x4_END_K                                         \n"

"    cmp     $0, %%r15                                       \n"
"    je      SAVE_C_8x4                                      \n"
"    ADD_C_8x4                                               \n"

"SAVE_C_8x4:                                                 \n"
"    SAVE_8x4                                                \n"

//----------------------------------------------------------------

"BEGIN_M4_N4:                                                \n"

"    cmpq      $4, %%rdi                                     \n"// M % 4
"    jb       BEGIN_M1_N4                                    \n"

"    mov     %%r14, %%rbx                                    \n"// Bc
"   prefetcht0         (%%rbx)                               \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"   prefetcht1         (%%r10)                               \n"
"    leaq     (%%r10, %%r8, 4), %%r11                        \n"// C1
"   prefetcht1         (%%r11)                               \n"
"    leaq     (%%r11, %%r8, 4), %%r12                        \n"// C2
"   prefetcht1         (%%r12)                               \n"
"    leaq     (%%r12, %%r8, 4), %%r13                        \n"// C3
"   prefetcht1         (%%r13)                               \n"

"    mov     %%rsi, %%rdx                                    \n"// K

"   vmovups        (%%rbx), %%xmm4                           \n"
"   vbroadcastss    (%%rax), %%xmm0                          \n"
"    vpxorq         %%xmm28, %%xmm28, %%xmm28                \n"
"    vpxorq         %%xmm29, %%xmm29, %%xmm29                \n"
"   vbroadcastss    4(%%rax), %%xmm1                         \n"
"    vpxorq         %%xmm30, %%xmm30, %%xmm30                \n"
"    vpxorq         %%xmm31, %%xmm31, %%xmm31                \n"

"    subq     $8, %%rdx                                      \n"

"MAIN_K_M4_N4:                                               \n"

"    KERNEL4x4_K1                                            \n"
"    KERNEL4x4_K2                                            \n"
"    KERNEL4x4_K1                                            \n"
"    KERNEL4x4_K2                                            \n"
"    KERNEL4x4_K1                                            \n"
"    KERNEL4x4_K2                                            \n"
"    KERNEL4x4_K1                                            \n"
"   cmp     $0, %%rdx                                        \n"
"    je         EDGE_K_M4_N4                                 \n"
"    KERNEL4x4_K2                                            \n"

"    subq     $8, %%rdx                                      \n"
"   jmp     MAIN_K_M4_N4                                     \n"

"EDGE_K_M4_N4:                                               \n"

"    KERNEL4x4_END_K                                         \n"

"    cmp     $0, %%r15                                       \n"
"    je      SAVE_C_4x4                                      \n"
"    ADD_C_4x4                                               \n"

"SAVE_C_4x4:                                                 \n"
"    SAVE_4x4                                                \n"


//----------------------------------------------------------------

"BEGIN_M1_N4:                                                \n"
"    cmpq      $1, %%rdi                                     \n"// M % 1
"    jb       END_M_N4                                       \n"

"    mov     %%r14, %%rbx                                    \n"// Bc
"   prefetcht0         (%%rbx)                               \n"

"    mov      %%rcx, %%r10                                   \n"// C0
"   prefetcht1         (%%r10)                               \n"

"    mov     %%rsi, %%rdx                                    \n"// K

"   vmovups        (%%rbx), %%xmm4                           \n"
"   vbroadcastss    (%%rax), %%xmm0                          \n"// A0                
"    vpxorq         %%xmm30, %%xmm30, %%xmm30                \n"

"    subq     $8, %%rdx                                      \n"

"MAIN_K_M1_N4:                                               \n"
"    KERNEL1x4_K1                                            \n"
"    KERNEL1x4_K2                                            \n"
"    KERNEL1x4_K1                                            \n"
"    KERNEL1x4_K2                                            \n"
"    KERNEL1x4_K1                                            \n"
"    KERNEL1x4_K2                                            \n"
"    KERNEL1x4_K1                                            \n"
"   cmp     $0, %%rdx                                        \n"
"    je         EDGE_K_M1_N4                                 \n"
"    KERNEL1x4_K2                                            \n"

"    subq     $8, %%rdx                                      \n"
"   jmp     MAIN_K_M1_N4                                     \n"

"EDGE_K_M1_N4:                                               \n"
"    KERNEL1x4_END_K                                         \n"

"    cmp     $0, %%r15                                       \n"
"    je      SAVE_C_1x4                                      \n"
"    ADD_C_1x4                                               \n"
									
"SAVE_C_1x4:                                                 \n"
"    SAVE_1x4                                                \n"
"   cmpq      $1, %%rdi                                      \n"
"    jnb     BEGIN_M1_N4                                     \n"//不小于（或等于）则跳转                                    

//-----------------------------------------------------------------


"END_M_N4:                                                   \n"

