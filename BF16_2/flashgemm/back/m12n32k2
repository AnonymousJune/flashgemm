.macro bf16_pack_b_n32
   vpunpcklwd  %%zmm7, %%zmm6, %%zmm4                       
   vpunpckhwd  %%zmm7, %%zmm6, %%zmm5                    

   vextracti32x4   $0x1, %%zmm4, %%xmm0                     
   vextracti32x4   $0x2, %%zmm4, %%xmm1                     
   vextracti32x4   $0x3, %%zmm4, %%xmm2                     

   vextracti32x4   $0x0, %%zmm5, %%xmm3                     
   vextracti32x4   $0x1, %%zmm5, %%xmm6                     
   vextracti32x4   $0x2, %%zmm5, %%xmm7                     
      
   vinserti32x4    $0x2, %%xmm0, %%zmm4, %%zmm4
   vinserti32x4    $0x0, %%xmm1, %%zmm5, %%zmm5
   vinserti32x4    $0x2, %%xmm2, %%zmm5, %%zmm5

   vbroadcastss    (%%rax), %%zmm0                           // A0 in zmm0
   vbroadcastss    4(%%rax), %%zmm1                          // A1 in zmm1

   vinserti32x4    $0x1, %%xmm3, %%zmm4, %%zmm4
   vinserti32x4    $0x3, %%xmm6, %%zmm4, %%zmm4
   vinserti32x4    $0x1, %%xmm7, %%zmm5, %%zmm5

   vmovups         %%zmm4, (%%rbp)                           // store back B
   vmovups         %%zmm5, 64(%%rbp)                         // store back B
   addq            $128, %%rbp
.endm

.macro bf16_kernel_m12n32k2_pack                
   bf16_pack_b_n32

   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm19                 

   vmovdqu16     (%%rbx), %%zmm6                               // load next B
   prefetcht2  64(%%rbx)                                    
   leaq        (%%rbx, %%r8, 2), %%rbx                      

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm21                 

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm23                 

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm25                 

   vmovdqu16         (%%rbx), %%zmm7                         // load next B
   prefetcht2  64(%%rbx)                                    
   leaq        (%%rbx, %%r8, 2), %%rbx                      

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm27                 
   addq          $48, %%rax                                 

   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm29                 

   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm31                 
.endm                                                       

.macro bf16_kernel_m12n32k2_pack_end // deference is no prefetch A and B 
   bf16_pack_b_n32                           

   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm19                 

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm21                 

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm23                 

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm25                 

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm27                 
   addq          $48, %%rax                                 

   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm29                 

   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm31                 
.endm  

.macro    bf16_kernel_m12n32k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        

   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm19                 

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm21                 

    addq              $128, %%rbx                           

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm23                 

   prefetcht0         64(%%rbx)                             

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm24                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm25                 

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm26                 
   vmovdqu16         (%%rbx), %%zmm6                          
    addq              $48, %%rax                            
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm27                 

   vbroadcastss    (%%rax), %%zmm0                          
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm28                 
   vmovdqu16         64(%%rbx), %%zmm7                        
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm29                 

   vbroadcastss    4(%%rax), %%zmm1                         
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm30                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm31                 
.endm                                                       

.macro    bf16_kernel_m12n32k2_2                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm19                 

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm21                 

   addq              $128, %%rbx                           

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm23                 

   prefetcht0         64(%%rbx)                             

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm24                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm25                 

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm26                 
   vmovdqu16         (%%rbx), %%zmm4                          
    addq              $48, %%rax                            
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm27                 

   vbroadcastss    (%%rax), %%zmm0                          
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm28                 
   vmovdqu16         64(%%rbx), %%zmm5                        
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm29                 

   vbroadcastss    4(%%rax), %%zmm1                         
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm30                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm31                 
.endm                                                       

.macro    bf16_kernel_m12n32k2_end                          
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9                  
   vbroadcastss    12(%%rax), %%zmm3                        

   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm19                 

   vbroadcastss    32(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm21                 

   vbroadcastss    36(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm23                 

   vbroadcastss    40(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm24                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm25                 

   vbroadcastss    44(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm26                 
    addq              $48, %%rax                            
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm27                 

   vdpbf16ps        %%zmm2, %%zmm6, %%zmm28                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm29                 

   vdpbf16ps        %%zmm3, %%zmm6, %%zmm30                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm31                 
.endm                                                        

.macro    bf16_save_c_m12n32                                   
   vmovups         %%zmm8, (%%r10)                          
   vmovups         %%zmm9, 64(%%r10)                        
   vmovups         %%zmm10, (%%r11)                         
   vmovups         %%zmm11, 64(%%r11)                       
   vmovups         %%zmm12, (%%r12)                         
   vmovups         %%zmm13, 64(%%r12)                       
   vmovups         %%zmm14, (%%r13)                         
   vmovups         %%zmm15, 64(%%r13)                       

    leaq  (%%r13, %%r8, 4), %%r10                            // C0
    leaq     (%%r10, %%r8, 4), %%r11                         // C1
    leaq     (%%r11, %%r8, 4), %%r12                         // C2
    leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm16, (%%r10)                         
   vmovups         %%zmm17, 64(%%r10)                       
   vmovups         %%zmm18, (%%r11)                         
   vmovups         %%zmm19, 64(%%r11)                       
   vmovups         %%zmm20, (%%r12)                         
   vmovups         %%zmm21, 64(%%r12)                       
   vmovups         %%zmm22, (%%r13)                         
   vmovups         %%zmm23, 64(%%r13)                       

    leaq  (%%r13, %%r8, 4), %%r10                            // C0
    leaq     (%%r10, %%r8, 4), %%r11                         // C1
    leaq     (%%r11, %%r8, 4), %%r12                         // C2
    leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm24, (%%r10)                         
   vmovups         %%zmm25, 64(%%r10)                       
   vmovups         %%zmm26, (%%r11)                         
   vmovups         %%zmm27, 64(%%r11)                       
    subq             $12, %%rdi                             
   vmovups         %%zmm28, (%%r12)                         
   vmovups         %%zmm29, 64(%%r12)                       
   vmovups         %%zmm30, (%%r13)                         
   vmovups         %%zmm31, 64(%%r13)                       

    leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

.macro    bf16_save_c_m12n32_2
   vcvtne2ps2bf16  %%zmm8, %%zmm9, %%zmm0                                   
   vcvtne2ps2bf16  %%zmm10, %%zmm11, %%zmm1                                   
   vcvtne2ps2bf16  %%zmm12, %%zmm13, %%zmm2                                   
   vcvtne2ps2bf16  %%zmm14, %%zmm15, %%zmm3                                   
   vmovups         %%zmm0, (%%r10)                          
   vmovups         %%zmm1, 64(%%r10)                        
   vmovups         %%zmm2, 128(%%r10)                         
   vmovups         %%zmm3, 192(%%r10)                                              
   
   addq           $256, %%r10

   vcvtne2ps2bf16  %%zmm16, %%zmm17, %%zmm4
   vcvtne2ps2bf16  %%zmm18, %%zmm19, %%zmm5
   vcvtne2ps2bf16  %%zmm20, %%zmm21, %%zmm6
   vcvtne2ps2bf16  %%zmm22, %%zmm23, %%zmm7
   vmovups         %%zmm4, (%%r10)                         
   vmovups         %%zmm5, 64(%%r10)                       
   vmovups         %%zmm6, 128(%%r10)                         
   vmovups         %%zmm7, 192(%%r10)                                            

   addq           $256, %%r10

   vcvtne2ps2bf16  %%zmm24, %%zmm25, %%zmm8
   vcvtne2ps2bf16  %%zmm26, %%zmm27, %%zmm9
   vcvtne2ps2bf16  %%zmm28, %%zmm29, %%zmm10
   vcvtne2ps2bf16  %%zmm30, %%zmm31, %%zmm11
   vmovups         %%zmm8, (%%r10)                         
   vmovups         %%zmm9, 64(%%r10)                       
   vmovups         %%zmm10, 128(%%r10)                         
   vmovups         %%zmm11, 192(%%r10)                       

   subq           $12, %%rdi
   addq           $256, %%r10
.endm  

//-----------------------------------------------------------------

.macro    bf16_kernel_m8n32k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm11                 

   prefetcht0      256(%%rax)
   addq             $128, %%rbx
   prefetcht0       64(%%rbx)                      

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm13
   vmovdqu16        (%%rbx), %%zmm6                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm15

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm17                 
   vmovdqu16        64(%%rbx), %%zmm7

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm19
   
   addq              $32, %%rax
   vbroadcastss     (%%rax), %%zmm0
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm20
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm21            
                          
   vbroadcastss     4(%%rax), %%zmm1                 
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm23                 
.endm                                                       

.macro    bf16_kernel_m8n32k2_2                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 

   prefetcht0         256(%%rax)
   addq              $128, %%rbx                            
   prefetcht0         64(%%rbx)

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13

   vbroadcastss    20(%%rax), %%zmm1
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm17
   vmovdqu16       (%%rbx), %%zmm4                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm19
   vmovdqu16       64(%%rbx), %%zmm5
   
   addq             $32, %%rax

   vbroadcastss    (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm21                 

   vbroadcastss    4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm23                              
.endm                                                       

.macro    bf16_kernel_m8n32k2_end                            
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9                  
   
   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13                 

   vbroadcastss    20(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15                 

   vbroadcastss    24(%%rax), %%zmm2                        
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm16                 
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm17                 

   vbroadcastss    28(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm18                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm19
   
   addq             $32, %%rax
                     
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm20                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm21                 
                       
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm22                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm23                 
.endm                                                       

.macro    bf16_save_c_m8n32                                   
   vmovups         %%zmm8, (%%r10)                          
   vmovups         %%zmm9, 64(%%r10)                        
   vmovups         %%zmm10, (%%r11)                         
   vmovups         %%zmm11, 64(%%r11)                       
   vmovups         %%zmm12, (%%r12)                         
   vmovups         %%zmm13, 64(%%r12)                       
   vmovups         %%zmm14, (%%r13)                         
   vmovups         %%zmm15, 64(%%r13)                       

   leaq     (%%r13, %%r8, 4), %%r10                         // C0
   leaq     (%%r10, %%r8, 4), %%r11                         // C1
   leaq     (%%r11, %%r8, 4), %%r12                         // C2
   leaq     (%%r12, %%r8, 4), %%r13                         // C3

   vmovups         %%zmm16, (%%r10)                         
   vmovups         %%zmm17, 64(%%r10)                       
   vmovups         %%zmm18, (%%r11)                         
   vmovups         %%zmm19, 64(%%r11)                       
   vmovups         %%zmm20, (%%r12)                         
   vmovups         %%zmm21, 64(%%r12)                       
   vmovups         %%zmm22, (%%r13)
   vmovups         %%zmm23, 64(%%r13)
                       
   subq            $8, %%rdi                                              
   leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm

.macro    bf16_save_c_m8n32_2
   vcvtne2ps2bf16  %%zmm8, %%zmm9, %%zmm0                                   
   vcvtne2ps2bf16  %%zmm10, %%zmm11, %%zmm1                                   
   vcvtne2ps2bf16  %%zmm12, %%zmm13, %%zmm2                                   
   vcvtne2ps2bf16  %%zmm14, %%zmm15, %%zmm3                                   
   vmovups         %%zmm0, (%%r10)                          
   vmovups         %%zmm1, 64(%%r10)                        
   vmovups         %%zmm2, 128(%%r10)                         
   vmovups         %%zmm3, 192(%%r10)                                              
   
   addq           $256, %%r10

   vcvtne2ps2bf16  %%zmm16, %%zmm17, %%zmm4
   vcvtne2ps2bf16  %%zmm18, %%zmm19, %%zmm5
   vcvtne2ps2bf16  %%zmm20, %%zmm21, %%zmm6
   vcvtne2ps2bf16  %%zmm22, %%zmm23, %%zmm7
   vmovups         %%zmm4, (%%r10)                         
   vmovups         %%zmm5, 64(%%r10)                       
   vmovups         %%zmm6, 128(%%r10)                         
   vmovups         %%zmm7, 192(%%r10)  

   subq           $8, %%rdi                                              
   addq           $256, %%r10
.endm                                                       

//-----------------------------------------------------------------

.macro    bf16_kernel_m4n32k2_1                               
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm4, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm5, %%zmm9
   addq             $128, %%rbx
   prefetcht0       64(%%rbx)
   vmovdqu16        (%%rbx), %%zmm6                  

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm4, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm5, %%zmm11                 
   vmovdqu16        64(%%rbx), %%zmm7

   prefetcht0      256(%%rax) 
   addq              $16, %%rax

   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm4, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm5, %%zmm13                

   vbroadcastss     4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm4, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm5, %%zmm15
.endm                                                       

.macro    bf16_kernel_m4n32k2_2                               
   vbroadcastss     8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9
   addq             $128, %%rbx                            
   prefetcht0       64(%%rbx)
   vmovdqu16        (%%rbx), %%zmm4                  

   vbroadcastss     12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 
   vmovdqu16        64(%%rbx), %%zmm5

   prefetcht0         256(%%rax)
   addq             $16, %%rax

   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13

   vbroadcastss     4(%%rax), %%zmm1
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15
.endm                                                       

.macro    bf16_kernel_m4n32k2_end
   vbroadcastss    8(%%rax), %%zmm2                         
   vdpbf16ps        %%zmm0, %%zmm6, %%zmm8                  
   vdpbf16ps        %%zmm0, %%zmm7, %%zmm9

   vbroadcastss    12(%%rax), %%zmm3                        
   vdpbf16ps        %%zmm1, %%zmm6, %%zmm10                 
   vdpbf16ps        %%zmm1, %%zmm7, %%zmm11                 

   prefetcht0         256(%%rax)                            

   addq             $16, %%rax
   vbroadcastss     (%%rax), %%zmm0                        
   vdpbf16ps        %%zmm2, %%zmm6, %%zmm12                 
   vdpbf16ps        %%zmm2, %%zmm7, %%zmm13                 

   vbroadcastss     4(%%rax), %%zmm1                        
   vdpbf16ps        %%zmm3, %%zmm6, %%zmm14                 
   vdpbf16ps        %%zmm3, %%zmm7, %%zmm15                 
.endm

.macro    bf16_save_c_m4n32                                   
   vmovups         %%zmm8, (%%r10)                          
   vmovups         %%zmm9, 64(%%r10)                        
   vmovups         %%zmm10, (%%r11)                         
   vmovups         %%zmm11, 64(%%r11)                       
   vmovups         %%zmm12, (%%r12)                         
   vmovups         %%zmm13, 64(%%r12)                       
   vmovups         %%zmm14, (%%r13)                         
   vmovups         %%zmm15, 64(%%r13)                       
                       
   subq            $4, %%rdi                                              
   leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                        

.macro    bf16_save_c_m4n32_2                                   
   vcvtne2ps2bf16  %%zmm8, %%zmm9, %%zmm0                                   
   vcvtne2ps2bf16  %%zmm10, %%zmm11, %%zmm1                                   
   vcvtne2ps2bf16  %%zmm12, %%zmm13, %%zmm2                                   
   vcvtne2ps2bf16  %%zmm14, %%zmm15, %%zmm3                                   
   vmovups         %%zmm0, (%%r10)                          
   vmovups         %%zmm1, 64(%%r10)                        
   vmovups         %%zmm2, 128(%%r10)                         
   vmovups         %%zmm3, 192(%%r10)                                              
   
   addq           $256, %%r10
   subq           $4, %%rdi                                              
.endm                                                       

//-----------------------------------------------------------------
//-----------------------------------------------------------------

GEMM_BF16_N32:                                  
   mov     %[C], %%rcx                                      
   mov     %[Cc], %%r10                                      
   mov     %[A], %%rax                                      
   mov     %[B], %%rbx

   prefetcht0         (%%rax)                               

   mov     %[K], %%rdx                                      
   mov     %[LN], %%r8                                     
   mov     %[Bc], %%r14                                    
   mov     %[M], %%rdi

   mov     %[LK], %%r15
   mov     %%rax, %%r9                                  

   prefetcht0         (%%rbx)
   mov     %%rdx, %%rsi

   mov    %[is_start_gemm], %%r12
   mov    %[is_end_gemm], %%r13
   test    $1, %%r12
   jz    BF16_BEGIN_M12N32                                       

//-----------------------------------------------------------------

BF16_BEGIN_PACK_N32:
   mov     %%r14, %%rbp                                      // Bc

   vmovdqu16 (%%rbx), %%zmm6                                  
   prefetcht2  64(%%rbx)                                    
   leaq    (%%rbx, %%r8, 2), %%rbx                          
   vmovdqu16 (%%rbx), %%zmm7                                  
   prefetcht2  64(%%rbx)                                    
   leaq    (%%rbx, %%r8, 2), %%rbx                          

   mov     %%rsi, %%rdx                                      // K
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11                 
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23
   vpxorq         %%zmm24, %%zmm24, %%zmm24                 
   vpxorq         %%zmm25, %%zmm25, %%zmm25                 
   vpxorq         %%zmm26, %%zmm26, %%zmm26                 
   vpxorq         %%zmm27, %%zmm27, %%zmm27
   vpxorq         %%zmm28, %%zmm28, %%zmm28                 
   vpxorq         %%zmm29, %%zmm29, %%zmm29                 
   vpxorq         %%zmm30, %%zmm30, %%zmm30                 
   vpxorq         %%zmm31, %%zmm31, %%zmm31                 
   cmp     $16, %%rdx
   jb      BF16_PACK_MAIN_M12N32K2
   subq    $16, %%rdx

BF16_PACK_MAIN_M12N32K16:                               
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   bf16_kernel_m12n32k2_pack
   cmp     $0, %%rdx                                        
   je      BF16_PACK_SAVEC_M12N32
   bf16_kernel_m12n32k2_pack
   cmp     $16, %%rdx                                        
   jb      BF16_PACK_MAIN_M12N32K2
   subq    $16, %%rdx                               
   jmp     BF16_PACK_MAIN_M12N32K16

BF16_PACK_MAIN_M12N32K2:
   subq    $2, %%rdx
   cmp     $0, %%rdx
   je      BF16_PACK_SAVEC_M12N32
   bf16_kernel_m12n32k2_pack
   jmp     BF16_PACK_MAIN_M12N32K2

BF16_PACK_SAVEC_M12N32: 
	bf16_kernel_m12n32k2_pack_end 
	jmp		BF16_BEGIN_SAVE_M12N32 


//-----------------------------------------------------------------

BF16_BEGIN_M12N32:
   cmpq    $12, %%rdi 
   jb      BF16_BEGIN_M8N32

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19                 
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23
   vpxorq         %%zmm24, %%zmm24, %%zmm24                 
   vpxorq         %%zmm25, %%zmm25, %%zmm25                 
   vpxorq         %%zmm26, %%zmm26, %%zmm26                 
   vpxorq         %%zmm27, %%zmm27, %%zmm27
   vpxorq         %%zmm28, %%zmm28, %%zmm28                 
   vpxorq         %%zmm29, %%zmm29, %%zmm29                 
   vpxorq         %%zmm30, %%zmm30, %%zmm30                 
   vpxorq         %%zmm31, %%zmm31, %%zmm31                                                       
   cmp     $16, %%rdx
   jb      BF16_MAIN_M12N32K2
   subq    $16, %%rdx

BF16_MAIN_K_M12N32K16:   // loop K+=4
   bf16_kernel_m12n32k2_1
   bf16_kernel_m12n32k2_2
   bf16_kernel_m12n32k2_1
   bf16_kernel_m12n32k2_2
   bf16_kernel_m12n32k2_1
   bf16_kernel_m12n32k2_2
   bf16_kernel_m12n32k2_1
   bf16_kernel_m12n32k2_2
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N32
   cmp     $16, %%rdx
   jb      BF16_MAIN_M12N32K2
   subq  $16, %%rdx               
   jmp   BF16_MAIN_K_M12N32K16
                               

BF16_MAIN_M12N32K2:
   bf16_kernel_m12n32k2_1
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N32
   bf16_kernel_m12n32k2_2
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M12N32
   jmp     BF16_MAIN_M12N32K2

BF16_BEGIN_SAVE_M12N32:
   test      $1, %%r13
   jz       BF16_SAVE_C_M12N32_2                                         
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3                                       

BF16_SAVE_C_M12N32:                                                
   bf16_save_c_m12n32
   imul     $24, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp     BF16_BEGIN_M12N32

BF16_SAVE_C_M12N32_2:
   bf16_save_c_m12n32_2
   imul     $24, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp     BF16_BEGIN_M12N32

//-----------------------------------------------------------------

BF16_BEGIN_M8N32:
   cmpq    $8, %%rdi 
   jb      BF16_BEGIN_M4N32

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19                 
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23                                                     
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M8N32K2
   subq    $16, %%rdx

BF16_MAIN_K_M8N32K16:
   bf16_kernel_m8n32k2_1
   bf16_kernel_m8n32k2_2
   bf16_kernel_m8n32k2_1
   bf16_kernel_m8n32k2_2
   bf16_kernel_m8n32k2_1
   bf16_kernel_m8n32k2_2
   bf16_kernel_m8n32k2_1
   bf16_kernel_m8n32k2_2
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N32
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M8N32K2
   subq  $16, %%rdx                                                                  
   jmp   BF16_MAIN_K_M8N32K16                                

BF16_MAIN_M8N32K2:
   bf16_kernel_m8n32k2_1
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N32
   bf16_kernel_m8n32k2_2
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M8N32
   jmp     BF16_MAIN_M8N32K2

BF16_BEGIN_SAVE_M8N32:  
   test      $1, %%r13
   jz       BF16_SAVE_C_M8N32_2                       
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3                                      

BF16_SAVE_C_M8N32:                                                
   bf16_save_c_m8n32
   imul     $16, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M8N32                                    

BF16_SAVE_C_M8N32_2:                                                
   bf16_save_c_m8n32_2
   imul     $16, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M8N32

//-----------------------------------------------------------------

BF16_BEGIN_M4N32:
   cmpq    $0, %%rdi 
   je      BF16_END_N32
   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1 
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M4N32K2
   subq  $16, %%rdx

BF16_MAIN_K_M4N32K16:   // loop K+=4
   bf16_kernel_m4n32k2_1
   bf16_kernel_m4n32k2_2
   bf16_kernel_m4n32k2_1
   bf16_kernel_m4n32k2_2
   bf16_kernel_m4n32k2_1
   bf16_kernel_m4n32k2_2
   bf16_kernel_m4n32k2_1
   bf16_kernel_m4n32k2_2
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N32
   cmpq    $16, %%rdx
   jb      BF16_MAIN_M4N32K2
   subq  $16, %%rdx                                                                   
   jmp   BF16_MAIN_K_M4N32K16                             

BF16_MAIN_M4N32K2:
   bf16_kernel_m4n32k2_1
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N32
   bf16_kernel_m4n32k2_2
   subq    $2, %%rdx
   cmp     $0, %%rdx                                        
   je      BF16_BEGIN_SAVE_M4N32
   jmp     BF16_MAIN_M4N32K2

BF16_BEGIN_SAVE_M4N32: 
   test      $1, %%r13
   jz       BF16_SAVE_C_M4N32_2                                           
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3                                     

BF16_SAVE_C_M4N32:
   cmpq     $3, %%rdi
   je       BF16_SAVE_C_M3N32
   cmpq     $2, %%rdi
   je       BF16_SAVE_C_M2N32
   cmpq     $1, %%rdi
   je       BF16_SAVE_C_M1N32
   bf16_save_c_m4n32
   imul     $8, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M4N32

BF16_SAVE_C_M3N32:
   vmovups         %%zmm12, (%%r12)
   vmovups         %%zmm13, 64(%%r12)                      

BF16_SAVE_C_M2N32:
   vmovups         %%zmm10, (%%r11)                         
   vmovups         %%zmm11, 64(%%r11)

BF16_SAVE_C_M1N32:
   vmovups         %%zmm8, (%%r10)
   vmovups         %%zmm9, 64(%%r10)
   jmp      BF16_END_N32                                      

BF16_SAVE_C_M4N32_2:
   cmpq     $3, %%rdi
   je       BF16_SAVE_C_M3N32_2
   cmpq     $2, %%rdi
   je       BF16_SAVE_C_M2N32_2
   cmpq     $1, %%rdi
   je       BF16_SAVE_C_M1N32_2
   bf16_save_c_m4n32_2
   imul     $8, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax
   jmp      BF16_BEGIN_M4N32

BF16_SAVE_C_M3N32_2:
   vcvtne2ps2bf16  %%zmm12, %%zmm13, %%zmm2                                   
   vmovups         %%zmm2, 128(%%r10)                     

BF16_SAVE_C_M2N32_2:
   vcvtne2ps2bf16  %%zmm10, %%zmm11, %%zmm1
   vmovups         %%zmm1, 64(%%r10)                         

BF16_SAVE_C_M1N32_2:
   vcvtne2ps2bf16  %%zmm8, %%zmm9, %%zmm0
   vmovups         %%zmm0, (%%r10)

BF16_END_N32: