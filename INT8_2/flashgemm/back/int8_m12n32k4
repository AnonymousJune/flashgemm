.macro int8_pack_b_n32 
  vmovups (%%rbx), %%ymm0                                  
  prefetcht2  64(%%rbx)                                    
  leaq    (%%rbx, %%r8, 1), %%rbx                          
  vmovups (%%rbx), %%ymm1                                  
  prefetcht2  64(%%rbx)                                    
  leaq    (%%rbx, %%r8, 1), %%rbx                          

  vpunpcklbw  %%ymm1, %%ymm0, %%ymm4                       
  vpunpckhbw  %%ymm1, %%ymm0, %%ymm5                       

  vmovups (%%rbx), %%ymm2                                  
  prefetcht2  64(%%rbx)                                    
  leaq    (%%rbx, %%r8, 1), %%rbx                          
  vmovups (%%rbx), %%ymm3                                  
  prefetcht2  64(%%rbx)                                    
  leaq    (%%rbx, %%r8, 1), %%rbx                          

  vpunpcklbw  %%ymm3, %%ymm2, %%ymm6                       
  vpunpckhbw  %%ymm3, %%ymm2, %%ymm7                       

  vpunpcklwd  %%ymm6, %%ymm4, %%ymm0                       
  vpunpckhwd  %%ymm6, %%ymm4, %%ymm1                       
  vpunpcklwd  %%ymm7, %%ymm5, %%ymm2                       
  vpunpckhwd  %%ymm7, %%ymm5, %%ymm3                       

  vextracti32x4   $0x1, %%ymm0, %%xmm4                     
  vextracti32x4   $0x1, %%ymm1, %%xmm5                     
  vextracti32x4   $0x1, %%ymm2, %%xmm6                     
  vextracti32x4   $0x1, %%ymm3, %%xmm7                     

  vinserti32x4    $0x1, %%xmm1, %%zmm0, %%zmm0             
  vinserti32x4    $0x2, %%xmm2, %%zmm0, %%zmm0             
  vbroadcastss    (%%rax), %%zmm2                           // A0
  vinserti32x4    $0x3, %%xmm3, %%zmm0, %%zmm0             
  vbroadcastss    4(%%rax), %%zmm3                          // A1
  vmovups         %%zmm0, (%%rbp)                          

  vinserti32x4    $0x1, %%xmm5, %%zmm4, %%zmm4             
  vinserti32x4    $0x2, %%xmm6, %%zmm4, %%zmm4             
  vinserti32x4    $0x3, %%xmm7, %%zmm4, %%zmm4             
  vmovups         %%zmm4, 64(%%rbp)                        
  addq            $128, %%rbp                              

.endm                                                       

.macro    int8_kernel_m12n32k4_pack 
  int8_pack_b_n32                          

  vbroadcastss    8(%%rax), %%zmm5                          // A2
  vpdpbusd    %%zmm0, %%zmm2, %%zmm8                        // A0*(B0-15)
  vpdpbusd    %%zmm4, %%zmm2, %%zmm9                        // A0*(B16-31)

  vbroadcastss    12(%%rax), %%zmm6                         // A3
  vpdpbusd    %%zmm0, %%zmm3, %%zmm10                       // A1*(B0-15)
  vpdpbusd    %%zmm4, %%zmm3, %%zmm11                       // A1*(B16-31)

  prefetcht0         256(%%rax)                            

  vbroadcastss    16(%%rax), %%zmm2                         // A4
  vpdpbusd    %%zmm0, %%zmm5, %%zmm12                      
  vpdpbusd    %%zmm4, %%zmm5, %%zmm13                      

  vbroadcastss    20(%%rax), %%zmm3                         // A5
  vpdpbusd    %%zmm0, %%zmm6, %%zmm14                      
  vpdpbusd    %%zmm4, %%zmm6, %%zmm15                      

  vbroadcastss    24(%%rax), %%zmm5                         // A6
  vpdpbusd    %%zmm0, %%zmm2, %%zmm16                      
  vpdpbusd    %%zmm4, %%zmm2, %%zmm17                      

  vbroadcastss    28(%%rax), %%zmm6                         // A7
  vpdpbusd    %%zmm0, %%zmm3, %%zmm18                      
  vpdpbusd    %%zmm4, %%zmm3, %%zmm19                      

  vbroadcastss    32(%%rax), %%zmm2                         // A8
  vpdpbusd    %%zmm0, %%zmm5, %%zmm20                      
  vpdpbusd    %%zmm4, %%zmm5, %%zmm21                      

  vbroadcastss    36(%%rax), %%zmm3                         // A9
  vpdpbusd    %%zmm0, %%zmm6, %%zmm22                      
  vpdpbusd    %%zmm4, %%zmm6, %%zmm23                      

  vbroadcastss    40(%%rax), %%zmm5                         // A10
  vpdpbusd    %%zmm0, %%zmm2, %%zmm24                      
  vpdpbusd    %%zmm4, %%zmm2, %%zmm25                      

  prefetcht0         384(%%rax)                            

  vbroadcastss    44(%%rax), %%zmm6                         // A11
  addq              $48, %%rax                             
  vpdpbusd    %%zmm0, %%zmm3, %%zmm26                      
  vpdpbusd    %%zmm4, %%zmm3, %%zmm27                      

  vbroadcastss    (%%rax), %%zmm2                           // next A0
  vpdpbusd    %%zmm0, %%zmm5, %%zmm28                      
  vpdpbusd    %%zmm4, %%zmm5, %%zmm29                      

  vbroadcastss    4(%%rax), %%zmm3                          // next A1
  vpdpbusd    %%zmm0, %%zmm6, %%zmm30                      
  vpdpbusd    %%zmm4, %%zmm6, %%zmm31                      

.endm                                                       

.macro    int8_kernel_m12n32k4_k1                               

   vbroadcastss    8(%%rax), %%zmm2                         
   vpdpbusd    %%zmm4, %%zmm0, %%zmm8                       
   vpdpbusd    %%zmm5, %%zmm0, %%zmm9                       

   vbroadcastss    12(%%rax), %%zmm3                        

   vpdpbusd    %%zmm4, %%zmm1, %%zmm10                      
   vpdpbusd    %%zmm5, %%zmm1, %%zmm11                      

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vpdpbusd    %%zmm4, %%zmm2, %%zmm12                      
   vpdpbusd    %%zmm5, %%zmm2, %%zmm13                      

   vbroadcastss    20(%%rax), %%zmm1                        
   vpdpbusd    %%zmm4, %%zmm3, %%zmm14                      
   vpdpbusd    %%zmm5, %%zmm3, %%zmm15                      

   vbroadcastss    24(%%rax), %%zmm2                        
   vpdpbusd    %%zmm4, %%zmm0, %%zmm16                      
   vpdpbusd    %%zmm5, %%zmm0, %%zmm17                      

   vbroadcastss    28(%%rax), %%zmm3                        
   vpdpbusd    %%zmm4, %%zmm1, %%zmm18                      
   vpdpbusd    %%zmm5, %%zmm1, %%zmm19                      

   vbroadcastss    32(%%rax), %%zmm0                        
   vpdpbusd    %%zmm4, %%zmm2, %%zmm20                      
   vpdpbusd    %%zmm5, %%zmm2, %%zmm21                      

    addq              $128, %%rbx                           

   vbroadcastss    36(%%rax), %%zmm1                        
   vpdpbusd    %%zmm4, %%zmm3, %%zmm22                      
   vpdpbusd    %%zmm5, %%zmm3, %%zmm23                      

   prefetcht0         64(%%rbx)                             

   vbroadcastss    40(%%rax), %%zmm2                        
   vpdpbusd    %%zmm4, %%zmm0, %%zmm24                      
   vpdpbusd    %%zmm5, %%zmm0, %%zmm25                      

   vbroadcastss    44(%%rax), %%zmm3                        
   vpdpbusd    %%zmm4, %%zmm1, %%zmm26                      
   vmovups         (%%rbx), %%zmm6                          
    addq              $48, %%rax                            
   vpdpbusd    %%zmm5, %%zmm1, %%zmm27                      

   vbroadcastss    (%%rax), %%zmm0                          
   vpdpbusd    %%zmm4, %%zmm2, %%zmm28                      
   vmovups         64(%%rbx), %%zmm7                        
   vpdpbusd    %%zmm5, %%zmm2, %%zmm29                      

   vbroadcastss    4(%%rax), %%zmm1                         
   vpdpbusd    %%zmm4, %%zmm3, %%zmm30                      
   vpdpbusd    %%zmm5, %%zmm3, %%zmm31                      

.endm                                                       

.macro    int8_kernel_m12n32k4_k2                               

   vbroadcastss    8(%%rax), %%zmm2                         
   vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
   vpdpbusd    %%zmm7, %%zmm0, %%zmm9                       

   vbroadcastss    12(%%rax), %%zmm3                        
   vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
   vpdpbusd    %%zmm7, %%zmm1, %%zmm11                      

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
   vpdpbusd    %%zmm7, %%zmm2, %%zmm13                      

   vbroadcastss    20(%%rax), %%zmm1                        
   vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm15                      

   vbroadcastss    24(%%rax), %%zmm2                        
   vpdpbusd    %%zmm6, %%zmm0, %%zmm16                      
   vpdpbusd    %%zmm7, %%zmm0, %%zmm17                      

   vbroadcastss    28(%%rax), %%zmm3                        
   vpdpbusd    %%zmm6, %%zmm1, %%zmm18                      
   vpdpbusd    %%zmm7, %%zmm1, %%zmm19                      

   vbroadcastss    32(%%rax), %%zmm0                        
   vpdpbusd    %%zmm6, %%zmm2, %%zmm20                      
   vpdpbusd    %%zmm7, %%zmm2, %%zmm21                      

    addq              $128, %%rbx                           

   vbroadcastss    36(%%rax), %%zmm1                        
   vpdpbusd    %%zmm6, %%zmm3, %%zmm22                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm23                      

   prefetcht0         64(%%rbx)                             

   vbroadcastss    40(%%rax), %%zmm2                        
   vpdpbusd    %%zmm6, %%zmm0, %%zmm24                      
   vpdpbusd    %%zmm7, %%zmm0, %%zmm25                      

   vbroadcastss    44(%%rax), %%zmm3                        
   vpdpbusd    %%zmm6, %%zmm1, %%zmm26                      
   vmovups         (%%rbx), %%zmm4                          
    addq              $48, %%rax                            
   vpdpbusd    %%zmm7, %%zmm1, %%zmm27                      

   vbroadcastss    (%%rax), %%zmm0                          
   vpdpbusd    %%zmm6, %%zmm2, %%zmm28                      
   vmovups         64(%%rbx), %%zmm5                        
   vpdpbusd    %%zmm7, %%zmm2, %%zmm29                      

   vbroadcastss    4(%%rax), %%zmm1                         
   vpdpbusd    %%zmm6, %%zmm3, %%zmm30                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm31                      

.endm                                                       

.macro    int8_kernel_m12n32k4_end                            

   vbroadcastss    8(%%rax), %%zmm2                         
   vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
   vpdpbusd    %%zmm7, %%zmm0, %%zmm9                       
   vbroadcastss    12(%%rax), %%zmm3                        

   vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
   vpdpbusd    %%zmm7, %%zmm1, %%zmm11                      

   prefetcht0         256(%%rax)                            

   vbroadcastss    16(%%rax), %%zmm0                        
   vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
   vpdpbusd    %%zmm7, %%zmm2, %%zmm13                      

   vbroadcastss    20(%%rax), %%zmm1                        
   vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm15                      

   vbroadcastss    24(%%rax), %%zmm2                        
   vpdpbusd    %%zmm6, %%zmm0, %%zmm16                      
   vpdpbusd    %%zmm7, %%zmm0, %%zmm17                      

   vbroadcastss    28(%%rax), %%zmm3                        
   vpdpbusd    %%zmm6, %%zmm1, %%zmm18                      
   vpdpbusd    %%zmm7, %%zmm1, %%zmm19                      

   vbroadcastss    32(%%rax), %%zmm0                        
   vpdpbusd    %%zmm6, %%zmm2, %%zmm20                      
   vpdpbusd    %%zmm7, %%zmm2, %%zmm21                      

   vbroadcastss    36(%%rax), %%zmm1                        
   vpdpbusd    %%zmm6, %%zmm3, %%zmm22                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm23                      

   vbroadcastss    40(%%rax), %%zmm2                        
   vpdpbusd    %%zmm6, %%zmm0, %%zmm24                      
   vpdpbusd    %%zmm7, %%zmm0, %%zmm25                      

   vbroadcastss    44(%%rax), %%zmm3                        
   vpdpbusd    %%zmm6, %%zmm1, %%zmm26                      
    addq              $48, %%rax                            
   vpdpbusd    %%zmm7, %%zmm1, %%zmm27                      

   vpdpbusd    %%zmm6, %%zmm2, %%zmm28                      
   vpdpbusd    %%zmm7, %%zmm2, %%zmm29                      

   vpdpbusd    %%zmm6, %%zmm3, %%zmm30                      
   vpdpbusd    %%zmm7, %%zmm3, %%zmm31                      

.endm                                                       

.macro    int8_save_c_m12n32                                   
  vmovups         %%zmm8, (%%r10)                          
  vmovups         %%zmm9, 64(%%r10)                        
  vmovups         %%zmm10, (%%r11)                         
  vmovups         %%zmm11, 64(%%r11)                       
  vmovups         %%zmm12, (%%r12)                         
  vmovups         %%zmm13, 64(%%r12)                       
  vmovups         %%zmm14, (%%r13)                         
  vmovups         %%zmm15, 64(%%r13)                       

  leaq     (%%r13, %%r8, 4), %%r10                         // C0
  leaq     (%%r10, %%r8, 4), %%r11                         // C1
  leaq     (%%r11, %%r8, 4), %%r12                         // C2
  leaq     (%%r12, %%r8, 4), %%r13                         // C3

  vmovups         %%zmm16, (%%r10)                         
  vmovups         %%zmm17, 64(%%r10)                       
  vmovups         %%zmm18, (%%r11)                         
  vmovups         %%zmm19, 64(%%r11)                       
  vmovups         %%zmm20, (%%r12)                         
  vmovups         %%zmm21, 64(%%r12)                       
  vmovups         %%zmm22, (%%r13)                         
  vmovups         %%zmm23, 64(%%r13)                       

  leaq     (%%r13, %%r8, 4), %%r10                         // C0
  leaq     (%%r10, %%r8, 4), %%r11                         // C1
  leaq     (%%r11, %%r8, 4), %%r12                         // C2
  leaq     (%%r12, %%r8, 4), %%r13                         // C3

  vmovups         %%zmm24, (%%r10)                         
  vmovups         %%zmm25, 64(%%r10)                       
  vmovups         %%zmm26, (%%r11)                         
  vmovups         %%zmm27, 64(%%r11)                       
  subq             $12, %%rdi                             
  vmovups         %%zmm28, (%%r12)                         
  vmovups         %%zmm29, 64(%%r12)                       
  vmovups         %%zmm30, (%%r13)                         
  vmovups         %%zmm31, 64(%%r13)                       

  leaq      (%%r13, %%r8, 4), %%rcx                        // C0

.endm                                                       

.macro    int8_save_c_m12n32_2                                   
  vpmovsdb   %%zmm8, %%xmm0                                  
  vpmovsdb   %%zmm9, %%xmm1                                  
  vpmovsdb   %%zmm10, %%xmm2                                  
  vpmovsdb   %%zmm11, %%xmm3
  vinserti32x4    $0x1, %%xmm1, %%zmm0, %%zmm0
  vinserti32x4    $0x2, %%xmm2, %%zmm0, %%zmm0
  vinserti32x4    $0x3, %%xmm3, %%zmm0, %%zmm0
  vmovups         %%zmm0, (%%r10)
  vpmovsdb   %%zmm12, %%xmm4                                  
  vpmovsdb   %%zmm13, %%xmm5                                  
  vpmovsdb   %%zmm14, %%xmm6                                  
  vpmovsdb   %%zmm15, %%xmm7
  vinserti32x4    $0x1, %%xmm5, %%zmm4, %%zmm4
  vinserti32x4    $0x2, %%xmm6, %%zmm4, %%zmm4
  vinserti32x4    $0x3, %%xmm7, %%zmm4, %%zmm4
  vmovups         %%zmm4, 64(%%r10)
  addq            $128, %%r10

  vpmovsdb   %%zmm16, %%xmm8
  vpmovsdb   %%zmm17, %%xmm9
  vpmovsdb   %%zmm18, %%xmm10
  vpmovsdb   %%zmm19, %%xmm11
  vinserti32x4    $0x1, %%xmm9, %%zmm8, %%zmm8
  vinserti32x4    $0x2, %%xmm10, %%zmm8, %%zmm8
  vinserti32x4    $0x3, %%xmm11, %%zmm8, %%zmm8
  vmovups         %%zmm8, (%%r10)
  vpmovsdb   %%zmm20, %%xmm12
  vpmovsdb   %%zmm21, %%xmm13
  vpmovsdb   %%zmm22, %%xmm14
  vpmovsdb   %%zmm23, %%xmm15
  vinserti32x4    $0x1, %%xmm13, %%zmm12, %%zmm12
  vinserti32x4    $0x2, %%xmm14, %%zmm12, %%zmm12
  vinserti32x4    $0x3, %%xmm15, %%zmm12, %%zmm12
  vmovups         %%zmm12, 64(%%r10)
  addq            $128, %%r10

  vpmovsdb   %%zmm24, %%xmm16
  vpmovsdb   %%zmm25, %%xmm17
  vpmovsdb   %%zmm26, %%xmm18
  vpmovsdb   %%zmm27, %%xmm19
  vinserti32x4    $0x1, %%xmm17, %%zmm16, %%zmm16
  vinserti32x4    $0x2, %%xmm18, %%zmm16, %%zmm16
  vinserti32x4    $0x3, %%xmm19, %%zmm16, %%zmm16
  vmovups         %%zmm16, (%%r10)
  vpmovsdb   %%zmm28, %%xmm20
  vpmovsdb   %%zmm29, %%xmm21
  vpmovsdb   %%zmm30, %%xmm22
  vpmovsdb   %%zmm31, %%xmm23
  vinserti32x4    $0x1, %%xmm21, %%zmm20, %%zmm20
  vinserti32x4    $0x2, %%xmm22, %%zmm20, %%zmm20
  vinserti32x4    $0x3, %%xmm23, %%zmm20, %%zmm20
  vmovups         %%zmm20, 64(%%r10)
  addq           $128, %%r10
  
  subq           $12, %%rdi
.endm  

//-----------------------------------------------------------------

.macro    int8_kernel_m8n32k4_k1
  vbroadcastss    8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm4, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm5, %%zmm0, %%zmm9                       

  vbroadcastss    12(%%rax), %%zmm3                        

  vpdpbusd    %%zmm4, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm5, %%zmm1, %%zmm11                      

  prefetcht0         256(%%rax)                            

  vbroadcastss    16(%%rax), %%zmm0                        
  vpdpbusd    %%zmm4, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm5, %%zmm2, %%zmm13

  vmovups         (%%rbx), %%zmm6

  vbroadcastss    20(%%rax), %%zmm1                        
  vpdpbusd    %%zmm4, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm5, %%zmm3, %%zmm15

  vbroadcastss    24(%%rax), %%zmm2                        
  vpdpbusd    %%zmm4, %%zmm0, %%zmm16                      
  vpdpbusd    %%zmm5, %%zmm0, %%zmm17
  
  vmovups         64(%%rbx), %%zmm7

  vbroadcastss    28(%%rax), %%zmm3                        
  vpdpbusd    %%zmm4, %%zmm1, %%zmm18                      
  vpdpbusd    %%zmm5, %%zmm1, %%zmm19
  
  addq              $32, %%rax

  vbroadcastss    (%%rax), %%zmm0                        
  vpdpbusd    %%zmm4, %%zmm2, %%zmm20
  vpdpbusd    %%zmm5, %%zmm2, %%zmm21

  addq              $128, %%rbx                           

  vbroadcastss    4(%%rax), %%zmm1
  vpdpbusd    %%zmm4, %%zmm3, %%zmm22
  vpdpbusd    %%zmm5, %%zmm3, %%zmm23
.endm                                                       

.macro    int8_kernel_m8n32k4_k2                               
  vbroadcastss    8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm7, %%zmm0, %%zmm9                       

  vbroadcastss    12(%%rax), %%zmm3                        

  vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm11                      

  prefetcht0         256(%%rax)                            

  vbroadcastss    16(%%rax), %%zmm0                        
  vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm7, %%zmm2, %%zmm13

  vmovups         (%%rbx), %%zmm4

  vbroadcastss    20(%%rax), %%zmm1                        
  vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm7, %%zmm3, %%zmm15

  vbroadcastss    24(%%rax), %%zmm2                        
  vpdpbusd    %%zmm6, %%zmm0, %%zmm16                      
  vpdpbusd    %%zmm7, %%zmm0, %%zmm17

  vmovups         64(%%rbx), %%zmm5

  vbroadcastss    28(%%rax), %%zmm3                        
  vpdpbusd    %%zmm6, %%zmm1, %%zmm18                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm19

  addq              $32, %%rax

  vbroadcastss    (%%rax), %%zmm0                        
  vpdpbusd    %%zmm6, %%zmm2, %%zmm20
  vpdpbusd    %%zmm7, %%zmm2, %%zmm21

  addq              $128, %%rbx                           

  vbroadcastss    4(%%rax), %%zmm1
  vpdpbusd    %%zmm6, %%zmm3, %%zmm22
  vpdpbusd    %%zmm7, %%zmm3, %%zmm23
.endm                                                       

.macro    int8_kernel_m8n32k4_end
  vbroadcastss    8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm7, %%zmm0, %%zmm9                       

  vbroadcastss    12(%%rax), %%zmm3
  vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm11                      

  prefetcht0         256(%%rax)                            

  vbroadcastss    16(%%rax), %%zmm0                        
  vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm7, %%zmm2, %%zmm13

  vbroadcastss    20(%%rax), %%zmm1                        
  vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm7, %%zmm3, %%zmm15

  vbroadcastss    24(%%rax), %%zmm2                        
  vpdpbusd    %%zmm6, %%zmm0, %%zmm16                      
  vpdpbusd    %%zmm7, %%zmm0, %%zmm17

  vbroadcastss    28(%%rax), %%zmm3                        
  vpdpbusd    %%zmm6, %%zmm1, %%zmm18                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm19

  addq              $32, %%rax
                       
  vpdpbusd    %%zmm6, %%zmm2, %%zmm20
  vpdpbusd    %%zmm7, %%zmm2, %%zmm21   

  vpdpbusd    %%zmm6, %%zmm3, %%zmm22
  vpdpbusd    %%zmm7, %%zmm3, %%zmm23                      
.endm                                                       

.macro    int8_save_c_m8n32                                   
  vmovups         %%zmm8, (%%r10)                          
  vmovups         %%zmm9, 64(%%r10)                        
  vmovups         %%zmm10, (%%r11)                         
  vmovups         %%zmm11, 64(%%r11)                       
  vmovups         %%zmm12, (%%r12)                         
  vmovups         %%zmm13, 64(%%r12)                       
  vmovups         %%zmm14, (%%r13)                         
  vmovups         %%zmm15, 64(%%r13)                       

  leaq     (%%r13, %%r8, 4), %%r10                         // C0
  leaq     (%%r10, %%r8, 4), %%r11                         // C1
  leaq     (%%r11, %%r8, 4), %%r12                         // C2
  leaq     (%%r12, %%r8, 4), %%r13                         // C3

  vmovups         %%zmm16, (%%r10)                         
  vmovups         %%zmm17, 64(%%r10)                       
  vmovups         %%zmm18, (%%r11)                         
  vmovups         %%zmm19, 64(%%r11)                       
  vmovups         %%zmm20, (%%r12)                         
  vmovups         %%zmm21, 64(%%r12)                       
  vmovups         %%zmm22, (%%r13)                         
  vmovups         %%zmm23, 64(%%r13)                       
                      
  subq             $8, %%rdi
  leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

.macro    int8_save_c_m8n32_2                                   
   vpmovsdb   %%zmm8, %%xmm0                                  
   vpmovsdb   %%zmm9, %%xmm1                                  
   vpmovsdb   %%zmm10, %%xmm2                                  
   vpmovsdb   %%zmm11, %%xmm3
   vinserti32x4    $0x1, %%xmm1, %%zmm0, %%zmm0
   vinserti32x4    $0x2, %%xmm2, %%zmm0, %%zmm0
   vinserti32x4    $0x3, %%xmm3, %%zmm0, %%zmm0
   vmovups         %%zmm0, (%%r10)
   vpmovsdb   %%zmm12, %%xmm4                                  
   vpmovsdb   %%zmm13, %%xmm5                                  
   vpmovsdb   %%zmm14, %%xmm6                                  
   vpmovsdb   %%zmm15, %%xmm7
   vinserti32x4    $0x1, %%xmm5, %%zmm4, %%zmm4
   vinserti32x4    $0x2, %%xmm6, %%zmm4, %%zmm4
   vinserti32x4    $0x3, %%xmm7, %%zmm4, %%zmm4
   vmovups         %%zmm4, 64(%%r10)
   addq            $128, %%r10

   vpmovsdb   %%zmm16, %%xmm8
   vpmovsdb   %%zmm17, %%xmm9
   vpmovsdb   %%zmm18, %%xmm10
   vpmovsdb   %%zmm19, %%xmm11
   vinserti32x4    $0x1, %%xmm9, %%zmm8, %%zmm8
   vinserti32x4    $0x2, %%xmm10, %%zmm8, %%zmm8
   vinserti32x4    $0x3, %%xmm11, %%zmm8, %%zmm8
   vmovups         %%zmm8, (%%r10)
   vpmovsdb   %%zmm20, %%xmm12
   vpmovsdb   %%zmm21, %%xmm13
   vpmovsdb   %%zmm22, %%xmm14
   vpmovsdb   %%zmm23, %%xmm15
   vinserti32x4    $0x1, %%xmm13, %%zmm12, %%zmm12
   vinserti32x4    $0x2, %%xmm14, %%zmm12, %%zmm12
   vinserti32x4    $0x3, %%xmm15, %%zmm12, %%zmm12
   vmovups         %%zmm12, 64(%%r10)
   addq            $128, %%r10

   subq           $8, %%rdi
.endm  

//-----------------------------------------------------------------

.macro    int8_kernel_m4n32k4_k1
  vbroadcastss    8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm4, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm5, %%zmm0, %%zmm9

  vmovups         (%%rbx), %%zmm6                       

  vbroadcastss    12(%%rax), %%zmm3                        
  vpdpbusd    %%zmm4, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm5, %%zmm1, %%zmm11

  vmovups         64(%%rbx), %%zmm7
  prefetcht0      256(%%rax)

  addq            $16, %%rax
  vbroadcastss    (%%rax), %%zmm0                        
  vpdpbusd    %%zmm4, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm5, %%zmm2, %%zmm13

  vbroadcastss    4(%%rax), %%zmm1                        
  vpdpbusd    %%zmm4, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm5, %%zmm3, %%zmm15

  addq              $128, %%rbx                           
.endm                                                       

.macro    int8_kernel_m4n32k4_k2                               
  vbroadcastss    8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm7, %%zmm0, %%zmm9

  vmovups         (%%rbx), %%zmm4                       

  vbroadcastss    12(%%rax), %%zmm3                        
  vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm11

  vmovups         64(%%rbx), %%zmm5
  prefetcht0      256(%%rax)

  addq            $16, %%rax
  vbroadcastss    (%%rax), %%zmm0                        
  vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm7, %%zmm2, %%zmm13

  vbroadcastss    4(%%rax), %%zmm1                        
  vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm7, %%zmm3, %%zmm15

  addq              $128, %%rbx 
.endm                                                       

.macro    int8_kernel_m4n32k4_end
  broadcastss     8(%%rax), %%zmm2                         
  vpdpbusd    %%zmm6, %%zmm0, %%zmm8                       
  vpdpbusd    %%zmm7, %%zmm0, %%zmm9                      

  vbroadcastss    12(%%rax), %%zmm3                        
  vpdpbusd    %%zmm6, %%zmm1, %%zmm10                      
  vpdpbusd    %%zmm7, %%zmm1, %%zmm11

  prefetcht0      256(%%rax)
                       
  vpdpbusd    %%zmm6, %%zmm2, %%zmm12                      
  vpdpbusd    %%zmm7, %%zmm2, %%zmm13
                        
  vpdpbusd    %%zmm6, %%zmm3, %%zmm14                      
  vpdpbusd    %%zmm7, %%zmm3, %%zmm15                      
.endm                                                       

.macro    int8_save_c_m4n32                                   
  vmovups         %%zmm8, (%%r10)                          
  vmovups         %%zmm9, 64(%%r10)                        
  vmovups         %%zmm10, (%%r11)                         
  vmovups         %%zmm11, 64(%%r11)                       
  vmovups         %%zmm12, (%%r12)                         
  vmovups         %%zmm13, 64(%%r12)                       
  vmovups         %%zmm14, (%%r13)                         
  vmovups         %%zmm15, 64(%%r13)
                      
  subq             $4, %%rdi
  leaq      (%%r13, %%r8, 4), %%rcx                        // C0
.endm                                                       

.macro    int8_save_c_m4n32_2                                   
   vpmovsdb   %%zmm8, %%xmm16                                  
   vpmovsdb   %%zmm9, %%xmm17                                  
   vpmovsdb   %%zmm10, %%xmm18                                  
   vpmovsdb   %%zmm11, %%xmm19
   vinserti32x4    $0x1, %%xmm17, %%zmm16, %%zmm16
   vinserti32x4    $0x2, %%xmm18, %%zmm16, %%zmm16
   vinserti32x4    $0x3, %%xmm19, %%zmm16, %%zmm16
   vmovups         %%zmm16, (%%r10)
   vpmovsdb   %%zmm12, %%xmm20                                  
   vpmovsdb   %%zmm13, %%xmm21                                  
   vpmovsdb   %%zmm14, %%xmm22                                  
   vpmovsdb   %%zmm15, %%xmm23
   vinserti32x4    $0x1, %%xmm21, %%zmm20, %%zmm20
   vinserti32x4    $0x2, %%xmm22, %%zmm20, %%zmm20
   vinserti32x4    $0x3, %%xmm23, %%zmm20, %%zmm20
   vmovups         %%zmm20, 64(%%r10)
   addq            $128, %%r10
   subq            $4, %%rdi
.endm  

//-----------------------------------------------------------------

GEMM_INT8_N32:
  mov     %[C], %%rcx                                      
  mov     %[Cc], %%r10                                      
  mov     %[A], %%rax                                      
  mov     %[B], %%rbx                                      

  prefetcht0         (%%rax)                               

  mov     %[K], %%rdx                                      
  mov     %[LN], %%r8                                     
  mov     %[Bc], %%r14                                    
  mov     %[M], %%rdi

  mov     %[LK], %%r15
  mov     %%rax, %%r9
  prefetcht0         (%%rbx)                                      
  mov     %%rdx, %%rsi

  mov    %[is_start_gemm], %%r12
  test   $1, %%r12
  jz     INT8_BEGIN_M12N32                                    

INT8_BEGIN_PACK_N32:                                            
   mov     %%r14, %%rbp                                      // Bc
   mov     %%rsi, %%rdx                                      // K

   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15                 
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19                 
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23                 
   vpxorq         %%zmm24, %%zmm24, %%zmm24                 
   vpxorq         %%zmm25, %%zmm25, %%zmm25                 
   vpxorq         %%zmm26, %%zmm26, %%zmm26                 
   vpxorq         %%zmm27, %%zmm27, %%zmm27                 
   vpxorq         %%zmm28, %%zmm28, %%zmm28                 
   vpxorq         %%zmm29, %%zmm29, %%zmm29                 
   vpxorq         %%zmm30, %%zmm30, %%zmm30                 
   vpxorq         %%zmm31, %%zmm31, %%zmm31                 
   cmp     $32, %%rdx
   jb      INT8_PACK_MAIN_M12N32K4
   subq    $32, %%rdx                     
                        
INT8_PACK_MAIN_M12N32K32:                                           
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack                                          
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack
   int8_kernel_m12n32k4_pack                                  
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M12N32
   cmp     $32, %%rdx                                        
   jb      INT8_PACK_MAIN_M12N32K4
   subq    $32, %%rdx
   jmp     INT8_PACK_MAIN_M12N32K32                                 

INT8_PACK_MAIN_M12N32K4:
   subq    $4, %%rdx
   cmp     $0, %%rdx
   je      INT8_BEGIN_SAVE_M12N32
   int8_kernel_m12n32k4_pack
   jmp     INT8_PACK_MAIN_M12N32K4

//-----------------------------------------------------------------

INT8_BEGIN_M12N32:
   cmpq    $12, %%rdi 
   jb      INT8_BEGIN_M8N32                                               

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11                 
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15                 
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19                 
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23                 
   vpxorq         %%zmm24, %%zmm24, %%zmm24                 
   vpxorq         %%zmm25, %%zmm25, %%zmm25                 
   vpxorq         %%zmm26, %%zmm26, %%zmm26                 
   vpxorq         %%zmm27, %%zmm27, %%zmm27                 
   vpxorq         %%zmm28, %%zmm28, %%zmm28                 
   vpxorq         %%zmm29, %%zmm29, %%zmm29                 
   vpxorq         %%zmm30, %%zmm30, %%zmm30                 
   vpxorq         %%zmm31, %%zmm31, %%zmm31
   cmp     $32, %%rdx
   jb      INT8_MAIN_M12N32K4
   subq    $32, %%rdx

INT8_MAIN_M12N32K32:
   int8_kernel_m12n32k4_k1                                      
   int8_kernel_m12n32k4_k2                                      
   int8_kernel_m12n32k4_k1                                      
   int8_kernel_m12n32k4_k2                                      
   int8_kernel_m12n32k4_k1                                      
   int8_kernel_m12n32k4_k2                                      
   int8_kernel_m12n32k4_k1
   int8_kernel_m12n32k4_k2                                      
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M12N32                                   
   cmp     $32, %%rdx
   jb      INT8_MAIN_M12N32K4                                     
   subq  $32, %%rdx
   jmp   INT8_MAIN_M12N32K32                 

INT8_MAIN_M12N32K4:
   int8_kernel_m12n32k4_k1
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M12N32
   int8_kernel_m12n32k4_k2
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M12N32
   jmp     INT8_MAIN_M12N32K4

INT8_BEGIN_SAVE_M12N32:
   mov      %[is_end_gemm], %%r13
   test     $1, %%r13
   jz       INT8_SAVE_C_M12N32_2                                          
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3                            

INT8_SAVE_C_M12N32:                                                
  int8_save_c_m12n32                                         
  imul     $12, %%r15, %%r11 // temp use %%r11
  add      %%r11, %%r9
  movq     %%r9, %%rax                                    
  jmp      INT8_BEGIN_M12N32

INT8_SAVE_C_M12N32_2:
  int8_save_c_m12n32_2
  imul     $12, %%r15, %%r11 // temp use %%r11
  add      %%r11, %%r9
  movq     %%r9, %%rax                                    
  jmp      INT8_BEGIN_M12N32


//-----------------------------------------------------------------

INT8_BEGIN_M8N32:
   cmpq    $8, %%rdi 
   jb      INT8_BEGIN_M4N32                                               

   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11                 
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1                 
   vpxorq         %%zmm16, %%zmm16, %%zmm16                 
   vpxorq         %%zmm17, %%zmm17, %%zmm17                 
   vpxorq         %%zmm18, %%zmm18, %%zmm18                 
   vpxorq         %%zmm19, %%zmm19, %%zmm19                 
   vpxorq         %%zmm20, %%zmm20, %%zmm20                 
   vpxorq         %%zmm21, %%zmm21, %%zmm21                 
   vpxorq         %%zmm22, %%zmm22, %%zmm22                 
   vpxorq         %%zmm23, %%zmm23, %%zmm23
   cmp     $32, %%rdx
   jb      INT8_MAIN_M8N32K4
   subq    $32, %%rdx

INT8_MAIN_M8N32K32:
   int8_kernel_m8n32k4_k1                                      
   int8_kernel_m8n32k4_k2                                      
   int8_kernel_m8n32k4_k1                                      
   int8_kernel_m8n32k4_k2                                      
   int8_kernel_m8n32k4_k1                                      
   int8_kernel_m8n32k4_k2                                      
   int8_kernel_m8n32k4_k1
   int8_kernel_m8n32k4_k2                                      
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M8N32                                   
   cmp     $32, %%rdx
   jb      INT8_MAIN_M8N32K4                                     
   subq  $32, %%rdx
   jmp   INT8_MAIN_M8N32K32                 

INT8_MAIN_M8N32K4:
   int8_kernel_m8n32k4_k1
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M8N32
   int8_kernel_m8n32k4_k2
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M8N32
   jmp     INT8_MAIN_M8N32K4

INT8_BEGIN_SAVE_M8N32:
   mov      %[is_end_gemm], %%r13
   test     $1, %%r13
   jz       INT8_SAVE_C_M8N32_2                                          
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3                            

INT8_SAVE_C_M8N32:                                                
  int8_save_c_m8n32                                         
  imul     $8, %%r15, %%r11 // temp use %%r11
  add      %%r11, %%r9
  movq     %%r9, %%rax                                    
  jmp      INT8_BEGIN_M8N32

INT8_SAVE_C_M8N32_2:
  int8_save_c_m8n32_2
  imul     $8, %%r15, %%r11 // temp use %%r11
  add      %%r11, %%r9
  movq     %%r9, %%rax                                    
  jmp      INT8_BEGIN_M8N32


//-----------------------------------------------------------------

INT8_BEGIN_M4N32:
   cmpq    $4, %%rdi 
   jb      INT8_END_N32
   mov     %%r14, %%rbx                                      // Bc
   mov     %%rsi, %%rdx                                      // K
   vmovups        (%%rbx), %%zmm4                            // B0-15
   vmovups     64(%%rbx), %%zmm5                             // B16-31
   vpxorq         %%zmm8, %%zmm8, %%zmm8                    
   vpxorq         %%zmm9, %%zmm9, %%zmm9                    
   vpxorq         %%zmm10, %%zmm10, %%zmm10                 
   vpxorq         %%zmm11, %%zmm11, %%zmm11
   vbroadcastss    (%%rax), %%zmm0                           // A0
   vbroadcastss    4(%%rax), %%zmm1                          // A1                 
   vpxorq         %%zmm12, %%zmm12, %%zmm12                 
   vpxorq         %%zmm13, %%zmm13, %%zmm13                 
   vpxorq         %%zmm14, %%zmm14, %%zmm14                 
   vpxorq         %%zmm15, %%zmm15, %%zmm15
   cmp     $32, %%rdx
   jb      INT8_MAIN_M4N32K4
   subq    $32, %%rdx

INT8_MAIN_M4N32K32:
   int8_kernel_m4n32k4_k1                                      
   int8_kernel_m4n32k4_k2                                      
   int8_kernel_m4n32k4_k1                                      
   int8_kernel_m4n32k4_k2                                      
   int8_kernel_m4n32k4_k1                                      
   int8_kernel_m4n32k4_k2                                      
   int8_kernel_m4n32k4_k1
   int8_kernel_m4n32k4_k2                                      
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M4N32                                   
   cmp     $32, %%rdx
   jb      INT8_MAIN_M4N32K4                                     
   subq  $32, %%rdx
   jmp   INT8_MAIN_M4N32K32                 

INT8_MAIN_M4N32K4:
   int8_kernel_m4n32k4_k1
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M4N32
   int8_kernel_m4n32k4_k2
   subq    $4, %%rdx
   cmp     $0, %%rdx                                        
   je      INT8_BEGIN_SAVE_M4N32
   jmp     INT8_MAIN_M4N32K4

INT8_BEGIN_SAVE_M4N32:
   mov      %[is_end_gemm], %%r13
   test     $1, %%r13
   jz INT8_SAVE_C_M4N32_2                                          
   mov      %%rcx, %%r10                                     // C0
   leaq     (%%r10, %%r8, 4), %%r11                          // C1
   leaq     (%%r11, %%r8, 4), %%r12                          // C2
   leaq     (%%r12, %%r8, 4), %%r13                          // C3

INT8_SAVE_C_M4N32:
   cmpq     $3, %%rdi
   je       INT8_SAVE_C_M3N32
   cmpq     $2, %%rdi
   je       INT8_SAVE_C_M2N32
   cmpq     $1, %%rdi
   je       INT8_SAVE_C_M1N32
   int8_save_c_m4n32                                         
   imul     $4, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax                                    
   jmp      INT8_BEGIN_M4N32

INT8_SAVE_C_M3N32:
   vmovups         %%zmm12, (%%r12)                         
   vmovups         %%zmm13, 64(%%r12)

INT8_SAVE_C_M2N32:
   vmovups         %%zmm10, (%%r11)                         
   vmovups         %%zmm11, 64(%%r11)

INT8_SAVE_C_M1N32:
   vmovups         %%zmm8, (%%r10)                          
   vmovups         %%zmm9, 64(%%r10)
   jmp      INT8_END_N32

INT8_SAVE_C_M4N32_2:
   cmpq     $3, %%rdi
   je       INT8_SAVE_C_M3N32_2
   cmpq     $2, %%rdi
   je       INT8_SAVE_C_M2N32_2
   cmpq     $1, %%rdi
   je       INT8_SAVE_C_M1N32_2
   int8_save_c_m4n32_2
   imul     $4, %%r15, %%r11 // temp use %%r11
   add      %%r11, %%r9
   movq     %%r9, %%rax                                    
   jmp      INT8_BEGIN_M4N32

INT8_SAVE_C_M3N32_2:
   vpmovsdb   %%zmm12, %%xmm20                                  
   vpmovsdb   %%zmm13, %%xmm21
   vmovups    %%xmm20, 64(%%r10)
   vmovups    %%xmm21, 80(%%r10)

INT8_SAVE_C_M2N32_2:
   vpmovsdb   %%zmm10, %%xmm18                                  
   vpmovsdb   %%zmm11, %%xmm19
   vmovups    %%xmm18, 32(%%r10)
   vmovups    %%xmm19, 48(%%r10)

INT8_SAVE_C_M1N32_2:
   vpmovsdb   %%zmm8, %%xmm16                                  
   vpmovsdb   %%zmm9, %%xmm17
   vmovups    %%xmm16, (%%r10)
   vmovups    %%xmm17, 16(%%r10)

INT8_END_N32:
