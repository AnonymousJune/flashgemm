BEGIN_NPACK_B_matrix:
   mov %[LN], %%rdi
   mov %[K], %%r8
   mov %[B], %%rbx
   mov %[Bc], %%rbp

   mov  %%r8, %%rsi 
   imul $128, %%rsi  // Bc+=4(size_of_float)*K*32(nr)

   mov  %%rdi, %%r9
   imul $32, %%r9   // B+=4(size_of_float)*LN*8(loop_k)
   
   mov  %%rbp, %%r10
   mov  %%rbx, %%r11

   mov %[N], %%rcx


NPACK_B_N32:
   movq %%rbx, %%r12
   leaq (%%r12, %%rdi, 4), %%r13
   leaq (%%r13, %%rdi, 4), %%r14
   leaq (%%r14, %%rdi, 4), %%r15

   vmovups (%%r12), %%zmm0
   vmovups 64(%%r12), %%zmm1
   vmovups (%%r13), %%zmm2
   vmovups 64(%%r13), %%zmm3
   vmovups (%%r14), %%zmm4
   vmovups 64(%%r14), %%zmm5
   vmovups (%%r15), %%zmm6
   vmovups 64(%%r15), %%zmm7

   vmovups %%zmm0, (%%rbp)
   vmovups %%zmm1, 64(%%rbp)
   vmovups %%zmm2, 128(%%rbp)
   vmovups %%zmm3, 192(%%rbp)
   vmovups %%zmm4, 256(%%rbp)
   vmovups %%zmm5, 320(%%rbp)
   vmovups %%zmm6, 384(%%rbp)
   vmovups %%zmm7, 448(%%rbp)

   leaq (%%r15, %%rdi, 4), %%r12
   leaq (%%r12, %%rdi, 4), %%r13
   leaq (%%r13, %%rdi, 4), %%r14
   leaq (%%r14, %%rdi, 4), %%r15

   vmovups (%%r12), %%zmm8
   vmovups 64(%%r12), %%zmm9
   vmovups (%%r13), %%zmm10
   vmovups 64(%%r13), %%zmm11
   vmovups (%%r14), %%zmm12
   vmovups 64(%%r14), %%zmm13
   vmovups (%%r15), %%zmm14
   vmovups 64(%%r15), %%zmm15

   vmovups %%zmm8, 512(%%rbp)
   vmovups %%zmm9, 576(%%rbp)
   vmovups %%zmm10, 640(%%rbp)
   vmovups %%zmm11, 704(%%rbp)
   vmovups %%zmm12, 768(%%rbp)
   vmovups %%zmm13, 832(%%rbp)
   vmovups %%zmm14, 896(%%rbp)
   vmovups %%zmm15, 960(%%rbp)

   addq $128, %%rbx  // 4*32
   addq %%rsi, %%rbp

   subq $32, %%rcx
   cmp  $0, %%rcx
   jg   NPACK_B_N32

NPACK_B_K8:
   subq $8, %%r8
   cmp  $0, %%r8
   je   NPACK_B_END
   addq %%r9, %%r11
   mov  %%r11, %%rbx
   addq $1024, %%r10 // K(8)*nr(32)*size_of_float(4)
   mov  %%r10, %%rbp
   mov  %[N], %%rcx
   jmp  NPACK_B_N32

NPACK_B_END:        