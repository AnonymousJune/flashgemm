// 共4x64/4x16两种核，仅适用于M=4的情况
// Ac已打包(zmm0-7)，B不打包(zmm8-15)，C(zmm16-31)
// Bc, %%r14, %%rbp没有用了
// rbx: 存放当前k行开始的B指针 
// rbp: 存放预取k行的B指针
// r9: 存放当前[kc,nr]B块开始的指针
//-----------------------------------------------------------------

.macro	KERNEL4x64_K1_MLN								
	addq  			$16, %%rax
	leaq           (%%rbx, %%r8, 4), %%rbx
	leaq           (%%rbp, %%r8, 4), %%rbp

	vfmadd231ps		%%zmm0, %%zmm8, %%zmm16 			
	vfmadd231ps		%%zmm0, %%zmm9, %%zmm17 			
	vfmadd231ps		%%zmm0, %%zmm10, %%zmm18 			
	vfmadd231ps		%%zmm0, %%zmm11, %%zmm19
	vbroadcastss	(%%rax), %%zmm4
	vmovups 		   (%%rbx), %%zmm12
	prefetcht1      (%%rbp)
	prefetcht2     256(%%rbx)

	vfmadd231ps		%%zmm1, %%zmm8, %%zmm20 			
	vfmadd231ps		%%zmm1, %%zmm9, %%zmm21 			
	vfmadd231ps		%%zmm1, %%zmm10, %%zmm22 			
	vfmadd231ps		%%zmm1, %%zmm11, %%zmm23
	vbroadcastss	4(%%rax), %%zmm5
	vmovups 		   64(%%rbx), %%zmm13
	prefetcht1      64(%%rbp)
	prefetcht2     320(%%rbx)

	vfmadd231ps		%%zmm2, %%zmm8, %%zmm24 			
	vfmadd231ps		%%zmm2, %%zmm9, %%zmm25 			
	vfmadd231ps		%%zmm2, %%zmm10, %%zmm26 			
	vfmadd231ps		%%zmm2, %%zmm11, %%zmm27
	vbroadcastss	8(%%rax), %%zmm6
	vmovups 		   128(%%rbx), %%zmm14
	prefetcht1      128(%%rbp)
	prefetcht2     384(%%rbx)

	vfmadd231ps		%%zmm3, %%zmm8, %%zmm28 			
	vfmadd231ps		%%zmm3, %%zmm9, %%zmm29 			
	vfmadd231ps		%%zmm3, %%zmm10, %%zmm30 			
	vfmadd231ps		%%zmm3, %%zmm11, %%zmm31
	vbroadcastss	12(%%rax), %%zmm7
	vmovups 		   192(%%rbx), %%zmm15
	prefetcht1      192(%%rbp)
	prefetcht2     448(%%rbx)	   					

.endm 												

.macro	KERNEL4x64_K2_MLN	
	addq  			$16, %%rax
	leaq           (%%rbx, %%r8, 4), %%rbx
	leaq           (%%rbp, %%r8, 4), %%rbp

	vfmadd231ps		%%zmm4, %%zmm12, %%zmm16 			
	vfmadd231ps		%%zmm4, %%zmm13, %%zmm17 			
	vfmadd231ps		%%zmm4, %%zmm14, %%zmm18 			
	vfmadd231ps		%%zmm4, %%zmm15, %%zmm19
	vbroadcastss	(%%rax), %%zmm0
	vmovups 		   (%%rbx), %%zmm8
	prefetcht1      (%%rbp)
	prefetcht2     256(%%rbx)

	vfmadd231ps		%%zmm5, %%zmm12, %%zmm20 			
	vfmadd231ps		%%zmm5, %%zmm13, %%zmm21 			
	vfmadd231ps		%%zmm5, %%zmm14, %%zmm22 			
	vfmadd231ps		%%zmm5, %%zmm15, %%zmm23
	vbroadcastss	4(%%rax), %%zmm1
	vmovups 		   64(%%rbx), %%zmm9
	prefetcht1      64(%%rbp)
	prefetcht2     320(%%rbx)

	vfmadd231ps		%%zmm6, %%zmm12, %%zmm24 			
	vfmadd231ps		%%zmm6, %%zmm13, %%zmm25 			
	vfmadd231ps		%%zmm6, %%zmm14, %%zmm26 			
	vfmadd231ps		%%zmm6, %%zmm15, %%zmm27
	vbroadcastss	8(%%rax), %%zmm2
	vmovups 		   128(%%rbx), %%zmm10
	prefetcht1      128(%%rbp)
	prefetcht2     384(%%rbx)

	vfmadd231ps		%%zmm7, %%zmm12, %%zmm28 			
	vfmadd231ps		%%zmm7, %%zmm13, %%zmm29 			
	vfmadd231ps		%%zmm7, %%zmm14, %%zmm30 			
	vfmadd231ps		%%zmm7, %%zmm15, %%zmm31
	vbroadcastss	12(%%rax), %%zmm3
	vmovups 		   192(%%rbx), %%zmm11
	prefetcht1      192(%%rbp)
	prefetcht2     448(%%rbx)	            				
    					
.endm 												

.macro	KERNEL4x64_END_K_MLN							

	vfmadd231ps		%%zmm4, %%zmm12, %%zmm16 			
	vfmadd231ps		%%zmm4, %%zmm13, %%zmm17 			
	vfmadd231ps		%%zmm4, %%zmm14, %%zmm18 			
	vfmadd231ps		%%zmm4, %%zmm15, %%zmm19

	vfmadd231ps		%%zmm5, %%zmm12, %%zmm20 			
	vfmadd231ps		%%zmm5, %%zmm13, %%zmm21 			
	vfmadd231ps		%%zmm5, %%zmm14, %%zmm22 			
	vfmadd231ps		%%zmm5, %%zmm15, %%zmm23

	vfmadd231ps		%%zmm6, %%zmm12, %%zmm24 			
	vfmadd231ps		%%zmm6, %%zmm13, %%zmm25 			
	vfmadd231ps		%%zmm6, %%zmm14, %%zmm26 			
	vfmadd231ps		%%zmm6, %%zmm15, %%zmm27

	vfmadd231ps		%%zmm7, %%zmm12, %%zmm28 			
	vfmadd231ps		%%zmm7, %%zmm13, %%zmm29 			
	vfmadd231ps		%%zmm7, %%zmm14, %%zmm30 			
	vfmadd231ps		%%zmm7, %%zmm15, %%zmm31										

.endm 												

.macro	ADD_C_4x64_MLN									

	vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm16, %%zmm16			
	vmovups 		64(%%r10), %%zmm1        		
	vaddps 			%%zmm1, %%zmm17, %%zmm17
	vmovups 		128(%%r10), %%zmm2        			
	vaddps 			%%zmm2, %%zmm18, %%zmm18			
	vmovups 		192(%%r10), %%zmm3        		
	vaddps 			%%zmm3, %%zmm19, %%zmm19

	vmovups 		(%%r11), %%zmm4        			
	vaddps 			%%zmm4, %%zmm20, %%zmm20		
	vmovups 		64(%%r11), %%zmm5        		
	vaddps 			%%zmm5, %%zmm21, %%zmm21		
	vmovups 		128(%%r11), %%zmm6        			
	vaddps 			%%zmm6, %%zmm22, %%zmm22		
	vmovups 		192(%%r11), %%zmm7        		
	vaddps 			%%zmm7, %%zmm23, %%zmm23

	vmovups 		(%%r12), %%zmm8        			
	vaddps 			%%zmm8, %%zmm24, %%zmm24		
	vmovups 		64(%%r12), %%zmm9        		
	vaddps 			%%zmm9, %%zmm25, %%zmm25		
	vmovups 		128(%%r12), %%zmm10        			
	vaddps 			%%zmm10, %%zmm26, %%zmm26		
	vmovups 		192(%%r12), %%zmm11        		
	vaddps 			%%zmm11, %%zmm27, %%zmm27					

	vmovups 		(%%r13), %%zmm12        			
	vaddps 			%%zmm12, %%zmm28, %%zmm28		
	vmovups 		64(%%r13), %%zmm13        		
	vaddps 			%%zmm13, %%zmm29, %%zmm29		
	vmovups 		128(%%r13), %%zmm14        			
	vaddps 			%%zmm14, %%zmm30, %%zmm30		
	vmovups 		192(%%r13), %%zmm15        		
	vaddps 			%%zmm15, %%zmm31, %%zmm31

.endm 												

.macro	SAVE_4x64_MLN 									

	vmovups 		%%zmm16, (%%r10)
	vmovups 		%%zmm17, 64(%%r10)
	vmovups 		%%zmm18, 128(%%r10)
	vmovups 		%%zmm19, 192(%%r10)

	vmovups 		%%zmm20, (%%r11)        		
	vmovups 		%%zmm21, 64(%%r11)        		
	vmovups 		%%zmm22, 128(%%r11)        		
	vmovups 		%%zmm23, 192(%%r11)

	vmovups 		%%zmm24, (%%r12)        		
	vmovups 		%%zmm25, 64(%%r12)        		
	vmovups 		%%zmm26, 128(%%r12)        		
	vmovups 		%%zmm27, 192(%%r12)    

	vmovups 		%%zmm28, (%%r13)        		
	vmovups 		%%zmm29, 64(%%r13)        		
	vmovups 		%%zmm30, 128(%%r13)        		
	vmovups 		%%zmm31, 192(%%r13)     		        		
			
	subq			$64, %%rdi		// TODO
	addq			$256, %%rcx		// C0
	addq  			$256, %%r9 		// TODO	

.endm 												

//-----------------------------------------------------------------

.macro	KERNEL4x16_K1_MLN								

	addq  			$16, %%rax
	leaq           (%%rbx, %%r8, 4), %%rbx
	leaq           (%%rbp, %%r8, 4), %%rbp

	vfmadd231ps		%%zmm0, %%zmm8, %%zmm16
	vbroadcastss	(%%rax), %%zmm4
	vmovups 		(%%rbx), %%zmm12
	prefetcht1		(%%rbp)
	prefetcht2		256(%%rbx)

	vfmadd231ps		%%zmm1, %%zmm8, %%zmm20 			
	vbroadcastss	4(%%rax), %%zmm5

	vfmadd231ps		%%zmm2, %%zmm8, %%zmm24 			
	vbroadcastss	8(%%rax), %%zmm6

	vfmadd231ps		%%zmm3, %%zmm8, %%zmm28 			
	vbroadcastss	12(%%rax), %%zmm7
   			 		
.endm 												

.macro	KERNEL4x16_K2_MLN								

	addq  			$16, %%rax
	leaq           (%%rbx, %%r8, 4), %%rbx
	leaq           (%%rbp, %%r8, 4), %%rbp

	vfmadd231ps		%%zmm4, %%zmm12, %%zmm16
	vbroadcastss	(%%rax), %%zmm0
	vmovups 		(%%rbx), %%zmm12
	prefetcht1		(%%rbp)
	prefetcht2		256(%%rbx)

	vfmadd231ps		%%zmm5, %%zmm12, %%zmm20 			
	vbroadcastss	4(%%rax), %%zmm1

	vfmadd231ps		%%zmm6, %%zmm12, %%zmm24 			
	vbroadcastss	8(%%rax), %%zmm2

	vfmadd231ps		%%zmm7, %%zmm12, %%zmm28 			
	vbroadcastss	12(%%rax), %%zmm3              		 			

.endm 												

.macro	KERNEL4x16_END_K_MLN							

	vfmadd231ps		%%zmm4, %%zmm12, %%zmm16 			
	vfmadd231ps		%%zmm5, %%zmm12, %%zmm20 			
	vfmadd231ps		%%zmm6, %%zmm12, %%zmm24 			
	vfmadd231ps		%%zmm7, %%zmm12, %%zmm28 							 				
 		
.endm 												

.macro	ADD_C_4x16_MLN									

	vmovups 		(%%r10), %%zmm0        			
	vaddps 			%%zmm0, %%zmm16, %%zmm16			

	vmovups 		(%%r11), %%zmm4        			
	vaddps 			%%zmm4, %%zmm20, %%zmm20		

	vmovups 		(%%r12), %%zmm8        			
	vaddps 			%%zmm8, %%zmm24, %%zmm24							

	vmovups 		(%%r13), %%zmm12        			
	vaddps 			%%zmm12, %%zmm28, %%zmm28		

.endm 												

.macro	SAVE_4x16_MLN

	vmovups 		%%zmm16, (%%r10)
	vmovups 		%%zmm20, (%%r11)        		
	vmovups 		%%zmm24, (%%r12)        		   
	vmovups 		%%zmm28, (%%r13)        		        		       		 		
       		       		
	subq 			$16, %%rdi
	addq  	   		$64, %%rcx	// C0
	addq  			$64, %%r9 	// TODO					
.endm 		

//-----------------------------------------------------------------

SMM_KERNEL4x64_MLN:								

	mov 	%[C], %%rcx   	 						
	mov 	%[Ac], %%rax   	 						
	mov 	%[B], %%rbx   	 						

	prefetcht0 		(%%rax)
	prefetcht0 		(%%rbx)             

	mov 	%[K], %%rdx								// K(kc)
	mov  	%[LN], %%r8
	mov  	%%rax, %%r14								// 存储Ac地址
	movq	%[N], %%rdi
	mov 	%[k_tag], %%r15							// kk=0把C存回内存, 否则加回对应的C位置

	mov   %%r8, %%rsi
	shl   $5, %%rsi  // TODO B预取的间隔
	mov 	%%rbx, %%r9 							 // B备份

//-----------------------------------------------------------------

BEGIN_M4N64_MLN:	
	cmpq     $64, %%rdi
	jb   	   BEGIN_M4N16_MLN
	mov 	   %[K], %%rdx              // K

	mov   %%r9, %%rbx                   	// B
	vmovups			(%%rbx), %%zmm8      	// B0
	vmovups     	64(%%rbx), %%zmm9    	// B1
	vmovups     	128(%%rbx), %%zmm10  	// B2
	vmovups     	192(%%rbx), %%zmm11  	// B3

	mov 	%%r14, %%rax                // Ac
	vbroadcastss	(%%rax), %%zmm0    	// A0
	vbroadcastss	4(%%rax), %%zmm1    // A1
	vbroadcastss	8(%%rax), %%zmm2    // A2
	vbroadcastss	12(%%rax), %%zmm3   // A3

	mov   %%r9, %%rbp                      
	addq  %%rsi, %%rbp                  // B prefetch
	prefetcht1       (%%rbp)
	prefetcht1       64(%%rbp)
	prefetcht1       128(%%rbp)
	prefetcht1       192(%%rbp)

	vpxorq 			%%zmm16, %%zmm16, %%zmm16 				
	vpxorq 			%%zmm17, %%zmm17, %%zmm17 				
	vpxorq 			%%zmm18, %%zmm18, %%zmm18 				
	vpxorq 			%%zmm19, %%zmm19, %%zmm19 				
	vpxorq 			%%zmm20, %%zmm20, %%zmm20				
	vpxorq 			%%zmm21, %%zmm21, %%zmm21				
	vpxorq 			%%zmm22, %%zmm22, %%zmm22				
	vpxorq 			%%zmm23, %%zmm23, %%zmm23				
	vpxorq 			%%zmm24, %%zmm24, %%zmm24				
	vpxorq 			%%zmm25, %%zmm25, %%zmm25				
	vpxorq 			%%zmm26, %%zmm26, %%zmm26				
	vpxorq 			%%zmm27, %%zmm27, %%zmm27				
	vpxorq 			%%zmm28, %%zmm28, %%zmm28				
	vpxorq 			%%zmm29, %%zmm29, %%zmm29				
	vpxorq 			%%zmm30, %%zmm30, %%zmm30				
	vpxorq 			%%zmm31, %%zmm31, %%zmm31				

	mov  	%%rcx, %%r10                     // C0
	prefetcht2 		(%%r10)                 		
	prefetcht2 		64(%%r10)                 		
	prefetcht2 		128(%%r10)                 		
	prefetcht2 		192(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11          // C1
	prefetcht2 		(%%r11)                 		
	prefetcht2 		64(%%r11)                 		
	prefetcht2 		128(%%r11)                 		
	prefetcht2 		192(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12          // C2
	prefetcht2 		(%%r12)                 		
	prefetcht2 		64(%%r12)                 		
	prefetcht2 		128(%%r12)                 		
	prefetcht2 		192(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13          // C3
	prefetcht2 		(%%r13) 		 			
	prefetcht2 		64(%%r13) 		 			
	prefetcht2 		128(%%r13) 		 			
	prefetcht2 		192(%%r13) 		 			

	subq 	$8, %%rdx 

MAIN_K_M4N64_MLN:											

	KERNEL4x64_K1_MLN 									
	KERNEL4x64_K2_MLN 									
	KERNEL4x64_K1_MLN 									
	KERNEL4x64_K2_MLN 									
	KERNEL4x64_K1_MLN 									
	KERNEL4x64_K2_MLN 									
	KERNEL4x64_K1_MLN 									
	cmp 	$0, %%rdx           					
	je 		EDGE_K_M4N64_MLN									
	KERNEL4x64_K2_MLN
	subq 	$8, %%rdx  												
	jmp     MAIN_K_M4N64_MLN       							

EDGE_K_M4N64_MLN:											
	KERNEL4x64_END_K_MLN 								

BEGIN_SAVE_M4N64_MLN:										
	cmp 	$0, %%r15								
	je  	SAVE_C_M4N64_MLN 									
	ADD_C_4x64_MLN 									

SAVE_C_M4N64_MLN: 											
	SAVE_4x64_MLN 										
	cmpq  	$64, %%rdi 	// TODO							
	jnb 	BEGIN_M4N64_MLN 					//不小于（或等于）则跳转			

//----------------------------------------------------------------

BEGIN_M4N16_MLN:		
	cmpq		$16, %%rdi
	jb			END_M4N64_MLN
	mov			%[K], %%rdx					// K

	mov			%%r9, %%rbx					// B
	vmovups			(%%rbx), %%zmm8      	// B0

	mov 	%%r14, %%rax                // Ac
	vbroadcastss	(%%rax), %%zmm0    	// A0
	vbroadcastss	4(%%rax), %%zmm1    // A1
	vbroadcastss	8(%%rax), %%zmm2    // A2
	vbroadcastss	12(%%rax), %%zmm3   // A3

	mov   %%r9, %%rbp                      
	addq  %%rsi, %%rbp                  // B prefetch
	prefetcht1       (%%rbp)

	vpxorq 			%%zmm16, %%zmm16, %%zmm16 				
	vpxorq 			%%zmm17, %%zmm17, %%zmm17 				
	vpxorq 			%%zmm18, %%zmm18, %%zmm18 				
	vpxorq 			%%zmm19, %%zmm19, %%zmm19 				
	vpxorq 			%%zmm20, %%zmm20, %%zmm20				
	vpxorq 			%%zmm21, %%zmm21, %%zmm21				
	vpxorq 			%%zmm22, %%zmm22, %%zmm22				
	vpxorq 			%%zmm23, %%zmm23, %%zmm23				
	vpxorq 			%%zmm24, %%zmm24, %%zmm24				
	vpxorq 			%%zmm25, %%zmm25, %%zmm25				
	vpxorq 			%%zmm26, %%zmm26, %%zmm26				
	vpxorq 			%%zmm27, %%zmm27, %%zmm27				
	vpxorq 			%%zmm28, %%zmm28, %%zmm28				
	vpxorq 			%%zmm29, %%zmm29, %%zmm29				
	vpxorq 			%%zmm30, %%zmm30, %%zmm30				
	vpxorq 			%%zmm31, %%zmm31, %%zmm31				

	mov  	%%rcx, %%r10                     // C0
	prefetcht2 		(%%r10)
	leaq 	(%%r10, %%r8, 4), %%r11          // C1
	prefetcht2 		(%%r11)
	leaq 	(%%r11, %%r8, 4), %%r12          // C2
	prefetcht2 		(%%r12)
	leaq 	(%%r12, %%r8, 4), %%r13          // C3
	prefetcht2 		(%%r13)	 			

	subq 	$8, %%rdx 

MAIN_K_M4N16_MLN:											
	KERNEL4x16_K1_MLN 									
	KERNEL4x16_K2_MLN 									
	KERNEL4x16_K1_MLN 									
	KERNEL4x16_K2_MLN 									
	KERNEL4x16_K1_MLN 									
	KERNEL4x16_K2_MLN 									
	KERNEL4x16_K1_MLN 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4N16_MLN									
	KERNEL4x16_K2_MLN
   subq 	$8, %%rdx			
   jmp   MAIN_K_M4N16_MLN       							

EDGE_K_M4N16_MLN:											
	KERNEL4x16_END_K_MLN 																		
	cmp 	$0, %%r15								
	je  	SAVE_C_M4N16_MLN 									
	ADD_C_4x16_MLN 									

SAVE_C_M4N16_MLN: 											
	SAVE_4x16_MLN

//----------------------------------------------------------------

END_M4N64_MLN:												
