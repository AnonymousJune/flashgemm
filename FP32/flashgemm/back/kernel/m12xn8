.macro	KERNEL12x8_PACK_K1							

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm21 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm22 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm26 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm27 		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm29 		
   vmovups 		(%%rbx), %%ymm6        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm31 		
   vmovups 		%%ymm4, (%%rbp)       			
	addq  			$32, %%rbp                //64->32 						

.endm 												

.macro	KERNEL12x8_PACK_K2							

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm21 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   prefetcht0 		32(%%rbx)       //          		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
   vmovups 		(%%rbx), %%ymm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		
   vmovups 		%%ymm6, (%%rbp)       			
	addq  			$32, %%rbp                //64->32 						

.endm 												

.macro	KERNEL12x8_PACK_END_K						

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm21 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
	addq  			$48, %%rax 						

   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		
   vmovups 		%%ymm6, (%%rbp)       			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL12x8_K1								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm21 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm22 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm26 		

	addq  			$32, %%rbx 						 // B

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm27 		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm29 		
   vmovups 		(%%rbx), %%ymm6        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm31 		

.endm 												

.macro	KERNEL12x8_K2								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm21 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

	addq 			$32, %%rbx    //				

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   prefetcht0 		32(%%rbx)  //               		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
   vmovups 		(%%rbx), %%ymm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		

.endm 												

.macro	KERNEL12x8_END_K							

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm20 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm21 		

   vbroadcastss	16(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm22 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	20(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm23 		

   vbroadcastss	24(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	28(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   vbroadcastss	32(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

   vbroadcastss	36(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   vbroadcastss	40(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	44(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
   vmovups 		(%%rbx), %%ymm4        			
	addq  			$48, %%rax 						

   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		

.endm 												

.macro	ADD_C_12x8									

   vmovups 		(%%r10), %%ymm0        			
	vaddps 			%%ymm0, %%ymm20, %%ymm20		
   vmovups 		(%%r11), %%ymm1        			
	vaddps 			%%ymm1, %%ymm21, %%ymm21		
   vmovups 		(%%r12), %%ymm2        			
	vaddps 			%%ymm2, %%ymm22, %%ymm22		
   vmovups 		(%%r13), %%ymm3        			
	vaddps 			%%ymm3, %%ymm23, %%ymm23		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%ymm4        			
	vaddps 			%%ymm4, %%ymm24, %%ymm24		
   vmovups 		(%%r11), %%ymm5        			
	vaddps 			%%ymm5, %%ymm25, %%ymm25		
   vmovups 		(%%r12), %%ymm6        			
	vaddps 			%%ymm6, %%ymm26, %%ymm26		
   vmovups 		(%%r13), %%ymm7        			
	vaddps 			%%ymm7, %%ymm27, %%ymm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%ymm0        			
	vaddps 			%%ymm0, %%ymm28, %%ymm28		
   vmovups 		(%%r11), %%ymm1        			
	vaddps 			%%ymm1, %%ymm29, %%ymm29		
   vmovups 		(%%r12), %%ymm2        			
	vaddps 			%%ymm2, %%ymm30, %%ymm30		
   vmovups 		(%%r13), %%ymm3        			
	vaddps 			%%ymm3, %%ymm31, %%ymm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_12x8 									

   vmovups 		%%ymm20, (%%r10)        		
   vmovups 		%%ymm21, (%%r11)        		
	subq 			$12, %%rdi 						
   vmovups 		%%ymm22, (%%r12)        		
   vmovups 		%%ymm23, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%ymm24, (%%r10)        		
   vmovups 		%%ymm25, (%%r11)        		
   vmovups 		%%ymm26, (%%r12)        		
   vmovups 		%%ymm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%ymm28, (%%r10)        		
   vmovups 		%%ymm29, (%%r11)        		
   vmovups 		%%ymm30, (%%r12)        		
   vmovups 		%%ymm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL8x8_K1								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm24 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%ymm8    			
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm26 		

   vbroadcastss	20(%%rax), %%ymm9    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm27 		

   vbroadcastss	24(%%rax), %%ymm10    			
   vfmadd231ps		%%ymm8, %%ymm4, %%ymm28 		

   prefetcht0 		32(%%rbx) //                		

   vbroadcastss	28(%%rax), %%ymm11    			
   vfmadd231ps		%%ymm9, %%ymm4, %%ymm29 		
	addq  			$32, %%rbx 						 //
   vfmadd231ps		%%ymm10, %%ymm4, %%ymm30 		

	addq  			$32, %%rax 						
   vmovups 		(%%rbx), %%ymm6        			
   vfmadd231ps		%%ymm11, %%ymm4, %%ymm31 		

   vbroadcastss	(%%rax), %%ymm0    				
   vbroadcastss	4(%%rax), %%ymm1    			

.endm 												

.macro	KERNEL8x8_K2								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%ymm8    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

   vbroadcastss	20(%%rax), %%ymm9    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   vbroadcastss	24(%%rax), %%ymm10    			
   vfmadd231ps		%%ymm8, %%ymm6, %%ymm28 		

   prefetcht0 		32(%%rbx)              //   		

   vbroadcastss	28(%%rax), %%ymm11    			
   vfmadd231ps		%%ymm9, %%ymm6, %%ymm29 		
	addq  			$32, %%rbx 						 // B
   vfmadd231ps		%%ymm10, %%ymm6, %%ymm30 		

	addq  			$32, %%rax 						
   vmovups 		(%%rbx), %%ymm4        			
   vfmadd231ps		%%ymm11, %%ymm6, %%ymm31 		

   vbroadcastss	(%%rax), %%ymm0    				
   vbroadcastss	4(%%rax), %%ymm1    			

.endm 												

.macro	KERNEL8x8_END_K							

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm24 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm25 		

   prefetcht0 		256(%%rax)                 		

   vbroadcastss	16(%%rax), %%ymm8    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm26 		

   vbroadcastss	20(%%rax), %%ymm9    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm27 		

   vbroadcastss	24(%%rax), %%ymm10    			
   vfmadd231ps		%%ymm8, %%ymm6, %%ymm28 		

   vbroadcastss	28(%%rax), %%ymm11    			
   vfmadd231ps		%%ymm9, %%ymm6, %%ymm29 		
   vfmadd231ps		%%ymm10, %%ymm6, %%ymm30 		

	addq  			$32, %%rax 						
   vfmadd231ps		%%ymm11, %%ymm6, %%ymm31 		

.endm 												

.macro	ADD_C_8x8									

   vmovups 		(%%r10), %%ymm4        			
	vaddps 			%%ymm4, %%ymm24, %%ymm24		
   vmovups 		(%%r11), %%ymm5        			
	vaddps 			%%ymm5, %%ymm25, %%ymm25		
   vmovups 		(%%r12), %%ymm6        			
	vaddps 			%%ymm6, %%ymm26, %%ymm26		
   vmovups 		(%%r13), %%ymm7        			
	vaddps 			%%ymm7, %%ymm27, %%ymm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   vmovups 		(%%r10), %%ymm0        			
	vaddps 			%%ymm0, %%ymm28, %%ymm28		
   vmovups 		(%%r11), %%ymm1        			
	vaddps 			%%ymm1, %%ymm29, %%ymm29		
   vmovups 		(%%r12), %%ymm2        			
	vaddps 			%%ymm2, %%ymm30, %%ymm30		
   vmovups 		(%%r13), %%ymm3        			
	vaddps 			%%ymm3, %%ymm31, %%ymm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_8x8 									

   vmovups 		%%ymm24, (%%r10)        		
   vmovups 		%%ymm25, (%%r11)        		
	subq 			$8, %%rdi 						
   vmovups 		%%ymm26, (%%r12)        		
   vmovups 		%%ymm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovups 		%%ymm28, (%%r10)        		
   vmovups 		%%ymm29, (%%r11)        		
   vmovups 		%%ymm30, (%%r12)        		
   vmovups 		%%ymm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL4x8_K1								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm28 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm4, %%ymm29 		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$32, %%rbx 						 // B
   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm4, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm4, %%ymm31 		

   prefetcht0 		32(%%rbx)                 		
   vmovups 		(%%rbx), %%ymm6        			

.endm 												

.macro	KERNEL4x8_K2								

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$32, %%rbx 						 // B
   vbroadcastss	(%%rax), %%ymm0    				
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		

   vbroadcastss	4(%%rax), %%ymm1    			
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		

   vmovups 		(%%rbx), %%ymm4        			
   prefetcht0 		32(%%rbx)                 		

.endm 												

.macro	KERNEL4x8_END_K							

   vbroadcastss	8(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm6, %%ymm28 		

   vbroadcastss	12(%%rax), %%ymm3    			
   vfmadd231ps		%%ymm1, %%ymm6, %%ymm29 		
	addq  			$16, %%rax 						

   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 		
   vfmadd231ps		%%ymm3, %%ymm6, %%ymm31 		

.endm 												

.macro	ADD_C_4x8									

   vmovups 		(%%r10), %%ymm0        			
	vaddps 			%%ymm0, %%ymm28, %%ymm28		
   vmovups 		(%%r11), %%ymm1        			
	vaddps 			%%ymm1, %%ymm29, %%ymm29		
   vmovups 		(%%r12), %%ymm2        			
	vaddps 			%%ymm2, %%ymm30, %%ymm30		
   vmovups 		(%%r13), %%ymm3        			
	vaddps 			%%ymm3, %%ymm31, %%ymm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_4x8 	
   subq 			$4, %%rdi 						

   vmovups 		%%ymm28, (%%r10)        		
   vmovups 		%%ymm29, (%%r11)        		
   vmovups 		%%ymm30, (%%r12)        		
   vmovups 		%%ymm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												


//-----------------------------------------------------------------

.macro	KERNEL1x8_K1
   prefetcht0 		256(%%rax)
   addq  			$4, %%rax 

   vbroadcastss	(%%rax), %%ymm2    			
   vfmadd231ps		%%ymm0, %%ymm4, %%ymm30 				
							           		
	addq  			$32, %%rbx 						 // B	
   vmovups 		   (%%rbx), %%ymm6  
   prefetcht0 		32(%%rbx)        			

.endm 												

.macro	KERNEL1x8_K2		
   prefetcht0 		256(%%rax)  						
	addq  			$4, %%rax

   vbroadcastss	(%%rax), %%ymm0    			
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 					
                  		
	addq  			$32, %%rbx 						 // B		
   vmovups 		   (%%rbx), %%ymm4        			
   prefetcht0 		32(%%rbx)                 		

.endm 												

.macro	KERNEL1x8_END_K									
   vfmadd231ps		%%ymm2, %%ymm6, %%ymm30 					
	addq  			$4, %%rax 								
.endm 												

.macro	ADD_C_1x8											
   vmovups 		(%%r10), %%ymm2        			
	vaddps 			%%ymm2, %%ymm30, %%ymm30				
	mov  	%%rcx, %%r10 							   // C0
.endm 												

.macro	SAVE_1x8 									
   vmovups 		%%ymm30, (%%r10)        		
   subq 			$1, %%rdi        		       		
	leaq  	(%%r10, %%r8, 4), %%rcx 			// C0

.endm 												

//-----------------------------------------------------------------

SMM_NN_KERNEL12x8:								

   mov 	%[C], %%rcx   	 						
   mov 	%[A], %%rax   	 						
   mov 	%[B], %%rbx   	 						

   prefetcht0 		(%%rax)                 		

	mov 	%[K], %%rdx 							 // K
	mov  	%[LN], %%r8 							
	mov  	%[Bc], %%r14 							
	mov  	%[M], %%rdi 							
	mov 	%[k_tag], %%r15 						

   prefetcht0 		(%%rbx)                 		
	mov 	%%rbx, %%r9 							 // B
	mov 	%%rdx, %%rsi 							 // K

BEGIN_PACK12x8:									

	mov 	%%r9, %%rbx 							 // B
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
	mov 	%%r14, %%rbp 							 // Bc

   vmovups		(%%rbx), %%ymm4        				
	vpxorq 		%%ymm20, %%ymm20, %%ymm20 			
	vpxorq 		%%ymm21, %%ymm21, %%ymm21 			
   vbroadcastss	(%%rax), %%ymm0    				
	vpxorq 		%%ymm22, %%ymm22, %%ymm22 			
	vpxorq 		%%ymm23, %%ymm23, %%ymm23 			
   vbroadcastss	4(%%rax), %%ymm1    			
	vpxorq 		%%ymm24, %%ymm24, %%ymm24 			
	vpxorq 		%%ymm25, %%ymm25, %%ymm25			
	vpxorq 		%%ymm26, %%ymm26, %%ymm26 			
	vpxorq 		%%ymm27, %%ymm27, %%ymm27 			
	vpxorq 		%%ymm28, %%ymm28, %%ymm28 			
	vpxorq 		%%ymm29, %%ymm29, %%ymm29			
	vpxorq 		%%ymm30, %%ymm30, %%ymm30 			
	vpxorq 		%%ymm31, %%ymm31, %%ymm31 			

	subq 	$8, %%rdx 								

MAIN_PACK_K12x8:									

	KERNEL12x8_PACK_K1 							
	KERNEL12x8_PACK_K2 							
	KERNEL12x8_PACK_K1 							
	KERNEL12x8_PACK_K2 							
	KERNEL12x8_PACK_K1 							
	KERNEL12x8_PACK_K2 							
	KERNEL12x8_PACK_K1 							
   cmp 	$0, %%rdx           					
	je 		EDGE_PACK_K12x8 						
	KERNEL12x8_PACK_K2 							

	subq 	$8, %%rdx 								
   jmp     MAIN_PACK_K12x8       					

EDGE_PACK_K12x8:									

	KERNEL12x8_PACK_END_K 							
	jmp  	BEGIN_SAVE_12x8 						

//-----------------------------------------------------------------

BEGIN_M12x8:										

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10) 
	leaq 	(%%r10, %%r8, 4), %%r11 			 // C1
   prefetcht1 		(%%r11) 
	leaq 	(%%r11, %%r8, 4), %%r12 			 // C2
   prefetcht1 		(%%r12) 
	leaq 	(%%r12, %%r8, 4), %%r13 			 // C3
   prefetcht1 		(%%r13) 

	mov 	%%rsi, %%rdx 							 // K									

   vmovups		(%%rbx), %%ymm4        				
	vpxorq 		%%ymm20, %%ymm20, %%ymm20 			
	vpxorq 		%%ymm21, %%ymm21, %%ymm21 			
	vpxorq 		%%ymm22, %%ymm22, %%ymm22 			
   vbroadcastss	(%%rax), %%ymm0    				
	vpxorq 		%%ymm23, %%ymm23, %%ymm23 			
	vpxorq 		%%ymm24, %%ymm24, %%ymm24 			
   vbroadcastss	4(%%rax), %%ymm1    			
	vpxorq 		%%ymm25, %%ymm25, %%ymm25			
	vpxorq 		%%ymm26, %%ymm26, %%ymm26 			
	vpxorq 		%%ymm27, %%ymm27, %%ymm27 			
	vpxorq 		%%ymm28, %%ymm28, %%ymm28 			
	vpxorq 		%%ymm29, %%ymm29, %%ymm29			
	vpxorq 		%%ymm30, %%ymm30, %%ymm30 			
	vpxorq 		%%ymm31, %%ymm31, %%ymm31 			

	subq 	$8, %%rdx 								

MAIN_K12x8:										

	KERNEL12x8_K1 									
	KERNEL12x8_K2 									
	KERNEL12x8_K1 									
	KERNEL12x8_K2 									
	KERNEL12x8_K1 									
	KERNEL12x8_K2 									
	KERNEL12x8_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K12x8 							
	KERNEL12x8_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K12x8       						

EDGE_K12x8:										

	KERNEL12x8_END_K 								

BEGIN_SAVE_12x8:									
	cmp 	$0, %%r15								
	je  	SAVE_C12x8 							
	ADD_C_12x8 									

SAVE_C12x8: 										
	SAVE_12x8 										

	cmpq  	$12, %%rdi 								
	jnb 	BEGIN_M12x8                 // 不小于（或等于）则跳转 							

//------------------------------------------------------------------

BEGIN_M8_N8:										
   cmpq  	$8, %%rdi  								 // M % 8
	jb   	BEGIN_M4_N8 	                      //小于则跳转

	mov 	%%r14, %%rbx 							    // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							    // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
   vmovups		(%%rbx), %%ymm4        				
	vpxorq 		%%ymm24, %%ymm24, %%ymm24 			
	vpxorq 		%%ymm25, %%ymm25, %%ymm25			
   vbroadcastss	(%%rax), %%ymm0    				
	vpxorq 		%%ymm26, %%ymm26, %%ymm26 			
	vpxorq 		%%ymm27, %%ymm27, %%ymm27 			
   vbroadcastss	4(%%rax), %%ymm1    			
	vpxorq 		%%ymm28, %%ymm28, %%ymm28 			
	vpxorq 		%%ymm29, %%ymm29, %%ymm29			
	vpxorq 		%%ymm30, %%ymm30, %%ymm30 			
	vpxorq 		%%ymm31, %%ymm31, %%ymm31 			

	subq 	$8, %%rdx 								

MAIN_K_M8_N8:										

	KERNEL8x8_K1 									
	KERNEL8x8_K2 									
	KERNEL8x8_K1 									
	KERNEL8x8_K2 									
	KERNEL8x8_K1 									
	KERNEL8x8_K2 									
	KERNEL8x8_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M8_N8 							
	KERNEL8x8_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M8_N8       					

EDGE_K_M8_N8:										

	KERNEL8x8_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_8x8 							
	ADD_C_8x8 										

SAVE_C_8x8: 										
	SAVE_8x8 										

//----------------------------------------------------------------

BEGIN_M4_N8:										

	cmpq  	$4, %%rdi  							 // M % 4
	jb   	BEGIN_M1_N8 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%ymm4        				
   vbroadcastss	(%%rax), %%ymm0    				
	vpxorq 		%%ymm28, %%ymm28, %%ymm28 			
	vpxorq 		%%ymm29, %%ymm29, %%ymm29			
   vbroadcastss	4(%%rax), %%ymm1    			
	vpxorq 		%%ymm30, %%ymm30, %%ymm30 			
	vpxorq 		%%ymm31, %%ymm31, %%ymm31 			

	subq 	$8, %%rdx 								

MAIN_K_M4_N8:										

	KERNEL4x8_K1 									
	KERNEL4x8_K2 									
	KERNEL4x8_K1 									
	KERNEL4x8_K2 									
	KERNEL4x8_K1 									
	KERNEL4x8_K2 									
	KERNEL4x8_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4_N8 							
	KERNEL4x8_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M4_N8       					

EDGE_K_M4_N8:										

	KERNEL4x8_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_4x8 							
	ADD_C_4x8 										

SAVE_C_4x8: 										
	SAVE_4x8 										


//----------------------------------------------------------------

BEGIN_M1_N8:										
	cmpq  	$1, %%rdi  							 // M % 1
	jb   	END_M_N8 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		              		                 		

	mov 	%%rsi, %%rdx 							 // K

   vmovups		(%%rbx), %%ymm4        				
   vbroadcastss	(%%rax), %%ymm0    		 // A0				
	vpxorq 		%%ymm30, %%ymm30, %%ymm30 						

	subq 	$8, %%rdx 									

MAIN_K_M1_N8:										
	KERNEL1x8_K1 									
	KERNEL1x8_K2 									
	KERNEL1x8_K1 									
	KERNEL1x8_K2 									
	KERNEL1x8_K1 									
	KERNEL1x8_K2 									
	KERNEL1x8_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M1_N8 							
	KERNEL1x8_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M1_N8   					

EDGE_K_M1_N8:										
	KERNEL1x8_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_1x8 							
	ADD_C_1x8 
									
SAVE_C_1x8: 										
	SAVE_1x8
   cmpq  	$1, %%rdi 
	jnb 	BEGIN_M1_N8 					//不小于（或等于）则跳转									

//-----------------------------------------------------------------


END_M_N8:											

