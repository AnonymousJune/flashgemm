.macro	KERNEL12x1_PACK_K1							

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm21     		

   prefetcht0 		256(%%rax)                 		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm22     		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm26     		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm27     		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm29     		
   movss   		(%%rbx), %%xmm6        			
	addq  			$48, %%rax 						

   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm31     		
   vmovss   		%%xmm4, (%%rbp)       			
	addq  			$4, %%rbp                //64->16 						

.endm 												

.macro	KERNEL12x1_PACK_K2							

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm21     		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm22     		

   prefetcht0 		256(%%rax)                 		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

	leaq  	(%%rbx, %%r8, 4), %%rbx 				 // B

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   prefetcht0 		4(%%rbx)       //          		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
   movss   		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		
   vmovss   		%%xmm6, (%%rbp)       			
	addq  			$4, %%rbp                //64->16 						

.endm 												

.macro	KERNEL12x1_PACK_END_K						

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm21     		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm22     		

   prefetcht0 		256(%%rax)                 		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
	addq  			$48, %%rax 						

   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		
   vmovss   		%%xmm6, (%%rbp)       			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL12x1_K1								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm21     		

   prefetcht0 		256(%%rax)                 		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm22     		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm26     		

	addq  			$4, %%rbx 						 // B

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm27     		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm29     		
   movss   		(%%rbx), %%xmm6        			
	addq  			$48, %%rax 						

   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm31     		

.endm 												

.macro	KERNEL12x1_K2								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm21     		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm22     		

   prefetcht0 		256(%%rax)                 		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

	addq 			$4, %%rbx    //				

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   prefetcht0 		4(%%rbx)  //               		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
   movss   		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		

.endm 												

.macro	KERNEL12x1_END_K							

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm20     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm21     		

   movss       	16(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm22     		

   prefetcht0 		256(%%rax)                 		

   movss       	20(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm23     		

   movss       	24(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	28(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   movss       	32(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

   movss       	36(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   movss       	40(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	44(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
   movss   		(%%rbx), %%xmm4        			
	addq  			$48, %%rax 						

   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		

.endm 												

.macro	ADD_C_12x1									

   movss   		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm20, %%xmm20		
   movss   		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm21, %%xmm21		
   movss   		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm22, %%xmm22		
   movss   		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm23, %%xmm23		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   movss   		(%%r10), %%xmm4        			
	vaddps 			%%xmm4, %%xmm24, %%xmm24		
   movss   		(%%r11), %%xmm5        			
	vaddps 			%%xmm5, %%xmm25, %%xmm25		
   movss   		(%%r12), %%xmm6        			
	vaddps 			%%xmm6, %%xmm26, %%xmm26		
   movss   		(%%r13), %%xmm7        			
	vaddps 			%%xmm7, %%xmm27, %%xmm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   movss   		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   movss   		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   movss   		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   movss   		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_12x1 									

   vmovss   		%%xmm20, (%%r10)        		
   vmovss   		%%xmm21, (%%r11)        		
	subq 			$12, %%rdi 						
   vmovss   		%%xmm22, (%%r12)        		
   vmovss   		%%xmm23, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovss   		%%xmm24, (%%r10)        		
   vmovss   		%%xmm25, (%%r11)        		
   vmovss   		%%xmm26, (%%r12)        		
   vmovss   		%%xmm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovss   		%%xmm28, (%%r10)        		
   vmovss   		%%xmm29, (%%r11)        		
   vmovss   		%%xmm30, (%%r12)        		
   vmovss   		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL8x1_K1								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm24     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm25     		

   prefetcht0 		256(%%rax)                 		

   movss       	16(%%rax), %%xmm8    			
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm26     		

   movss       	20(%%rax), %%xmm9    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm27     		

   movss       	24(%%rax), %%xmm10    			
   vfmadd231ps      		%%xmm8, %%xmm4, %%xmm28     		

   prefetcht0 		4(%%rbx) //                		

   movss       	28(%%rax), %%xmm11    			
   vfmadd231ps      		%%xmm9, %%xmm4, %%xmm29     		
	addq  			$4, %%rbx 						 //
   vfmadd231ps      		%%xmm10, %%xmm4, %%xmm30     		

	addq  			$32, %%rax 						
   movss   		(%%rbx), %%xmm6        			
   vfmadd231ps      		%%xmm11, %%xmm4, %%xmm31     		

   movss       	(%%rax), %%xmm0    				
   movss       	4(%%rax), %%xmm1    			

.endm 												

.macro	KERNEL8x1_K2								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   prefetcht0 		256(%%rax)                 		

   movss       	16(%%rax), %%xmm8    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

   movss       	20(%%rax), %%xmm9    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   movss       	24(%%rax), %%xmm10    			
   vfmadd231ps      		%%xmm8, %%xmm6, %%xmm28     		

   prefetcht0 		4(%%rbx)              //   		

   movss       	28(%%rax), %%xmm11    			
   vfmadd231ps      		%%xmm9, %%xmm6, %%xmm29     		
	addq  			$4, %%rbx 						 // B
   vfmadd231ps      		%%xmm10, %%xmm6, %%xmm30     		

	addq  			$32, %%rax 						
   movss   		(%%rbx), %%xmm4        			
   vfmadd231ps      		%%xmm11, %%xmm6, %%xmm31     		

   movss       	(%%rax), %%xmm0    				
   movss       	4(%%rax), %%xmm1    			

.endm 												

.macro	KERNEL8x1_END_K							

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm24     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm25     		

   prefetcht0 		256(%%rax)                 		

   movss       	16(%%rax), %%xmm8    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm26     		

   movss       	20(%%rax), %%xmm9    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm27     		

   movss       	24(%%rax), %%xmm10    			
   vfmadd231ps      		%%xmm8, %%xmm6, %%xmm28     		

   movss       	28(%%rax), %%xmm11    			
   vfmadd231ps      		%%xmm9, %%xmm6, %%xmm29     		
   vfmadd231ps      		%%xmm10, %%xmm6, %%xmm30     		

	addq  			$32, %%rax 						
   vfmadd231ps      		%%xmm11, %%xmm6, %%xmm31     		

.endm 												

.macro	ADD_C_8x1									

   movss   		(%%r10), %%xmm4        			
	vaddps 			%%xmm4, %%xmm24, %%xmm24		
   movss   		(%%r11), %%xmm5        			
	vaddps 			%%xmm5, %%xmm25, %%xmm25		
   movss   		(%%r12), %%xmm6        			
	vaddps 			%%xmm6, %%xmm26, %%xmm26		
   movss   		(%%r13), %%xmm7        			
	vaddps 			%%xmm7, %%xmm27, %%xmm27		

	leaq  			(%%r13, %%r8, 4), %%r10 		 // C0
	leaq 			(%%r10, %%r8, 4), %%r11 		 // C1
	leaq 			(%%r11, %%r8, 4), %%r12 		 // C2
	leaq 			(%%r12, %%r8, 4), %%r13 		 // C3

   movss   		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   movss   		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   movss   		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   movss   		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_8x1 									

   vmovss   		%%xmm24, (%%r10)        		
   vmovss   		%%xmm25, (%%r11)        		
	subq 			$8, %%rdi 						
   vmovss   		%%xmm26, (%%r12)        		
   vmovss   		%%xmm27, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%r10 				 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

   vmovss   		%%xmm28, (%%r10)        		
   vmovss   		%%xmm29, (%%r11)        		
   vmovss   		%%xmm30, (%%r12)        		
   vmovss   		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												

//-----------------------------------------------------------------------

.macro	KERNEL4x1_K1								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm28     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm4, %%xmm29     		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$4, %%rbx 						 // B
   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm4, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm4, %%xmm31     		

   prefetcht0 		4(%%rbx)                 		
   movss   		(%%rbx), %%xmm6        			

.endm 												

.macro	KERNEL4x1_K2								

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
	addq  			$16, %%rax 						

   prefetcht0 		256(%%rax)                 		
	addq  			$4, %%rbx 						 // B
   movss       	(%%rax), %%xmm0    				
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		

   movss       	4(%%rax), %%xmm1    			
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		

   movss   		(%%rbx), %%xmm4        			
   prefetcht0 		4(%%rbx)                 		

.endm 												

.macro	KERNEL4x1_END_K							

   movss       	8(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm6, %%xmm28     		

   movss       	12(%%rax), %%xmm3    			
   vfmadd231ps      		%%xmm1, %%xmm6, %%xmm29     		
	addq  			$16, %%rax 						

   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     		
   vfmadd231ps      		%%xmm3, %%xmm6, %%xmm31     		

.endm 												

.macro	ADD_C_4x1									

   movss   		(%%r10), %%xmm0        			
	vaddps 			%%xmm0, %%xmm28, %%xmm28		
   movss   		(%%r11), %%xmm1        			
	vaddps 			%%xmm1, %%xmm29, %%xmm29		
   movss   		(%%r12), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30		
   movss   		(%%r13), %%xmm3        			
	vaddps 			%%xmm3, %%xmm31, %%xmm31		

	mov  	%%rcx, %%r10 							 // C0
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3

.endm 												

.macro	SAVE_4x1 	
   subq 			$4, %%rdi 						

   vmovss   		%%xmm28, (%%r10)        		
   vmovss   		%%xmm29, (%%r11)        		
   vmovss   		%%xmm30, (%%r12)        		
   vmovss   		%%xmm31, (%%r13)        		

	leaq  	(%%r13, %%r8, 4), %%rcx 				 // C0

.endm 												
											

//-----------------------------------------------------------------

.macro	KERNEL1x1_K1
   prefetcht0 		256(%%rax)
   addq  			$4, %%rax 

   movss       	(%%rax), %%xmm2    			
   vfmadd231ps      		%%xmm0, %%xmm4, %%xmm30     				
							           		
	addq  			$4, %%rbx 						 // B	
   movss   		   (%%rbx), %%xmm6  
   prefetcht0 		4(%%rbx)        			

.endm 												

.macro	KERNEL1x1_K2		
   prefetcht0 		256(%%rax)  						
	addq  			$4, %%rax

   movss       	(%%rax), %%xmm0    			
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     					
                  		
	addq  			$4, %%rbx 						 // B		
   movss   		   (%%rbx), %%xmm4        			
   prefetcht0 		4(%%rbx)                 		

.endm 												

.macro	KERNEL1x1_END_K									
   vfmadd231ps      		%%xmm2, %%xmm6, %%xmm30     					
	addq  			$4, %%rax 								
.endm 												

.macro	ADD_C_1x1											
   movss   		(%%r10), %%xmm2        			
	vaddps 			%%xmm2, %%xmm30, %%xmm30				
	mov  	%%rcx, %%r10 							   // C0
.endm 												

.macro	SAVE_1x1 									
   vmovss   		%%xmm30, (%%r10)        		
   subq 			$1, %%rdi        		       		
	leaq  	(%%r10, %%r8, 4), %%rcx 			// C0

.endm 												

//-----------------------------------------------------------------

SMM_NN_KERNEL12x1:								

   mov 	%[C], %%rcx   	 						
   mov 	%[A], %%rax   	 						
   mov 	%[B], %%rbx   	 						

   prefetcht0 		(%%rax)                 		

	mov 	%[K], %%rdx 							 // K
	mov  	%[LN], %%r8 							
	mov  	%[Bc], %%r14 							
	mov  	%[M], %%rdi 							
	mov 	%[k_tag], %%r15 						

   prefetcht0 		(%%rbx)                 		
	mov 	%%rbx, %%r9 							 // B
	mov 	%%rdx, %%rsi 							 // K

BEGIN_PACK12x1:									

	mov 	%%r9, %%rbx 							 // B
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
	mov 	%%r14, %%rbp 							 // Bc

   movss  		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm20, %%xmm20, %%xmm20 			
	vpxorq 		%%xmm21, %%xmm21, %%xmm21 			
   movss       	(%%rax), %%xmm0    				
	vpxorq 		%%xmm22, %%xmm22, %%xmm22 			
	vpxorq 		%%xmm23, %%xmm23, %%xmm23 			
   movss       	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_PACK_K12x1:									

	KERNEL12x1_PACK_K1 							
	KERNEL12x1_PACK_K2 							
	KERNEL12x1_PACK_K1 							
	KERNEL12x1_PACK_K2 							
	KERNEL12x1_PACK_K1 							
	KERNEL12x1_PACK_K2 							
	KERNEL12x1_PACK_K1 							
   cmp 	$0, %%rdx           					
	je 		EDGE_PACK_K12x1 						
	KERNEL12x1_PACK_K2 							

	subq 	$8, %%rdx 								
   jmp     MAIN_PACK_K12x1       					

EDGE_PACK_K12x1:									

	KERNEL12x1_PACK_END_K 							
	jmp  	BEGIN_SAVE_12x1 						

//-----------------------------------------------------------------

BEGIN_M12x1:										

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10) 
	leaq 	(%%r10, %%r8, 4), %%r11 			 // C1
   prefetcht1 		(%%r11) 
	leaq 	(%%r11, %%r8, 4), %%r12 			 // C2
   prefetcht1 		(%%r12) 
	leaq 	(%%r12, %%r8, 4), %%r13 			 // C3
   prefetcht1 		(%%r13) 

	mov 	%%rsi, %%rdx 							 // K									

   movss  		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm20, %%xmm20, %%xmm20 			
	vpxorq 		%%xmm21, %%xmm21, %%xmm21 			
	vpxorq 		%%xmm22, %%xmm22, %%xmm22 			
   movss       	(%%rax), %%xmm0    				
	vpxorq 		%%xmm23, %%xmm23, %%xmm23 			
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
   movss       	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K12x1:										

	KERNEL12x1_K1 									
	KERNEL12x1_K2 									
	KERNEL12x1_K1 									
	KERNEL12x1_K2 									
	KERNEL12x1_K1 									
	KERNEL12x1_K2 									
	KERNEL12x1_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K12x1 							
	KERNEL12x1_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K12x1       						

EDGE_K12x1:										

	KERNEL12x1_END_K 								

BEGIN_SAVE_12x1:									
	cmp 	$0, %%r15								
	je  	SAVE_C12x1 							
	ADD_C_12x1 									

SAVE_C12x1: 										
	SAVE_12x1 										

	cmpq  	$12, %%rdi 								
	jnb 	BEGIN_M12x1                 // 不小于（或等于）则跳转 							

//------------------------------------------------------------------

BEGIN_M8_N1:										
   cmpq  	$8, %%rdi  								 // M % 8
	jb   	BEGIN_M4_N1 	                      //小于则跳转

	mov 	%%r14, %%rbx 							    // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							    // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K
   movss  		(%%rbx), %%xmm4        				
	vpxorq 		%%xmm24, %%xmm24, %%xmm24 			
	vpxorq 		%%xmm25, %%xmm25, %%xmm25			
   movss       	(%%rax), %%xmm0    				
	vpxorq 		%%xmm26, %%xmm26, %%xmm26 			
	vpxorq 		%%xmm27, %%xmm27, %%xmm27 			
   movss       	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K_M8_N1:										

	KERNEL8x1_K1 									
	KERNEL8x1_K2 									
	KERNEL8x1_K1 									
	KERNEL8x1_K2 									
	KERNEL8x1_K1 									
	KERNEL8x1_K2 									
	KERNEL8x1_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M8_N1 							
	KERNEL8x1_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M8_N1       					

EDGE_K_M8_N1:										

	KERNEL8x1_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_8x1 							
	ADD_C_8x1 										

SAVE_C_8x1: 										
	SAVE_8x1 										

//----------------------------------------------------------------

BEGIN_M4_N1:										

	cmpq  	$4, %%rdi  							 // M % 4
	jb   	BEGIN_M1_N1 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		
	leaq 	(%%r10, %%r8, 4), %%r11 				 // C1
   prefetcht1 		(%%r11)                 		
	leaq 	(%%r11, %%r8, 4), %%r12 				 // C2
   prefetcht1 		(%%r12)                 		
	leaq 	(%%r12, %%r8, 4), %%r13 				 // C3
   prefetcht1 		(%%r13)                 		

	mov 	%%rsi, %%rdx 							 // K

   movss  		(%%rbx), %%xmm4        				
   movss       	(%%rax), %%xmm0    				
	vpxorq 		%%xmm28, %%xmm28, %%xmm28 			
	vpxorq 		%%xmm29, %%xmm29, %%xmm29			
   movss       	4(%%rax), %%xmm1    			
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 			
	vpxorq 		%%xmm31, %%xmm31, %%xmm31 			

	subq 	$8, %%rdx 								

MAIN_K_M4_N1:										

	KERNEL4x1_K1 									
	KERNEL4x1_K2 									
	KERNEL4x1_K1 									
	KERNEL4x1_K2 									
	KERNEL4x1_K1 									
	KERNEL4x1_K2 									
	KERNEL4x1_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M4_N1 							
	KERNEL4x1_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M4_N1       					

EDGE_K_M4_N1:										

	KERNEL4x1_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_4x1 							
	ADD_C_4x1 										

SAVE_C_4x1: 										
	SAVE_4x1 										

//----------------------------------------------------------------


BEGIN_M1_N1:										
	cmpq  	$1, %%rdi  							 // M % 1
	jb   	END_M_N1 								

	mov 	%%r14, %%rbx 							 // Bc
   prefetcht0 		(%%rbx)                 		

	mov  	%%rcx, %%r10 							 // C0
   prefetcht1 		(%%r10)                 		              		                 		

	mov 	%%rsi, %%rdx 							 // K

   movss  		(%%rbx), %%xmm4        				
   movss       	(%%rax), %%xmm0    		 // A0				
	vpxorq 		%%xmm30, %%xmm30, %%xmm30 						

	subq 	$8, %%rdx 									

MAIN_K_M1_N1:										
	KERNEL1x1_K1 									
	KERNEL1x1_K2 									
	KERNEL1x1_K1 									
	KERNEL1x1_K2 									
	KERNEL1x1_K1 									
	KERNEL1x1_K2 									
	KERNEL1x1_K1 									
   cmp 	$0, %%rdx           					
	je 		EDGE_K_M1_N1 							
	KERNEL1x1_K2 									

	subq 	$8, %%rdx 								
   jmp     MAIN_K_M1_N1   					

EDGE_K_M1_N1:										
	KERNEL1x1_END_K 								

	cmp 	$0, %%r15								
	je  	SAVE_C_1x1 							
	ADD_C_1x1 
									
SAVE_C_1x1: 										
	SAVE_1x1 	
   cmpq  	$1, %%rdi 
	jnb 	BEGIN_M1_N1 					//不小于（或等于）则跳转								

//-----------------------------------------------------------------


END_M_N1:											

